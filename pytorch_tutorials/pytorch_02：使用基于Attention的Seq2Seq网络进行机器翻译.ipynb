{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_02：使用基于Attention的Seq2Seq网络进行机器翻译.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10DORGdxZhQZ1eTovCGTi3KKqpWx6PWXP",
      "authorship_tag": "ABX9TyMgHKdFjUOwojZJSiX0aVh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/nlp-ml-dl-notes/blob/master/pytorch_tutorials/pytorch_02%EF%BC%9A%E4%BD%BF%E7%94%A8%E5%9F%BA%E4%BA%8EAttention%E7%9A%84Seq2Seq%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QHcfN_AAw0A",
        "colab_type": "text"
      },
      "source": [
        "&emsp;&emsp;Seq2Seq网络简单但功能强大，使用两个循环神经网络协同工作，将一个Sequence转换为另一个Sequence。其中，编码器网络将输入压缩为一个向量，而解码器将该向量展开为一个新序列。\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/seq2seq.png\" width=\"500\">\n",
        "\n",
        "&emsp;&emsp;为了改进上图所示的模型，将使用一种注意力机制，该机制使解码器学会专注于输入序列的特定范围。\n",
        "\n",
        "&emsp;&emsp;首先导入所需的库"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgnKKC0lCsHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals,print_function,division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tViidGunDJbr",
        "colab_type": "code",
        "outputId": "4c15f251-ec75-4a62-e1f9-d477eff02af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b2ZWfqsDMd-",
        "colab_type": "code",
        "outputId": "1a0dda24-5db4-45e4-9024-d671f3818141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8_zanR8DYnb",
        "colab_type": "text"
      },
      "source": [
        "## 1.加载数据文件、准备数据\n",
        "\n",
        "&emsp;&emsp;使用的数据是英语到法语的翻译对集合。文件内容如下：\n",
        "\n",
        "I am cold. \\t J'ai froid.\n",
        "\n",
        "&emsp;&emsp;与字符级RNN教程中使用的字符编码类似，我们将一种语言中的每个单词表示为一个one-hot向量。与语言中存在的几十个字符相比，单词要多的多，因此编码向量很大。但是我们将仅适用语言中的几千个单词。\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/word-encoding.png\" width=\"500\">\n",
        "\n",
        "&emsp;&emsp;每个单词都有唯一的索引，以便用作网络的输入和输出。因此将新建一个Lang类，该类具有单词->索引(word2index)和索引->单词(index2word)字典，以及每个单词word2count的计数，用于以后替换替换掉稀有单词。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvK-kAY9G3ZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token=0\n",
        "EOS_token=1\n",
        "class Lang:\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "    self.word2index={}\n",
        "    self.word2count={}\n",
        "    self.index2word={0:'SOS',1:\"EOS\"}\n",
        "    self.n_words=2 #SOS和EOS\n",
        "  def addSentence(self,sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self,word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word]=self.n_words\n",
        "      self.word2count[word]=1\n",
        "      self.index2word[self.n_words]=word\n",
        "      self.n_words+=1\n",
        "    else:\n",
        "      self.word2count[word]+=1\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS8ONUyRJcOO",
        "colab_type": "text"
      },
      "source": [
        "&emsp;&emsp;文件都是Unicode编码，为简化起见，将Unicode编码转为ASCII，将所有内容都转为小写，并去掉大多数标点符号。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpMpsgu1Js88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFC',s)\n",
        "      if unicodedata.category(c)!='Mn'\n",
        "  )\n",
        "\n",
        "def normalizeString(s):\n",
        "  s=unicodeToAscii(s.lower().strip())#转为小写\n",
        "  s=re.sub(r\"([.!?])\",r\" \\1\",s)\n",
        "  s=re.sub(r\"[^a-zA-Z.!?]+\",r\" \",s)\n",
        "  return s\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ASmyLxXK0vC",
        "colab_type": "text"
      },
      "source": [
        "&emsp;&emsp;要读取数据文件，我们将文件分成几行，然后将行分为两对。文件都是英语->其他语言的。因此，如果要从其他语言->英语翻译，添加翻转标记以反转语句对。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6_Clc_xLcbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs(lang1,lang2,base_path,reverse=False):\n",
        "  print('Reading lines...')\n",
        "\n",
        "  lines=open(base_path+\"%s-%s-small.txt\"%(lang1,lang2),encoding='utf-8').read().strip().split('\\n')\n",
        "  print(len(lines))\n",
        "  pairs=[[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "  if reverse:\n",
        "    pairs=[list(reversed(p)) for p in pairs]\n",
        "    input_lang=Lang(lang2)\n",
        "    output_lang=Lang(lang1)\n",
        "  else:\n",
        "    input_lang=Lang(lang1)\n",
        "    output_lang=Lang(lang2)\n",
        "\n",
        "  return input_lang,output_lang,pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W-lpzWCM-lZ",
        "colab_type": "text"
      },
      "source": [
        "&emsp;&emsp;由于语句对比较多，但是想训练的快速一点。因此将数据集修剪为相对较短和简单的句子。最大长度为10个字(包括结尾的标点符号)，并且过滤掉翻译成“我是”或“他是”等形式的句子。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKd5-f2GSZ1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "  return len(p[0].split(' '))<MAX_LENGTH and len(p[1].split(' '))<MAX_LENGTH #and p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMiIFGmYTZt1",
        "colab_type": "text"
      },
      "source": [
        "完整的数据预处理流程如下：\n",
        "* 读取文本文件，并划分为行和语句对。\n",
        "* 标准化文本，并对文本进行过滤\n",
        "* 构造单词列表"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkJMVs5ETuCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(lang1,lang2,base_path,reverse=False):\n",
        "  input_lang,output_lang,pairs=readLangs(lang1,lang2,base_path,reverse)\n",
        "  print(\"Read %s sentence pairs\"%len(pairs))\n",
        "  pairs=filterPairs(pairs)\n",
        "  print(\"Trimmed to %s sentence pairs\"%len(pairs))\n",
        "  print(\"Counting words...\")\n",
        "  for pair in pairs:\n",
        "    input_lang.addSentence(pair[0])\n",
        "    output_lang.addSentence(pair[1])\n",
        "  print(\"Counted words:\")\n",
        "  print(input_lang.name,input_lang.n_words)\n",
        "  print(output_lang.name,output_lang.n_words)\n",
        "  return input_lang,output_lang,pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3geREln9oGg",
        "colab_type": "code",
        "outputId": "548b3de6-5ec2-4af1-8edf-5be09c08e45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls /content/drive/My\\ Drive/data/d2l-zh-tensoflow/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "airfoil_self_noise.dat\timg\t\t\tptb\n",
            "fr-en-small.txt\t\tjaychou_lyrics.txt.zip\tptb.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRS2tDGl-B9_",
        "colab_type": "code",
        "outputId": "c9aacc6a-95bf-48f6-8869-504c8e73feb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "base_path=\"/content/drive/My Drive/data/d2l-zh-tensoflow/\"\n",
        "input_lang,output_lang,pairs=prepareData('fr','en',base_path,True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "20\n",
            "Read 20 sentence pairs\n",
            "Trimmed to 20 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "en 37\n",
            "fr 45\n",
            "['he is my uncle .', 'c est mon oncle .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC2Z0ZH1-99C",
        "colab_type": "code",
        "outputId": "3e4de2ef-0f9c-4871-c178-4e15b5c10256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(pairs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwTZHWnPBLIX",
        "colab_type": "text"
      },
      "source": [
        "## 2. Seq2Seq模型\n",
        "\n",
        "&emsp;&emsp;循环神经网络(RNN)使用自己输出作为下一步的输入。Seq2Seq网络也称为Encoder-Decoder网络，是被称为encoder和decoder的两个RNN网络组成的模型。Encoder读取输入然后输出单个向量，Decoder读取该向量并产生一个输出序列。\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/seq2seq.png\" width=\"500\">\n",
        "\n",
        "&emsp;&emsp;不像使用单个RNN进行序列预测(每个输入对应一个输出),seq2seq模型使我们摆脱了序列长度和顺序的限制，这使其非常适合在两种语言之间进行翻译。\n",
        "\n",
        "&emsp;&emsp;考虑序列“Je ne suis pas le chat noir” → “I am not the black cat”. 输入序列中的大多数单词在输出序列中有着直接的翻译，但是顺序略有不同，例如“chat noir” and “black cat”.由于采用\"ne/pas\"结构，因此在输入句子中还有一个单词。直接从输入单词的序列中产生正确的单词比较困难。\n",
        "\n",
        "&emsp;&emsp;使用seq2seq模型，编码器会创建一个向量，在理想情况下，该向量将输入序列的“含义”编码为一个向量，能够充分表达序列的意思。\n",
        "\n",
        "### 2.1 编码器\n",
        "\n",
        "&emsp;&emsp;seq2seq网络的编码器是一个RNN网络，它为输入句子中的每个单词输出一些值，对于每个输入字，编码器输出一个向量和一个隐藏状态，并将隐藏状态用于下一个输入字。\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/encoder-network.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jUK36YpOIc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size):\n",
        "    super(EncoderRNN,self).__init__()\n",
        "    self.hidden_size=hidden_size\n",
        "\n",
        "    self.embedding=nn.Embedding(input_size,hidden_size)\n",
        "    self.gru=nn.GRU(hidden_size,hidden_size)\n",
        "\n",
        "  def forward(self,input,hidden):\n",
        "    embedded=self.embedding(input).view(1,1,-1)\n",
        "    output=embedded\n",
        "    output,hidden=self.gru(output,hidden)\n",
        "    return output,hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size,device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmjNZxaHPzjZ",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 解码器\n",
        "&emsp;&emsp;解码器是另一个RNN网络，使用编码器的输出向量作为输入，然后输出一个单词序列进行翻译任务。\n",
        "\n",
        "### 2.2.1 简单解码器\n",
        "&emsp;&emsp;在简单解码器中，我们仅使用编码器最后时间步的输出作为输入。最后时间步的输出有时被称为上下文向量，因为它编码了整个序列的上下文。这个上下文向量被用作解码器的初始隐藏状态。\n",
        "&emsp;&emsp;在每个时间步进行解码时，解码器被给予一个token和隐藏状态作为输入。初始的token是字符序列的起始token <SOS>,初始隐藏状态是上下文向量(编码器的最后隐藏状态)。\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/decoder-network.png\" width=\"300\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAPw0tlQz833",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self,hidden_size,output_size):\n",
        "    super(DecoderRNN,self).__init__()\n",
        "    self.hidden_size=hidden_size\n",
        "\n",
        "    self.embedding=nn.Embedding(output_size,hidden_size)\n",
        "    self.gru=nn.GRU(hidden_size,hidden_size)\n",
        "    self.out=nn.Linear(hidden_size,output_size)\n",
        "    self.softmax=nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self,input,hidden):\n",
        "    output=self.embedding(input).view(1,1,-1)\n",
        "    output=F.relu(output)\n",
        "    output,hidden=self.gru(output,hidden)\n",
        "    output=self.softmax(self.out(output[0]))\n",
        "    return output,hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size,device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBulcInf17hg",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2 注意力机制的Decoder\n",
        "&emsp;&emsp;如果只有上下文向量在编码器和解码器之间传递，则该单个向量承担对整个句子进行编码的负担。注意力机制使解码器可以针对解码器自身输出的每一步，专注于编码器输出的不同部分。首先，我们计算一组注意力权重，将这些权重与编码器输出向量相乘以创建加权组合。结果(在代码中称为attn_applied)应包含有关输入序列特定部分的信息，从而帮助解码器选择正确的输出单词。\n",
        "\n",
        "<img src=\"https://i.imgur.com/1152PYf.png\" width=\"300\">\n",
        "\n",
        "&emsp;&emsp;注意力的权重使用另一个全连接层 attn进行计算，使用decoder的输入和隐藏层作为输入。由于训练数据中包含各种长度的句子，因此要实际创建和训练该层，我们必须选择可以应用的最大句子长度(输入长度，用于编码器输出).最大长度的句子将使用所有注意力权重，而较短的句子将仅使用前几个。\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/attention-decoder-network.png\" width=\"300\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9QmdM7fSSFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self,hidden_size,output_size,dropout_p=0.1,max_length=MAX_LENGTH):\n",
        "    super(AttnDecoderRNN,self).__init__()\n",
        "    self.hidden_size=hidden_size\n",
        "    self.output_size=output_size\n",
        "    self.dropout_p=dropout_p\n",
        "    self.max_length=max_length\n",
        "\n",
        "    self.embedding=nn.Embedding(self.output_size,self.hidden_size)\n",
        "    self.attn=nn.Linear(self.hidden_size*2,self.max_length)\n",
        "    self.attn_combine=nn.Linear(self.hidden_size*2,self.hidden_size)\n",
        "    self.dropout=nn.Dropout(self.dropout_p)\n",
        "    self.gru=nn.GRU(self.hidden_size,self.hidden_size)\n",
        "    self.out=nn.Linear(self.hidden_size,self.output_size)\n",
        "\n",
        "  def forward(self,input,hidden,encoder_outputs):\n",
        "    embedded=self.embedding(input).view(1,1,-1)\n",
        "    embedded=self.dropout(embedded)\n",
        "\n",
        "    attn_weights=F.softmax(self.attn(torch.cat((embedded[0],hidden[0]),1)),dim=1)\n",
        "\n",
        "    attn_applied=torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
        "\n",
        "    output=torch.cat((embedded[0],attn_applied[0]),1)\n",
        "    output=self.attn_combine(output).unsqueeze(0)\n",
        "    output=F.relu(output)\n",
        "    output,hidden=self.gru(output,hidden)\n",
        "    output=F.log_softmax(self.out(output[0]),dim=1)\n",
        "    return output,hidden,attn_weights\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size,device=device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mApjuYTAXe4n",
        "colab_type": "text"
      },
      "source": [
        "## 3.  训练\n",
        "\n",
        "### 3.1 准备训练数据\n",
        "&emsp;&emsp;为了训练，对于每个语句对，我们需要一个输入张量(输入序列的索引)和目标张量(目标序列的词索引)。需要在两个序列后添加EOS token。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbo8v9G0dW07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang,sentence):\n",
        "  return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang,sentence):\n",
        "  indexes=indexesFromSentence(lang,sentence)\n",
        "  indexes.append(EOS_token)\n",
        "  return torch.tensor(indexes,dtype=torch.long,device=device).view(-1,1)\n",
        "\n",
        "def tensorsFromPairs(pair):\n",
        "  input_tensor=tensorFromSentence(input_lang,pair[0])\n",
        "  target_tensor=tensorFromSentence(output_lang,pair[1])\n",
        "  return (input_tensor,target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84kvmXjIfBMh",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 训练模型\n",
        "&emsp;&emsp;为了进行训练，我们通过编码器运行输入语句，并跟踪每个输出和最新的隐藏状态。然后，为解码器提供<SOS>token作为第一个输入，并将编码器的最后一个隐藏状态作为其第一个隐藏状态。\n",
        "\n",
        "&emsp;&emsp;\"Teacher forcing\"是使用真实的输出作为下一个输入，而不是使用编码器的预测作为输入。使用\"Teacher forcing\"会使训练收敛的更快，但是使用网络时可能会出现不稳定。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHEr57WPhOsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio=0.5\n",
        "\n",
        "def train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion,max_length=MAX_LENGTH):\n",
        "  encoder_hidden=encoder.initHidden()\n",
        "\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "\n",
        "  input_length=input_tensor.size(0)\n",
        "  target_length=target_tensor.size(0)\n",
        "\n",
        "  encoder_outputs=torch.zeros(max_length,encoder.hidden_size,device=device)\n",
        "\n",
        "  loss=0\n",
        "\n",
        "  for ei in range(input_length):\n",
        "    encoder_output,encoder_hidden=encoder(input_tensor[ei],encoder_hidden)\n",
        "    encoder_outputs[ei]=encoder_output[0,0]\n",
        "\n",
        "  decoder_input=torch.tensor([[SOS_token]],device=device) #SOS token作为解码器的初始输入\n",
        "\n",
        "  decoder_hidden=encoder_hidden #编码器的最后时间步的隐藏状态作为解码器的初始隐藏状态\n",
        "\n",
        "  use_teacher_forcing=True if random.random() <teacher_forcing_ratio else False\n",
        "\n",
        "  if use_teacher_forcing:\n",
        "    for di in range(target_length):\n",
        "      decoder_output,decoder_hidden,decoder_attention=decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
        "      loss+=criterion(decoder_output,target_tensor[di])\n",
        "      decoder_input=target_tensor[di]\n",
        "\n",
        "  else:\n",
        "    for di in range(target_length):\n",
        "      decoder_output,decoder_hidden,decoder_attention=decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
        "      topv,topi=decoder_output.topk(1)\n",
        "      decoder_input=topi.squeeze().detach() #detach from history as input\n",
        "      loss+=criterion(decoder_output,target_tensor[di])\n",
        "      if decoder_input.item()==EOS_token:\n",
        "        break\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item()/target_length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0IFX5Frp6lK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE99Ajvej1V7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def asMinutes(s):\n",
        "  m=math.floor(s/60)\n",
        "  s-=m*60\n",
        "  return '%dm %ds'%(m,s)\n",
        "\n",
        "def timeSince(since,percent):\n",
        "  now=time.time()\n",
        "  s=now-since \n",
        "  es=s/(since)\n",
        "  rs=es-s\n",
        "  return '%s(- %s)'%(asMinutes(s),asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibEx7O3NkcZH",
        "colab_type": "text"
      },
      "source": [
        "### 整个训练过程包括\n",
        "\n",
        "* 启动一个计时器\n",
        "* 初始化优化器和损失函数\n",
        "* 创建训练数据集\n",
        "* 创建空损失数组用于绘图"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wntHIf-Rk2U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder,decoder,n_iters,print_every=1000,plot_every=100,learning_rate=0.01):\n",
        "  start=time.time()\n",
        "  plot_losses=[]\n",
        "  print_loss_total=0\n",
        "  plot_loss_total=0\n",
        "  encoder_optimizer=optim.SGD(encoder.parameters(),lr=learning_rate)\n",
        "  decoder_optimizer=optim.SGD(encoder.parameters(),lr=learning_rate)\n",
        "  training_pairs=[tensorsFromPairs(random.choice(pairs)) for i in range(n_iters)]\n",
        "  criterion=nn.NLLLoss()\n",
        "  for iter in range(1,n_iters+1):\n",
        "    training_pair=training_pairs[iter-1]\n",
        "    input_tensor=training_pair[0]\n",
        "    target_tensor=training_pair[1]\n",
        "    loss=train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion)\n",
        "    print_loss_total+=loss\n",
        "    plot_loss_total+=loss\n",
        "\n",
        "    if iter % print_every==0:\n",
        "      print_loss_avg=print_loss_total / print_every\n",
        "      print_loss_total=0\n",
        "      print(\"%s (%d %d%%) %.4f\"%(timeSince(start,iter/n_iters),iter,iter/n_iters*100,print_loss_avg))\n",
        "    if iter % plot_every==0:\n",
        "      plot_loss_avg=plot_loss_total/plot_every\n",
        "      plot_losses.append(plot_loss_avg)\n",
        "      plot_loss_total=0\n",
        "  showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJqQv0W5oOZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFt0tQOgomk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showPlot(points):\n",
        "  plt.figure()\n",
        "  fig,ax=plt.subplots()\n",
        "  loc=ticker.MultipleLocator(base=0.2)\n",
        "  ax.yaxis.set_major_locator(loc)\n",
        "  plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiXpbtKso9VP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder,decoder,sentence,max_length=MAX_LENGTH):\n",
        "  with torch.no_grad():\n",
        "    input_tensor=tensorFromSentence(input_lang,sentence)\n",
        "    input_length=input_tensor.size()[0]\n",
        "    encoder_hidden=encoder.initHidden()\n",
        "    encoder_outputs=torch.zeros(max_length,encoder.hidden_size,device=device)\n",
        "    for ei in range(input_length):\n",
        "      encoder_output,encoder_hidden=encoder(input_tensor[ei],encoder_hidden)\n",
        "      encoder_outputs[ei]+=encoder_output[0,0]\n",
        "    decoder_input=torch.tensor([[SOS_token]],device=device)\n",
        "    decoder_hidden=encoder_hidden\n",
        "    decoded_words=[]\n",
        "    decoder_attentions=torch.zeros(max_length,max_length)\n",
        "\n",
        "    for di in range(max_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "      decoder_attentions[di] = decoder_attention.data\n",
        "      topv, topi = decoder_output.data.topk(1)\n",
        "      if topi.item() == EOS_token:\n",
        "        decoded_words.append('<EOS>')\n",
        "        break\n",
        "      else:\n",
        "        decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "      decoder_input = topi.squeeze().detach()\n",
        "\n",
        "    return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uihyOCngrHBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "  for i in range(n):\n",
        "    pair = random.choice(pairs)\n",
        "    print('>', pair[0])\n",
        "    print('=', pair[1])\n",
        "    output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    print('<', output_sentence)\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzmLE1F7rM4U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "d3bdd40c-b35e-46b7-fc3e-b6b6069d303f"
      },
      "source": [
        "hidden_size=256\n",
        "encoder1=EncoderRNN(input_lang.n_words,hidden_size).to(device)\n",
        "attn_decoder1=AttnDecoderRNN(hidden_size,output_lang.n_words,dropout_p=0.1).to(device)\n",
        "trainIters(encoder1,attn_decoder1,75000,print_every=5000)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 14s(- -2m 45s) (5000 6%) 3.1094\n",
            "2m 27s(- -3m 32s) (10000 13%) 2.9876\n",
            "3m 41s(- -4m 18s) (15000 20%) 2.9589\n",
            "4m 57s(- -5m 2s) (20000 26%) 2.9449\n",
            "6m 13s(- -7m 46s) (25000 33%) 2.9438\n",
            "7m 30s(- -8m 29s) (30000 40%) 2.9249\n",
            "8m 45s(- -9m 14s) (35000 46%) 2.9264\n",
            "10m 1s(- -11m 58s) (40000 53%) 2.9190\n",
            "11m 18s(- -12m 41s) (45000 60%) 2.9218\n",
            "12m 35s(- -13m 24s) (50000 66%) 2.9173\n",
            "13m 52s(- -14m 7s) (55000 73%) 2.9173\n",
            "15m 5s(- -16m 54s) (60000 80%) 2.9173\n",
            "16m 21s(- -17m 38s) (65000 86%) 2.9188\n",
            "17m 34s(- -18m 25s) (70000 93%) 2.9171\n",
            "18m 47s(- -19m 12s) (75000 100%) 2.9178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdEguz-KsCQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}