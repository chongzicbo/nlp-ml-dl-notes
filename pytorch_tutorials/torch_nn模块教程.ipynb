{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch.nn模块教程.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/Cf1U9sHfblZa/03JWmBx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/nlp-ml-dl-notes/blob/master/pytorch_tutorials/torch_nn%E6%A8%A1%E5%9D%97%E6%95%99%E7%A8%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLRnoWPS7Wom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import  torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp1nycup7kQ5",
        "colab_type": "text"
      },
      "source": [
        "##01. torch.nn.EmbeddingBag()\n",
        "\n",
        "class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean')\\\n",
        "计算一个'bags'里的embeddings的均值或和，不用实例化中间的embeddings。默认计算均值。\\\n",
        "对于固定长度的bags:\n",
        "* nn.EmbeddingBag和mode=sum相当于nn.Embedding()之后的torch.sum(dim=1)\n",
        "* 其与mode=mean相当于nn.Embedding()之后的torch.mean(dim=1)\n",
        "\n",
        "然而，nn.EmbeddingBag在时间和内存上更加高效。\\\n",
        "参数：\n",
        "* num_embeddings (int)：embeddings字典的大小\n",
        "* embedding_dim （int）:每个embedding向量的大小(维度)\n",
        "* max_norm (float,可选):如果给出，则重新归一化embeddings，使其范数小于该值\n",
        "* norm_type(float,可选):为max_norm参数计算p范数时的P.\n",
        "* scale_grad_by_freq (boolean, 可选) – 如果给出, 会根据 words 在 mini-batch 中的频率缩放梯度\n",
        "* mode (string, 可选) – ‘sum’ | ‘mean’. 指定减少 bag 的方式. 默认: ‘mean’\n",
        "\n",
        "###Inputs: input, offsets\n",
        "\n",
        "* input (N or BxN): LongTensor, 包括要提取的 embeddings 的索引, 当 input 是形状为 N 的 1D 张量时, 一个给出的 offsets 张量中包括: mini-batch 中每个新序列的起始位置\n",
        "* offsets (B or None): LongTensor, 包括一个 mini-batch 的可变长度序列中的每个新样本的起始位置 如果 input 是 2D (BxN) 的, offset 就不用再给出; 如果 input 是一个 mini-batch 的固定长度的序列, 每个序列的长度为 N\n",
        "###形状：\n",
        "\n",
        "* 输入：LongTensor N, N = 要提取的 embeddings 的数量,\n",
        "或者是 LongTensor BxN, B = mini-batch 中序列的数量, N = 每个序列中 embeddings 的数量\n",
        "\n",
        "* Offsets: LongTensor B, B = bags 的数量, 值为每个 bag 中 input 的 offset, i.e. 是长度的累加. Offsets 不会给出, 如果 Input是 2D 的BxN 张量, 输入被认为是固定长度的序列\n",
        "输出：(B, embedding_dim)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGv4pUiD7smc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f1864e77-3f78-4a32-8c55-0a740b6122fb"
      },
      "source": [
        "embedding_sum=torch.nn.EmbeddingBag(10,3,mode='sum')\n",
        "input=torch.autograd.Variable(torch.LongTensor([1,2,4,5,4,3,2,9]))#将indice为[1,2,4,5]的embedding进行sum，[4,3,2,9]的embedding也进行相加\n",
        "offsets=torch.autograd.Variable(torch.LongTensor([0,4]))\n",
        "embedding_sum(input,offsets)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.5947,  2.4057,  1.2095],\n",
              "        [ 1.4345, -0.5633,  0.2688]], grad_fn=<EmbeddingBagBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQNLhSPJALZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "68909f90-baf2-45fc-a715-44c670b7b50a"
      },
      "source": [
        "input=torch.autograd.Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))#将indice为[1,2,4,5]的embedding进行sum，[4,3,2,9]的embedding也进行相加\n",
        "# offsets=torch.autograd.Variable(torch.LongTensor([0,4]))\n",
        "embedding_sum(input)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.5947,  2.4057,  1.2095],\n",
              "        [ 1.4345, -0.5633,  0.2688]], grad_fn=<EmbeddingBagBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZjDNBNRB6mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}