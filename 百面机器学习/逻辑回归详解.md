# 1.Logistic回归概述

逻辑回归假设数据服从**伯努利分布**，通过极大化似然函数，使用**梯度下降法**求解参数，来达到将数据**二分类**的目的。

逻辑回归包含五点：

* 逻辑回归的假设

* 逻辑回归的损失函数

* 逻辑回归的求解方法

* 逻辑回归的目的

* 逻辑回归如何分类

  

  

# 2. 逻辑回归的假设



逻辑回归的第一个基本假设是**数据服从伯努利分布**

```
伯努利分布：是一个离散型概率分布，若成功，则随机变量取值1；若失败，随机变量取值为0。成功概率记为p,失败概率为q=1-p。
```

$$
f(x)=\left\{
\begin{aligned}
p &  & if \quad x=1 \\
q &  & if \quad x=0 \\

\end{aligned}
\right.
$$

既然假设数据服从伯努利分布，那就存在一个成功和失败，对应二分类就是正类和负类，则样本为正类的概率是$p$,为负类的概率是$q=1-p$,即：
$$
p=h_\theta(x;\theta) \\
q=1-h_\theta(x;\theta)
$$
逻辑回归的第二个假设是正类的**概率由sigmoid函数计算**，即：
$$
p=\frac{1}{1+e^{-\theta^Tx}}
$$
预测正样本的概率为：
$$
p(y=1|x;\theta)=h_\theta(x;\theta)=\frac{1}{1+e^{-\theta^Tx}}
$$
预测为负类的概率：
$$
p(y=0|x;\theta)=1-h_{\theta}(x;\theta)=\frac{1}{1+e^{\theta ^Tx}}
$$
写在一起表示预测样本的类别：
$$
\hat y=p=p(y=1|x;\theta)^y(1-p(y=1|x;\theta))^{1-y}
$$
上式表示，$\hat y$是个概率，$y=1$就是正类的概率，$y=0$就是负类的概率。



# 3. 逻辑回归的损失函数

逻辑回归的损失函数是它的极大似然函数，为啥？

```
极大似然估计：利用已知的样本结果信息，反推最具有可能(最大概率)导致这些样本结果出现的模型参数值(模型已知，参数未知)
```

联系到逻辑回归里，首先确定下模型是否已定，模型就是用来预测的公式：
$$
\hat y=(\frac{1}{1+e^{-\theta^Tx}})^y(\frac{1}{1+e^{\theta ^T x}})^{1-y}
$$
参数就是里面的$\theta$,样本的结果信息就是$x,y$，是训练样本的特征和标签。已知信息就是特征取这些值的情况下，它应该属于$y$类(正或负)。反推最具有可能（最大概率）导致这些样本结果出现的参数。举个例子，我们已经知道了一个样本点，是正类，那么我们把它丢入这个模型后，它预测的结果一定得是正类啊，正类才是正确的，才是我们所期望的，我们要尽可能的让它最大，这样才符合我们的真实标签。反过来一样的，如果你丢的是负类，那这个式子计算的就是负类的概率，同样我们要让它最大，所以此时不用区分正负类。概括一下

```
一个样本，不分正负类，丢入模型，多的不说，就是一个字，让他的概率尽可能大
```

对于整个训练集，**期望所有样本的概率都达到最大，就是我们的目标函数，**本身是个联合概率，但是假设每个样本独立，那所有样本的概率就可以写成：
$$
lossfunction=\sum_{i=1}^N(\frac{1}{1+e^{-\theta ^Tx_i}})^{y_i}(\frac{1}{1+e^{\theta ^Tx_i}})^{1-y_i}
$$
此时它只能叫目标函数，因为它是我们的目标。最大上式即可求得参数。

而损失函数为log lossfunction

* 第一步，取对数，去掉连乘，变为连加：
  $$
  log(loss_function)=\sum_{i=1}^N(y_ilog(\frac{1}{1+e^{-\theta ^Tx_i}})+(1-y_i)log(\frac{1}{1+e^{\theta^Tx_i}}))
  $$

* 由于损失函数一般需要最小化，所以加个负号：
  $$
  log(loss_function)=-\sum_{i=1}^N(y_ilog(\frac{1}{1+e^{-\theta ^Tx_i}})+(1-y_i)log(\frac{1}{1+e^{-\theta^Tx_i}}))
  $$
  简化之后，就可以称之为损失函数了
  $$
  loss=\sum_{i=1}^N((y_i\theta ^Tx_i))-log(1+e^{\theta^Tx_i})
  $$

# 4.逻辑回归的求解方法

一般都是用**梯度下降法**来求解，梯度下降有随机**梯度下降、批梯度下降和small batch梯度下降**三种方式：

* 梯度下降会获得**全局最优解**，缺点是更新每个参数时需要遍历所有的数据，计算量很大，并且会有很多的冗余计算，导致的结果就是当数据量很大时，每个参数的更新很慢。
* 随机梯度下降是以高方差频繁更新，优点是使得SGD会调到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程变得复杂，并且很难收敛到全局最优解。
* 小批量梯度下降结合了SGD和batch GD的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到**更加稳定**的收敛效果。

**梯度下降的两个问题**

* 第一个是**对模型来说如何选择合适的学习率**。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要一个较大的学习率来逼近最优解。但是学习到后面的时候，参数和最优解已经隔得比较近，还保持最初的学习率容易越过最优点，在最优点附件来回震荡，也就是说容易学过头，跑偏了。
* 第二个是**对参数来说如何选择合适的学习率**。在实践中，对每个参数都保持同样的学习率也不合理。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。



# 5.逻辑回归的目的

将数据二分类



# 6.逻辑回归如何分类

设定一个阈值，判断正类概率是否大于阈值，一般阈值是0.5，所以只用判断正类概率是否大于0.5即可。



# 7. 逻辑回归为什么用极大似然函数作为损失函数

一般和平方损失函数(最小二乘法)拿来比较，因为线性回归用的是平方损失函数，原因是平方损失函数加上sigmoid函数将会是一个**非凸函数**，不易求解，会得到局部解，**用对数似然函数得到高阶连续可导函数**，可以得到最优解。

其次，是因为**对数损失函数更新起来很快**，因为只和x,y有关，和sigmoid本身的梯度无关

# 8.逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响

先说结论，如果在**损失函数最终收敛**的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。

但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，**每一个特征都是原来特征权重值的百分之一**。

如果在**随机采样**的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

# 9.为什么我们还是会在训练的过程中将高度相关的特征去掉

去掉高度相关的特征会让**模型的可解释性更好**

**可以大大提高训练的速度**。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是**特征多了，本身就会增大训练的时间**。



# 10.逻辑回归的优缺点总结

**优点：**

* 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
* 模型效果不错。在工程上是可以接受的(作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
* 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
* 资源占用少，尤其是内存。因为只需要存储各个维度的特征值。
* 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以容易的对这些概率分数进行阈值划分。

**缺点**:

* **准确率并不是很高**。因为形式非常的简单(非常类似线性模型),很难去拟合数据的真实分布。

* **很难处理数据不平衡的问题**。10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。

* **处理非线性数据麻烦**。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类问题。

* **逻辑回归本身无法筛选特征**。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

  