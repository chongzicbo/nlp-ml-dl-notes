# 1.SVM简介

SVM是一种二类分类模型。基本思想是在特征空间中寻找间隔最大的分类超平面使数据得到高效的二分类。具体有三种情况：

* 当训练样 本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机。
* 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支撑向量机。
* 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

# 2.SVM为什么采用间隔最大化（与感知机的区别）

当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。**感知机利用误分类最小策略**，求得分离超平面，不过此时的**解有无穷多个**。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是**最鲁棒**的，对未知实例的**泛化能力最强**。

# 3. SVM的目标（硬间隔）

两个目标：

* 间隔最大化
  $$
  \min_{w,b}\frac{1}{2}||w||
  $$
  

* 使样本正确分类
  $$
  y_i(w^Tx_i+b)\geq1,i=1,2,\dots,m
  $$
  其中$w$是超平面参数，目标1是从点到面的距离公式简化而来，目标2相当于感知机，只是把大于等于0缩放成了大于等于1，为了方便推导。有了两个目标，写在一起，就变成了SVM的终极目标：
  $$
  \min _{w,b}\frac{1}{2}||w||^2 \\
  s.t. y_i(w^Tx_i+b)\geq1,\forall i
  $$
  

# 4. 求解目标(硬间隔)

从目标公式可知是一个有约束条件的最优化问题，用拉格朗日函数解决：
$$
\min_{w,b}\max_\alpha L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum_{i=1}^m \alpha _i(1-y_i(w^Tx_i+b)) \\
s.t. \alpha _i \geq0,\forall i
$$
在满足Slater定理的时候，且过程满足KKT条件时，原问题可转为对偶问题：
$$
\max_\alpha\min_{w,b} L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum_{i=1}^m \alpha _i(1-y_i(w^Tx_i+b)) \\
s.t. \alpha _i \geq0,\forall i
$$
先求其内部最小值，对$w$和$b$求偏导并令其等于0可得：
$$
w=\sum _{i=1}^m \alpha_iy_ix_i \\
\sum_{i=1}^m \alpha _i y_i
$$
将其带入上式中可得：
$$
\max _\alpha L(w,b,\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j\\
s.t. \sum_{i=1}^m\alpha_iy_i=0(\alpha_i \geq0,i=1,2,\ldots,m)
$$
此时需要求解$\alpha$，l利用SMO算法：

SMO的算法的基本思路是每次选择两个变量$\alpha _i$和$\alpha _j$，选取的两个变量所对应的样本之间间隔要尽可能大，因为这样更新会带给目标函数值更大的变化。SMO算法之所以高效，是因为仅优化两个参数的过程实际仅有一个约束条件，其中一个可由另一个表示，这样的二次规划问题具有闭式解。

# 5. 软间隔

不管直接在原特征空间，还是在映射的高维空间，我们都假设样本是线性可分的。虽然理论上我们总能找到一个高维映射使数据线性可分，但在实际任务中，寻找一个合适的核函数核很困难。此外，由于数据通常有噪声存在，一味追求数据线性可分可能会使模型陷入过拟合，因此，我们放宽对样本的要求，允许少量样本分类错误。这样的想法就意味着对目标函数的改变，之前推导的目标函数里不允许任何错误，并且让间隔最大，现在给之前的目标函数加上一个误差，就相当于允许原先的目标出错，引入松弛变量$\xi _i \geq0$,公式变为：
$$
\min _{w,b,\xi}\frac{1}{2}||w||^2+\sum_{i=1}^m \xi_i
$$
那么这个松弛变量怎么计算呢，最开始试图用0，1损失去计算，但0，1损失函数并不连续，求最值时求导的时候不好求，所以引入合页损失（hinge loss）：
$$
l_{hinge}(z)=\max(0,1-z)
$$
函数图长这样：

![img](https://mmbiz.qpic.cn/mmbiz_png/DHibuUfpZvQeuNqtzSBzsAQFKc4YF8vawIxibw1tgLydx8u6LReOZTtDcico0tM6RL5gic2Xzic8wwnvgNvufAqK13A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

理解起来就是，原先制约条件是保证所有样本分类正确，$y_i(w^Tx_i+b)\geq1,\forall i$ ，现在出现错误的时候，一定是这个式子不被满足了，即 $y_i(w^Tx_i+b)<1,\forall i_{错误}$，衡量一下错了多少呢？因为左边一定小于1，那就跟1比较，因为1是边界，所以用1减去$y_i(w^Tx_i+b)$ 来衡量错误了多少，所以目标变为（正确分类的话损失为0，错误的话付出代价）：
$$
\min _{w,b}\frac{1}{2}||w||^2+\sum_{i=1}^m \max(0,1-y_i(w^Tx_i+b))
$$
但这个代价需要一个控制的因子，引入$C >0$,惩罚参数，即：
$$
\min _{w,b}\frac{1}{2}||w||^2+C\sum_{i=1}^m \max(0,1-y_i(w^Tx_i+b))
$$
可以想象，C越大说明把错误放的越大，说明对错误的容忍度就小，反之亦然。当C无穷大时，就变成一点错误都不能容忍，即变成硬间隔。实际应用时我们要合理选取C，C越小越容易欠拟合，C越大越容易过拟合。

所以软间隔的目标函数为：
$$
\min_{w,b\xi}\frac{1}{2}||w||^2+C\sum_{i=1}^n \xi_i \\
s.t. y_i(x_i^Tw+b) \geq 1-\xi_i \\
\xi_i \geq0,i=1,2,\dots,n
$$
其中：
$$
\xi_i=max(0,1-y_i(w^Tx_i+b))
$$

# 6.软间隔求解

