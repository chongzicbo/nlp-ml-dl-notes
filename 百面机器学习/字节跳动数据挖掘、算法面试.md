1、在做LR的时候，对连续数据做离散化处理的优势是什么？

稀疏向量内积乘法运算速度快，计算结果方便存储，容易scalable（扩展）。

离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。

逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。

离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。

特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。

2.为什么不用深度学习算法而使用LR

3.词袋模型是什么？

词表有10个单词，用固定长度为10的向量表示文档集。
在向量中有一个位置来对每个单词进行评分。
最简单的评分方法是将单词的存在标记为布尔值，0表示缺席，1表示存在。
其他的简单评分方法包括：
（1）计数，计算 每个单词在文档中出现的 次数 ；
（2）频率，计算文档中所有单词中 每个单词出现在文档中的 频率。

4.介绍一下词向量

现在常用word2vec构成词向量模型，它的底层采用基于CBOW和Skip-Gram算法的神经网络模型。 CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。 这样我们这个CBOW的例子里，我们的输入是4个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有4个神经元，输出层有词汇表大小个神经元。Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为2， 特定的这个词"go"是我们的输入，而这4个上下文词是我们的输出。 这样我们这个Skip-Gram的例子里，我们的输入是特定词， 输出是softmax概率排前4的4个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。

5.bert的位置信息怎么输入进去

因为transformer既没有RNN的recurrence也没有CNN的convolution，但序列顺序信息很重要，比如你欠我100万明天要还和我欠你100万明天要还的含义截然不同。。。

　transformer计算token的位置信息这里使用正弦波↓，类似模拟信号传播周期性变化。这样的循环函数可以一定程度上增加模型的泛化能力。

6.bert有decoder吗？

这个待讨论

7.为什么要在bert后面加crf

  CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合法的。在训练数据训练过程中，这些约束可以通过CRF层自动学习到。

这些约束可以是：

I：句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-”

II：标签“B-label1 I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合法的序列, 但是“B-Person I-Organization” 是非法标签序列.

III：标签序列“O I-label” is 非法的.实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”。

有了这些约束，标签序列预测中非法序列出现的概率将会大大降低。
8.textrank是什么？

9.RNN的缺点是？为什么会产生梯度消失，LSTM怎么解决？

RNN梯度消失是因为激活函数tanh函数的倒数在0到1之间，反向传播时更新前面时刻的参数时，当参数W初始化为小于1的数，则多个(tanh函数’ * W)相乘，将导致求得的偏导极小（小于1的数连乘），从而导致梯度消失。

10.深度学习中为什么会出现过拟合

过拟合：根本原因是特征维度过多，模型假设过于复杂，参数过多，训练数据过少，噪声过多，导致拟合的函数完美的预测训练集，但对新数据的测试集预测结果差。 过度的拟合了训练数据，而没有考虑到泛化能力。

防止过拟合的方法主要有：

正则化
dropout
增加训练数据
提前停止训练过程
11.dropout是怎么发挥作用的

12.在做项目时遇到什么困难，怎么解决

13.一道编程题