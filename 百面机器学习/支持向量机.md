![img](https://i0.wp.com/upload-images.jianshu.io/upload_images/12997560-d929b45635989169)

image

以下只是将知识点QA化，不要为了面试硬背答案，还是先得好好看书



### Q-List：

- 简要介绍一下SVM
- 支持向量机包含几种模型
- 什么是支持向量
- SVM为什么采用间隔最大化
- SVM的参数（C，ξ，![gamma](https://math.jianshu.com/math?formula=%5Cgamma)）
- Linear SVM和LR的异同
- SVM和感知机的区别
- 感知机的损失函数
- SVM的损失函数
- SVM怎么处理多分类
- SVM可以处理回归问题吗
- 为什么把原问题转换为对偶问题
- 为什么求解对偶问题更加高效
- alpha系数有多少个
- 核函数，种类、如何选择
- 高斯核为什么会把原始维度映射到无穷多维
- 核函数的意义是什么
- 什么样的函数可以作为一个有效的核函数
- 大规模数据能用SVM吗

### QA：

#### 简要介绍一下SVM

支持向量机SVM是一种**二类分类模型**。它的基本模型是定义在**特征空间中的间隔最大的线性分类器**。学习的**目标就是在特征空间内找到一个分离超平面，能够将实例分到不同的类。**



*(从分类平面，到求两类间的最大间隔，到转化为求间隔分之一，等优化问题，然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题，最后再利用SMO（序列最小优化）来解决这个对偶问题。)*



#### 支持向量机包含几种模型

- 当训练样本**线性可分**时，通过**硬间隔最大化**，学习一个线性分类器，即**线性可分支持向量机**。
- 当训练数据**近似线性可分**时，引入**松弛变量**，通过**软间隔最大化**，学习一个线性分类器，即**线性支持向量机**。
- 当训练数据**线性不可分**时，通过使用**核技巧及软间隔最大化**，学习**非线性支持向量机**。

#### 什么是支持向量

在线性可分的情况下，训练数据集的样本点中**与分离超平面距离最近的样本点的实例称为支持向量**（support vecotr）。



#### SVM为什么采用间隔最大化

当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机或神经网络等利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，**解是唯一**的。另一方面，此时的分隔超平面所产生的分类结果是**最鲁棒的**，对**未知实例的泛化能力最强。**



#### SVM中的参数 （C，ξ，γ)

线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件，为了解决这个问题，对每个样本点引入一个松弛变量![xi_ige0](https://math.jianshu.com/math?formula=%5Cxi_i%5Cge0)



C是惩罚系数，即**对误差的宽容度。C越大，说明越不能容忍出现误差，容易过拟合。C越小，容易欠拟合。C过大或过小，泛化能力变差。**



C理解为调节优化方向中两个指标（间隔大小，分类准确度）偏好的权重。**当C趋于无穷大时，这个问题也就是不允许出现分类误差的样本存在，那这就是一个hard-margin SVM问题（过拟合）。当C趋于0时，我们不再关注分类是否正确，只要求间隔越大越好，那么我们将无法得到有意义的解且算法不会收敛（欠拟合）**



![gamma](https://math.jianshu.com/math?formula=%5Cgamma)是选择RBF函数作为kernel后，该函数自带的一个参数。**隐含地决定了数据映射到新的特征空间后的分布，![gamma](https://math.jianshu.com/math?formula=%5Cgamma)越大，支持向量越少，![gamma](https://math.jianshu.com/math?formula=%5Cgamma)值越小，支持向量越多。支持向量的个数影响训练与预测的速度。**



这里面需要引入一下老师们的讨论，需要考虑到RBF核的特点，特征空间分布等方面。讨论



#### Linear SVM和LR的异同

- 相同点：
  LR和SVM都是分类算法。
  如果不考虑核函数，LR和SVM都是线性分类算法，即分类决策面都是线性的。
  LR和SVM都是监督学习算法。
- 不同点
  损失函数不同，LR是对数损失logloss，SVM是合页损失hinge loss。
  LR受所有数据影响，而SVM只考虑局部的边界线附近的点（支持向量）。

#### SVM和感知机的区别

- 相同点
  都是属于监督学习的一种分类器（决策函数）。
- 不同点
  感知机追求最大程度正确划分，**最小化错误，很容易造成过拟合**。支持向量机追求大致正确分类的同时，一定程度上避免过拟合。
  感知机使用的学习策略是**梯度下降法**；而SVM采用的是由不等式约束条件构造拉格朗日函数，然后求偏导令其为0，根据一大堆的ai参数（一直迭代到满足kkt条件为止，**kkt条件是用来满足不等式约束下的拉格朗日乘子法的泛化**），来最终求得w和b。
  SVM分类超平面的解是唯一的，要满足间隔最大化。感知机的解不唯一，没有间隔最大化的约束条件，满足分开数据点的分界面都是可以的
  感知机属于线性分类模型，SVM包括核技巧，实质是非线性分类器。

#### 感知机的损失函数

感知机的损失函数是所有误分类点到超平面的总距离:![L(w,b)=-sumlimits_{x_iin{M}}y_i(w*x_i+b)](https://math.jianshu.com/math?formula=L(w%2Cb)%3D-%5Csum%5Climits_%7Bx_i%5Cin%7BM%7D%7Dy_i(w*x_i%2Bb))



#### SVM的损失函数

SVM除了原始最优化问题，还有另外一种解释就是最优化以下目标函数：
![sumlimits_{i=1}^{N}[1-y_i(w*x_i+b)]_++lambdaleft|w ight|^2](https://math.jianshu.com/math?formula=%5Csum%5Climits_%7Bi%3D1%7D%5E%7BN%7D%5B1-y_i(w*x_i%2Bb)%5D_%2B%2B%5Clambda%5Cleft%5C%7Cw%5Cright%5C%7C%5E2)
其中第一项是经验损失Hinge Loss，合页损失函数。

![](https://math.jianshu.com/math?formula=L(y(w*x%2Bb))=$[1-y(w*x+b)]_+$

下标“+”表示取正值的函数，类似ReLu函数。那么当样本点被正确分类且函数间隔（确信度）
$$
y_i(w*x_i+b)>1
$$
时，损失为0，否则损失是![1-y_i(w*x_i+b)](https://math.jianshu.com/math?formula=1-y_i(w*x_i%2Bb))
目标函数第二项是系数为![lambda](https://math.jianshu.com/math?formula=%5Clambda)的w的L2范数，正则化项![lambda](https://math.jianshu.com/math?formula=%5Clambda)是可调节的超参数。



#### SVM怎么处理多分类

- 直接法
  直接修改目标函数，将多个分类面的参数求解合并到一个最优化问题中。计算复杂度高，实现起来比较麻烦，适合于小型问题。
- 间接法
  - 一对多法（one-vs-rest, OVR SVMs），一次把某类样本归为一类，其他的为另一类
  - 一对一法，任意两类样本设计一个SVM

#### SVM可以处理回归问题吗

对于回归问题，优化目标函数和分类问题一致，还是![minfrac{1}{2}left|w ight|^2](https://math.jianshu.com/math?formula=min%5Cfrac%7B1%7D%7B2%7D%5Cleft%5C%7Cw%5Cright%5C%7C%5E2)，但是约束条件不同，看下图



![img](https://i0.wp.com/upload-images.jianshu.io/upload_images/12997560-066e0188c03684b0)

image

蓝色线是分类超平面，蓝色区域边界就是间隔边界，上面的点就是支持向量，因此蓝色区域内的点是没有损失的，但是外面的点是由损失的，损失大小为红色线的长度，这个地方就很像线性回归的误差了。
因此，SVM回归模型的损失函数度量为：

![img](https://i0.wp.com/upload-images.jianshu.io/upload_images/12997560-6534cd9b92acae2a)

image





#### 为什么把原问题转换为对偶问题

- 对偶问题是凸优化问题
- 对偶问题将原始问题中的约束转为了对偶问题中的等式约束
- 方便核函数的引入
- 求解更高效，改变了问题的复杂度，由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关

#### 为什么求解对偶问题更加高效

因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0



#### alpha系数有多少个

样本点的个数



#### 核函数，种类、如何选择

- 线性核
- 多项式核
- 高斯核（RBF核函数）

- 如果特征的数量大到和样本数量差不多，选用线性核SVM或LR
- 如果特征的数量少，样本数量正常，选用RBF核函数
- 如果特征很少，样本很多，最好是手动加一些特征，转为第一个情况

#### 高斯核为什么会把原始维度映射到无穷多维

从泰勒展开式的角度来解释
参考



#### 核函数的意义是什么

核函数的真正意义是做到了没有真正映射到高维空间却达到了映射的作用，即减少了大量的映射计算。



#### 什么样的函数可以作为一个有效的核函数

只要满足Mercer定理。



#### 大规模数据能用SVM吗

SVM的空间消耗主要是在存储训练样本和核矩阵，由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的及其内存和运算时间。如果数据量很大，SVM的训练时间就会比较长，所以SVM在大数据的使用中比较受限。



### 参考

《统计学习方法》 李航
《机器学习》 周志华
https://blog.csdn.net/qq_32742009/article/details/81838152
https://blog.csdn.net/guo1988kui/article/details/80207551
http://blog.sina.com.cn/s/blog_62970c250102xfzj.html
http://blog.sina.com.cn/s/blog_6ae183910101cxbv.html