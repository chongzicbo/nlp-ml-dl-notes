# 1.朴素贝叶斯与LR的区别？

简单来说：朴素贝叶斯是生成模型，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进而求出联合分布概率P(XY),最后利用贝叶斯定理求解P(Y|X)， 而LR是判别模型，根据极大化对数似然函数直接求出条件概率P(Y|X)；朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的），而LR则对此没有要求；朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。

# 2.朴素贝叶斯“朴素”在哪里？

简单来说：利用贝叶斯定理求解联合概率P(XY)时，需要计算条件概率P(X|Y)。在计算P(X|Y)时，朴素贝叶斯做了一个很强的条件独立假设（当Y确定时，X的各个分量取值之间相互独立），即P(X1=x1,X2=x2,...Xj=xj|Y=yk) = P(X1=x1|Y=yk)*P(X2=x2|Y=yk)*...*P(Xj=xj|Y=yk)。

# 3.在估计条件概率P(X|Y)时出现概率为0的情况怎么办？

简单来说：引入λ，当λ=1时称为拉普拉斯平滑。



# 4.朴素贝叶斯的优缺点

优点：对小规模的数据表现很好，适合多分类任务，适合增量式训练。

缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。



贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择能使条件风险R(c|x)最小的类别标记： 

/-------------------------------极大似然估计---------------------------------/

估计类的常用策略：先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。即概率模型的训练过程就是参数估计过程。

参数估计两大学派：频率主义学派和贝叶斯学派。

（1）频率主义：参数虽然未知，但却是客观存在的固定值，因此，可通过优化似然函数等准则来确定参数值（最大似然）。（2）贝叶斯学派：参数是未观察到的随机变量，本身也可以有分布，因此，可假定参数服从一个先验分布，然后基于观察到的数据来计算参数的后验分布。

/*-----------------------------朴素贝叶斯------------------------------------*/

朴素贝叶斯：

（1）思想：对于给定的待分类项x，通过学习到的模型计算后验概率分布，即：在此项出现的条件下各个目标类别出现的概率，将后验概率最大的类作为x所属的类别。后验概率根据贝叶斯定理计算。

（2）关键：为避免贝叶斯定理求解时面临的组合爆炸、样本稀疏问题，引入了条件独立性假设。

（3）工作原理： 

  

（4）工作流程：1）准备阶段：确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本。 2）训练阶段：对每个类别计算在样本中的出现频率p(y)，并且计算每个特征属性划分对每个类别的条件概率p(yi | x)； 3）应用阶段：使用分类器进行分类，输入是分类器和待分类样本，输出是样本属于的分类类别。

采用了属性条件独立性假设，

​       

d:属性数目，xi为x在第i个属性上的取值。





贝叶斯估计：



极大似然估计中，直接用连乘计算出的概率值为0，该样本的其他属性值将失效。为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，在估计概率值需要“平滑”，

优点： 高效、易于训练。对小规模的数据表现很好，适合多分类任务，适合增量式训练。

缺点： 分类的性能不一定很高，对输入数据的表达形式很敏感。（离散、连续，值极大之类的）

Note：为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?

1)对于分类任务来说，只要各类别的条件概率排序正确、无需精准概率值即可导致正确分类；

2)如果属性间依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响。

/*---------------------------半朴素贝叶斯-----------------------------------*/

提出：现实任务中，条件独立性假设很难成立，于是，人们对属性独立性假设进行一定程度的放松。

想法：适当考虑一部分属性间的相互依赖信息，从而既不需进行联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。

 

/*-----------------------------贝叶斯网------------------------------------*/



/*-------------------------------面试篇---------------------------------*/

1、贝叶斯分类器与贝叶斯学习不同：

前者：通过最大后验概率进行单点估计；后者：进行分布估计。

2、后验概率最大化准则意义？



3、朴素贝叶斯需要注意的地方？

（1）给出的特征向量长度可能不同，这是需要归一化为通长度的向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。

（2）计算要点：



4、经典提问：Navie Bayes和Logistic回归区别是什么？

前者是生成式模型，后者是判别式模型，二者的区别就是生成式模型与判别式模型的区别。

1）首先，Navie Bayes通过已知样本求得先验概率P(Y), 及条件概率P(X|Y), 对于给定的实例，计算联合概率，进而求出后验概率。也就是说，它尝试去找到底这个数据是怎么生成的（产生的），然后再进行分类。哪个类别最有可能产生这个信号，就属于那个类别。

优点：样本容量增加时，收敛更快；隐变量存在时也可适用。

缺点：时间长；需要样本多；浪费计算资源

2）相比之下，Logistic回归不关心样本中类别的比例及类别下出现特征的概率，它直接给出预测模型的式子。设每个特征都有一个权重，训练样本数据更新权重w，得出最终表达式。梯度法。

优点：直接预测往往准确率更高；简化问题；可以反应数据的分布情况，类别的差异特征；适用于较多类别的识别。

缺点：收敛慢；不适用于有隐变量的情况。