## **1.1 激活函数作用**

　　　　在生物的神经传导中，神经元接受多个神经的输入电位，当电位超过一定值时，该神经元激活，输出一个变换后的神经电位值。而在神经网络的设计中引入了这一概念，来增强神经网络的非线性能力，更好的模拟自然界。所以**激活函数的主要目的是为了引入非线性能力，即输出不是输入的线性组合。**

　　假设下图中的隐藏层使用的为**线性激活函数（恒等激活函数：a=g(z)）**，可以看出，当激活函数为**线性激活函数**时，输出不过是输入特征的**线性组合**（无论多少层），而不使用神经网络也可以构建这样的线性组合。而当激活函数为**非线性激活函数**时，通过神经网络的不断加深，可以构建出各种有趣的函数。

![img](https://img2018.cnblogs.com/blog/1483773/201909/1483773-20190919220642461-1008100320.png)

 

## **1.2 激活函数发展**

　　　　激活函数的发展过程：**Sigmoid -> Tanh -> ReLU -> Leaky ReLU -> Maxout** (目前大部分backbone中都采用ReLU或Leaky ReLU)。还有一个特殊的激活函数Softmax，但一般只用在网络的最后一层，进行最后的分类和归一化。借一张图总结下：

![img](https://img2018.cnblogs.com/blog/1483773/201909/1483773-20190919221048305-744687654.png)

 

 

 　**sigmoid函数**：能将**输入值映射到0-1范围**内，目前很少用作隐藏层的激活函数，**用在二分类中预测最后层输出概率值**。函数特点如下：

![img](https://img2018.cnblogs.com/blog/1483773/201909/1483773-20190919222709424-1091925840.png)

 

 

 

　　**存在问题：**

- *Sigmoid**函数饱和使梯度消失***。当神经元的激活在接近0或1处时会饱和，在这些区域梯度几乎为0，这就会导致梯度消失，几乎就有没有信号通过神经传回上一层。
- *Sigmoid函数的**输出不是零中心的***。因为如果输入神经元的数据总是正数，那么**关于![[公式]](https://www.zhihu.com/equation?tex=w)的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降**。

　　tanh双曲正切函数：将输入值映射到-1—1范围内，目前很少用做隐藏层激活函数。

![img](https://img2018.cnblogs.com/blog/1483773/201909/1483773-20190919224018998-1211112040.png)

 

　　　　

　　　　**Tanh解决了Sigmoid的输出是不是零中心的问题，但仍然存在饱和问题。**

　　　　（为了防止饱和，**现在主流的做法会在激活函数前多做一步*batch normalization***，**尽可能保证每一层网络的输入具有均值较小的、零中心的分布**。）

 　**ReLU函数(Rectified Linear unit)**：相较于sigmoid和tanh函数，计算比较**简单，收敛速度较快**，是目前主要的激活函数。

　　　　　　　　对比sigmoid类函数主要变化是：**1）单侧抑制；2）相对宽阔的兴奋边界；3）稀疏激活**性。

![img](https://img2018.cnblogs.com/blog/1483773/201909/1483773-20190919230359342-1843933031.png)

 

　　　　　存在问题：**ReLU单元比较脆弱并且可能“死掉**”，而且是不可逆的，因此导致了数据多样化的丢失。通过合理设置学习率，会降低神经元“死掉”的概率。

 　Leaky ReLU：相比于ReLU，使负轴信息不会全部丢失**，解决了ReLU神经元“死掉”的问题**。

![img](https://img2018.cnblogs.com/blog/1483773/201909/1483773-20190924215552247-1159156316.png)

 

 

   **PReLU(Parametric ReLU)**: Leaky ReLU函数中的a，在训练过程中固定不变，常取值为0.01，而PReLU函数的a是一个学习参数，会随着梯度变化而更新。

　　![img](https://img2018.cnblogs.com/blog/1483773/201909/1483773-20190924220517889-1788954523.png)

 

 

 

　　**ELUs函数(Exponential Linear Units)**：负半轴为指数函数，使x=0时的导数变化更加平滑

　　![img](https://img2018.cnblogs.com/blog/1483773/201909/1483773-20190924220548715-306086271.png)

 

 

![img](https://img2018.cnblogs.com/blog/1483773/201910/1483773-20191031220832630-2012152110.png)

 

 

　　**softmax：** 用于多分类网络的输出，预测每一类的概率值，目的是让大的更大。

![img](https://img2018.cnblogs.com/blog/1483773/201909/1483773-20190924223150160-506275523.png)

 



## 什么是激活函数gelu？

在神经网络的建模过程中，模型很重要的性质就是非线性，同时为了模型泛化能力，需要加入随机正则，例如dropout(随机置一些输出为0,其实也是一种变相的随机非线性激活)， 而随机正则与非线性激活是分开的两个事情， 而其实模型的输入是由非线性激活与随机正则两者共同决定的。

GELUs正是在激活中引入了随机正则的思想，是一种对神经元输入的概率描述，直观上更符合自然的认识，同时实验效果要比Relus与ELUs都要好。

GELUs其实是 dropout、zoneout、Relus的综合，GELUs对于输入乘以一个0,1组成的mask，而该mask的生成则是依概率随机的依赖于输入。假**设输入为X, mask为m，则m服从一个伯努利分布($\phi(x),\phi(x)=P(X \leq x)$,X服从标准正太分布**，这么选择是因为神经元的输入趋向于正太分布，这么设定使得当输入x减小的时候，输入会有一个更高的概率被dropout掉，这样的激活变换就会随机依赖于输入了.

gelu就是高斯误差线性单元，这种激活函数在激活中加入了随机正则的思想，是一种对神经元输入的概率描述。公式如下

![img](https://pic2.zhimg.com/v2-a27cf0927c89dc12f2ac2b71c1c7bec9_r.jpg)

可以看到，x作为神经元的输入， ![[公式]](https://www.zhihu.com/equation?tex=%5Cphi%EF%BC%88x%EF%BC%89) 也就越大，x就越有可能被保留，x越小，越有可能激活结果为0.

## gelu的使用技巧

当训练过程中使用gelu作为激活函数进行训练的时候，建议使用一个带有动量(momentum)的优化器。

参考：https://www.jianshu.com/p/0cf1aff51117

　　  https://www.cnblogs.com/lliuye/p/9486500.html

　　   https://zhuanlan.zhihu.com/p/32610035