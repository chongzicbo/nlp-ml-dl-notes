{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter01 特征工程.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdZpNWmPjqo/4iCscFAT8D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/nlp-ml-dl-notes/blob/master/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/chapter01_%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K_DYs8VcUXu",
        "colab_type": "text"
      },
      "source": [
        "## 1.特征归一化\n",
        "\n",
        "### 1.1 为什么要进行特征归一化\n",
        "&emsp;&emsp;为了消除数据特征之间的量纲影响，使得不同指标之间具有可比性。如身高、体重对健康的影响。想要得到更为准确的结果，需要对特征进行归一化处理，使得各指标处于同一数量级。\\\n",
        "&emsp;&emsp;特征归一化可以使梯度下降时加快收敛速度，更快地找到最优解。\n",
        "\n",
        "### 1.2 特征归一化的种类\n",
        "&emsp;&emsp;最常用的特征归一化方法有两种\n",
        "\n",
        "* 线性函数归一化(Min-Max Scaling)。对原始数据进行线性变换，使得结果映射到[0,1]的范围，实现对原始数据的等比缩放。公式如下：\n",
        "$$\n",
        "X_{norm}=\\frac{X-X_{min}}{X_{max}-X_{min}}\n",
        "$$\n",
        "其中$X$为原始数据，$X_{min}$、$X_{min}$分别为数据的最大值和最小值。\n",
        "\n",
        "* 零均值归一化(Z-Score Normalization)。将原始数据映射到均值为0，标准差为1的分布上。公式为：\n",
        "$$\n",
        "z=\\frac{x- \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "### 1.3 不需要特征归一化的情形\n",
        "\n",
        "&emsp;&emsp;实际应用中，通过梯度下降法求解的模型通常需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。\\\n",
        "&emsp;&emsp;对于决策树模型则并不适用，以C4.5为例，决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比，而信息增益比跟特征是否经过归一化无关，因为归一化并不会改变样本在特征x上的信息增益。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaH6rMfxywJf",
        "colab_type": "text"
      },
      "source": [
        "## 2.类别型特征\n",
        "\n",
        "&emsp;&emsp;除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作\n",
        "\n",
        "### 2.1. 类别型特征应该怎么处理\n",
        "\n",
        "* 序号编码\n",
        "\n",
        "&emsp;&emsp;通常用于处理类别间具有大小关系的数据。如成绩高低：高、中、低。\n",
        "\n",
        "* 独热编码(one-hot)\n",
        "&emsp;&emsp;通常用于处理类别间不具有大小关系的数据。如血型：A,B,O等。\n",
        "对于类别取值较多的情况下使用独热编码需要注意以下问题：\\\n",
        "(1)使用稀疏向量来节省空间。独热编码的向量只有某一维取值为1，其他位置为0.因此可以利用向量的稀疏表示来节省空间，目前大部分的算法均接受稀疏向量形式输入。\n",
        "(2)配合特征选择来降低维度。高维度特征会带来以下问题：一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量；二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；三是通常只有部分维度是对分类、预测有帮助。因此可以考虑配合特征选择来降低维度。\n",
        "\n",
        "* 二进制编码\n",
        "\n",
        "&emsp;&emsp;以A、B、AB、O血型为例，id分别为1,2,3,4,则它们的二进制编码分别为001,010,011,100.可以看出，二进制编码本质上是利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数小于独热编码，节省了存储空间。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8RALhhn5leC",
        "colab_type": "text"
      },
      "source": [
        "## 4. 组合特征\n",
        "### 4.1 怎样有效的找到组合特征\n",
        "* 构建决策树\n",
        "\n",
        "&emsp;&emsp;每一条从根节点到叶节点的路径都可以看成一种特征组合的方式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b3NuWAQ5PeB",
        "colab_type": "text"
      },
      "source": [
        "## 5.文本表示模型\n",
        "\n",
        "### 5.1 文本表示模型的类型及缺点\n",
        "* 词袋模型\n",
        "\n",
        "&emsp;&emsp;词袋模型就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。每篇文章可以表示成一个向量，向量中的每一维代表一个单词，而该维的权重反映这个词在这篇文章中重要程度。常用TF-IDF表示：\n",
        "<center>TF-IDF(t,d)=TF(t,d) x IDF(t)</center>\n",
        "其中TF为单词t在文档d中出现的频率，IDF为逆文档频率，用来衡量单词t对表达语义所起的重要性，表示为：\n",
        "$$\n",
        "IDF(t)=log \\frac{文章总数}{包含单词t的文章总数+1}\n",
        "$$\n",
        "直观的解释就是如果一个词在多个文章中出现，则该词对于区分谋篇文章特殊语义的贡献较小，因此要对权重做惩罚。\n",
        "\n",
        "* N-gram模型\n",
        "\n",
        "&emsp;&emsp;将连续出现的n个词组成的词组作为一个单独的特征放到向量表示中去，构成N-gram模型。实际应用中，同一个词可能有多种词性变化，但是却具有相同的含义，一般会对单词进行词干抽取处理，即将不同词性的单词统一成为同一词干的形式。\n",
        "\n",
        "* 主题模型\n",
        "&emsp;&emsp;主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。\n",
        "\n",
        "* 词嵌入和深度学习模型\n",
        "\n",
        "&emsp;&emsp;词嵌入核心思想是将每个词映射成低维空间上的稠密向量。向量的每一维可以看作一个隐含的主题。如果一篇文章有N个单词，就可以用一个$N \\times K$的矩阵来表示这篇文档。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvz79VfgBBf9",
        "colab_type": "text"
      },
      "source": [
        "## 6 Word2Vec\n",
        "### 6.1 Word2Vec的工作原理\n",
        "&emsp;&emsp;Word2Vec有两种网络结构，分别是CBOW和Skip-gram。CBOW的目标是根据上下文出现的单词来预测当前单词的生成概率。而skip-gram是根据当前词来预测上下文中各个单词的概率。CBOW和skip-gram都可以表示成由输入层、映射层和输出层组成的神经网络。\\\n",
        "&emsp;&emsp;输入层中的每个单词由独热编码表示。映射层中，K个隐含单元的取值可以由N维输入向量以及连接输入和隐含单元之间的$N \\times K$维权重矩阵计算得到。在CBOW中，需要将各个输入词所计算出的隐含单元求和。输出层向量可以通过隐含层向量(K维)以及连接隐含层和输出层之间的$K \\times N$维权重矩阵计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后对输出层向量应用Softmax激活函数，可以计算生成每个单词的概率。\\\n",
        "\n",
        "&emsp;&emsp;从输入层到隐含层需要一个维度为$N \\times K$的权重矩阵，从隐含层到输出层又需要一个维度为$K \\times N$的权重矩阵，权重矩阵的学习使用反向传播算法，每次迭代时将权重沿梯度更优的方向进行更新。但是由于Softmax函数中存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，因此产生了Hierarchical Softmax和Negative Sampling两种改进方法。\n",
        "\n",
        "### 6.2 Word2Vec和LDA的区别与联系\n",
        "\n",
        "&emsp;&emsp;LDA是利用文档中单词的共现关系来对单词按主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到\"文档-主题\"和“主题-单词”两个概率分布。而Word2Vec其实是对“上下文-单词”矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。也就是说如果两个单词得到的词向量相似度高，那么他们可能经常出现在相似的上下文中。\n",
        "\n",
        "&emsp;&emsp;LDA和Word2Vec最大的不同在于模型本身，主题模型是一种基于概率图模型的生成式模型，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量(即主题)；而词嵌入模型一般表示为神经网络的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxvNneSR6l8a",
        "colab_type": "text"
      },
      "source": [
        "## 7. 图像数据不足时的处理方法\n",
        "### 7.1 图像数据不足带来的问题\n",
        "&emsp;&emsp;主要问题是过拟合\n",
        "\n",
        "### 7.2 解决办法\n",
        "* 简化模型，如非线性变成线性\n",
        "* 添加约束项以缩小假设空间(L1/L2正则项)\n",
        "* 集成学习\n",
        "* Dropout\n",
        "* 数据扩充：旋转、平移、缩放、裁剪、填充、添加噪声、颜色变换，改变图像亮度、清晰度、对比度、锐度等\n",
        "* 生成对抗网络\n",
        "* 迁移学习\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZWxzWtn9kvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}