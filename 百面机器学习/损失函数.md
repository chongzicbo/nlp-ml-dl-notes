**损失函数**用来评价模型的**预测值**和**真实值**不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。

**损失函数**分为**经验风险损失函数**和**结构风险损失函数**。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。

常见的损失函数以及其优缺点如下：

1. **0-1损失函数(zero-one loss)**

0-1损失是指预测值和目标值不相等为1， 否则为0:

![[公式]](https://www.zhihu.com/equation?tex=L+%28+Y+%2C+f+%28+X+%29+%29+%3D+%5Cleft%5C%7B+%5Cbegin%7Barray%7D+%7B+l+%7D+%7B+1+%2C+Y+%5Cneq+f+%28+X+%29+%7D+%5C%5C+%7B+0+%2C+Y+%3D+f+%28+X+%29+%7D+%5Cend%7Barray%7D+%5Cright.+++%5C%5C)

特点：

(1)0-1损失函数直接对应分类判断错误的个数，但是它是一个**非凸函数**，不太适用.

(2)**感知机**就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 ![[公式]](https://www.zhihu.com/equation?tex=%7CY+-+f%28x%29%7C+%3C+T) 时认为相等，

![[公式]](https://www.zhihu.com/equation?tex=L+%28+Y+%2C+f+%28+X+%29+%29+%3D+%5Cleft%5C%7B+%5Cbegin%7Barray%7D+%7B+l+%7D+%7B+1+%2C+%7C+Y+-+f+%28+X+%29+%7C+%5Cgeq+T+%7D+%5C%5C+%7B+0+%2C+%7C+Y+%3D+f+%28+X+%29+%7C+%3C+T+%7D+%5Cend%7Barray%7D+%5Cright.++%5C%5C)

\2. **绝对值损失函数**

绝对值损失函数是计算预测值与目标值的差的绝对值：

![[公式]](https://www.zhihu.com/equation?tex=L%28Y%2C+f%28x%29%29+%3D+%7CY+-+f%28x%29%7C++%5C%5C)

\3. **log对数损失函数**

**log对数损失函数**的标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28Y%2C+P%28Y%7CX%29%29+%3D+-logP%28Y%7CX%29++%5C%5C)

特点：

(1) log对数损失函数能非常好的**表征概率分布**，在很多场景尤其是**多分类**，如果需要知道结果属于每个类别的置信度，那它非常适合。

(2)健壮性不强，**相比于hinge loss对噪声更敏感**。

(3)**逻辑回归**的损失函数就是log对数损失函数。

\4. **平方损失函数**

平方损失函数标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L+%28+Y+%7C+f+%28+X+%29+%29+%3D+%5Csum+_+%7B+N+%7D+%28+Y+-+f+%28+X+%29+%29+%5E+%7B+2+%7D++%5C%5C)

特点：

(1)经常应用于回归问题

\5. **指数损失函数（exponential loss）**

**指数损失函数**的标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28Y%7Cf%28X%29%29+%3D+exp%5B-yf%28x%29%5D++%5C%5C)

特点：

(1)**对离群点、噪声非常敏感**。经常用在AdaBoost算法中。

\6. **Hinge 损失函数**

Hinge损失函数标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28y%2C+f%28x%29%29+%3D+max%280%2C+1-yf%28x%29%29+++%5C%5C)

特点：

(1)hinge损失函数表示如果被分类正确，损失为0，否则损失就为 ![[公式]](https://www.zhihu.com/equation?tex=1-yf%28x%29) 。**SVM**就是使用这个损失函数。

(2)一般的 ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29) 是预测值，在-1到1之间， ![[公式]](https://www.zhihu.com/equation?tex=y) 是目标值(-1或1)。其含义是， ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29+) 的值在-1和+1之间就可以了，并不鼓励 ![[公式]](https://www.zhihu.com/equation?tex=%7Cf%28x%29%7C+%3E+1) ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而**使分类器可以更专注于整体的误差。**

(3) 健壮性相对较高，**对异常点、噪声不敏感**，但它**没太好的概率解释**。

\7. **感知损失(perceptron loss)函数**

**感知损失函数**的标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28y%2C+f%28x%29%29+%3D+max%280%2C+-f%28x%29%29++%5C%5C)

特点：

(1)是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss**只要样本的判定类别正确的话，它就满意，不管其判定边界的距离**。它比Hinge loss简单，因为不是max-margin boundary，所以模**型的泛化能力没 hinge loss强**。

\8. **交叉熵损失函数 (Cross-entropy loss function)**

**交叉熵损失函数**的标准形式如下**:**

![[公式]](https://www.zhihu.com/equation?tex=C+%3D+-+%5Cfrac+%7B+1+%7D+%7B+n+%7D+%5Csum+_+%7B+x+%7D+%5B+y+%5Cln+a+%2B+%28+1+-+y+%29+%5Cln+%28+1+-+a+%29+%5D++%5C%5C)

注意公式中 ![[公式]](https://www.zhihu.com/equation?tex=x) 表示样本， ![[公式]](https://www.zhihu.com/equation?tex=y) 表示实际的标签， ![[公式]](https://www.zhihu.com/equation?tex=a) 表示预测的输出， ![[公式]](https://www.zhihu.com/equation?tex=n) 表示样本总数量。

特点：

(1)本质上也是一种**对数似然函数**，可用于二分类和多分类任务中。

二分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：

![[公式]](https://www.zhihu.com/equation?tex=loss+%3D+-+%5Cfrac+%7B+1+%7D+%7B+n+%7D+%5Csum+_+%7B+x+%7D+%5B+y+%5Cln+a+%2B+%28+1+-+y+%29+%5Cln+%28+1+-+a+%29+%5D+%5C%5C)

多分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：

![[公式]](https://www.zhihu.com/equation?tex=loss+%3D+-+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_i+y_ilna_i+%5C%5C)

(2)当使用sigmoid作为激活函数的时候，常用**交叉熵损失函数**而不用**均方误差损失函数**，因为它可以**完美解决平方损失函数权重更新过慢**的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。

最后奉献上交叉熵损失函数的实现代码：[cross_entropy](https://github.com/yyHaker/MachineLearning/blob/master/src/common_functions/loss_functions.py).



------

这里需要更正一点，**对数损失函数和交叉熵损失函数应该是等价的！！！**（此处感谢 

[@Areshyy](https://www.zhihu.com/people/11973a76f350913884b09d774b783b57)

 的指正，下面说明也是由他提供）



下面来具体说明：

![img](https://pic4.zhimg.com/v2-ac627eab5f07ead5144cfaaff7f2163b_r.jpg)

------

**相关高频问题：**

1.**交叉熵函数**与**最大似然函数**的联系和区别？

区别：**交叉熵函数**使用来描述模型预测值和真实值的差距大小，越大代表越不相近；**似然函数**的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。

联系：**交叉熵函数**可以由最大似然函数在伯努利分布的条件下推导出来，或者说**最小化交叉熵函数**的本质就是**对数似然函数的最大化**。

怎么推导的呢？我们具体来看一下。

设一个随机变量 ![[公式]](https://www.zhihu.com/equation?tex=X) 满足伯努利分布，

![[公式]](https://www.zhihu.com/equation?tex=P%28X%3D1%29+%3D+p%2C+P%28X%3D0%29%3D1-p+%5C%5C+)

则 ![[公式]](https://www.zhihu.com/equation?tex=X) 的概率密度函数为：

![[公式]](https://www.zhihu.com/equation?tex=P%28X%29%3Dp%5EX%281-p%29%5E%7B1-X%7D++%5C%5C)

因为我们只有一组采样数据 ![[公式]](https://www.zhihu.com/equation?tex=D) ，我们可以统计得到 ![[公式]](https://www.zhihu.com/equation?tex=X) 和 ![[公式]](https://www.zhihu.com/equation?tex=1-X) 的值，但是 ![[公式]](https://www.zhihu.com/equation?tex=p) 的概率是未知的，接下来我们就用**极大似然估计**的方法来估计这个 ![[公式]](https://www.zhihu.com/equation?tex=p) 值。

对于采样数据 ![[公式]](https://www.zhihu.com/equation?tex=D) ，其**对数似然函数**为:

![[公式]](https://www.zhihu.com/equation?tex=logP%28D%29+%3D+log%5Cprod_%7Bi%7D%5E%7BN%7DP%28D_i%29+%5C%5C+%3D+%5Csum_%7Bi%7Dlogp%28D_i%29+%5C%5C+%3D+%5Csum_%7Bi%7D%28D_ilogp+%2B+%281-D_i%29log%281-p%29%29++%5C%5C)

可以看到上式和**交叉熵函数**的形式几乎相同，**极大似然估计**就是要求这个式子的最大值。而由于上面函数的值总是小于0，一般像神经网络等对于损失函数会用最小化的方法进行优化，所以一般会在前面加一个负号，得到**交叉熵函数**（或**交叉熵损失函数**）：

![[公式]](https://www.zhihu.com/equation?tex=loss+%3D+-%5Csum_%7Bi%7D%28D_ilogp+%2B+%281-D_i%29log%281-p%29%29+++%5C%5C)

这个式子揭示了**交叉熵函数**与**极大似然估计**的联系，**最小化交叉熵函数**的本质就是**对数似然函数的最大化。**

现在我们可以用求导得到极大值点的方法来求其**极大似然估计**，首先将对数似然函数对 ![[公式]](https://www.zhihu.com/equation?tex=p) 进行求导，并令导数为0，得到

![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%7D%28D_i+%5Cfrac%7B1%7D%7Bp%7D+%2B+%281-D_i%29+%5Cfrac%7B1%7D%7Bp-1%7D%29+%3D0+%5C%5C)

消去分母，得：

![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%7D%5E%7BN%7D%28p-D_i%29+%3D+0+%5C%5C)

所以:

![[公式]](https://www.zhihu.com/equation?tex=p++%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_i+D_i++%5C%5C)

这就是伯努利分布下**最大似然估计**求出的概率 ![[公式]](https://www.zhihu.com/equation?tex=p) 。

\2. 在用sigmoid作为激活函数的时候，为什么要用**交叉熵损失函数**，而不用**均方误差损失函数**？

其实这个问题求个导，分析一下两个误差函数的参数更新过程就会发现原因了。

对于**均方误差损失函数**，常常定义为：

![[公式]](https://www.zhihu.com/equation?tex=C%3D+%5Cfrac%7B1%7D%7B2n%7D%5Csum_x%28a+-+y%29%5E2+%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=y) 是我们期望的输出， ![[公式]](https://www.zhihu.com/equation?tex=a) 为神经元的实际输出（ ![[公式]](https://www.zhihu.com/equation?tex=a+%3D+%5Csigma%28z%29%2C+z%3Dwx%2Bb) ）。在训练神经网络的时候我们使用梯度下降的方法来更新 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) ，因此需要计算代价函数对 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) 的导数：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C+%7D%7B%5Cpartial+w%7D+%3D+%28a-y%29%5Csigma%27%28z%29x+%5C%5C+%5Cfrac%7B%5Cpartial+C+%7D%7B%5Cpartial+b+%7D+%3D+%28a-y%29%5Csigma%27%28z%29%5C%5C)

然后更新参数 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) ：

![[公式]](https://www.zhihu.com/equation?tex=w+%3D+w+-+%5Ceta+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w%7D+%3D+w+-+%5Ceta+%28a-y%29%5Csigma%27%28z%29x+%5C%5C+b+%3D+b+-+%5Ceta+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b%7D+%3D+b+-+%5Ceta+%28a-y%29%5Csigma%27%28z%29+%5C%5C)

因为sigmoid的性质，导致 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%27%28x%29) 在 ![[公式]](https://www.zhihu.com/equation?tex=z) 取大部分值时会很小（如下图标出来的两端，几乎接近于平坦），这样会使得 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta+%28a-y%29%5Csigma%27%28z%29) 很小，导致参数 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) 更新非常慢。

![img](https://pic1.zhimg.com/v2-c5c4f0aaab1c73b82caca103ef6d8050_r.jpg)

那么为什么**交叉熵损失函数**就会比较好了呢？同样的对于**交叉熵损失函数**，计算一下参数更新的梯度公式就会发现原因。**交叉熵损失函数**一般定义为：

![[公式]](https://www.zhihu.com/equation?tex=C+%3D+-+%5Cfrac+%7B+1+%7D+%7B+n+%7D+%5Csum+_+%7B+x+%7D+%5B+y+%5Cln+a+%2B+%28+1+-+y+%29+%5Cln+%28+1+-+a+%29+%5D+%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=y) 是我们期望的输出， ![[公式]](https://www.zhihu.com/equation?tex=a) 为神经元的实际输出（ ![[公式]](https://www.zhihu.com/equation?tex=a+%3D+%5Csigma%28z%29%2C+z%3Dwx%2Bb) ）。同样可以看看它的导数：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a%7D+%3D+-%5Cfrac%7B1%7D%7Bn%7D%5Csum_x%5By%5Cfrac%7B1%7D%7Ba%7D%2B%28y-1%29%5Cfrac%7B1%7D%7B1-a%7D%5D+%5C%5C+%3D+-%5Cfrac%7B1%7D%7Bn%7D%5Csum_x%5B%5Cfrac%7B1%7D%7Ba%281-a%29%7Dy+-+%5Cfrac%7B1%7D%7B1-a%7D%5D+%5C%5C+%3D+-%5Cfrac%7B1%7D%7Bn%7D%5Csum_x%5B%5Cfrac%7B1%7D%7B%5Csigma%28x%29%281-%5Csigma%28x%29%29%7Dy+-+%5Cfrac%7B1%7D%7B1-%5Csigma%28x%29%7D%5D+%5C%5C)

另外，

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z%7D+%3D+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a%7D+%5Cfrac%7B%5Cpartial+a%7D%7B%5Cpartial+z%7D+%5C%5C+%3D-%5Cfrac%7B1%7D%7Bn%7D%5Csum_x%5B%5Cfrac%7B1%7D%7B%5Csigma%28x%29%281-%5Csigma%28x%29%29%7Dy+-+%5Cfrac%7B1%7D%7B1-%5Csigma%28x%29%7D%5D+%5Cbullet+%5Csigma%27%28x%29+%5C%5C+%3D+-%5Cfrac%7B1%7D%7Bn%7D%5Csum_x%5B%5Cfrac%7B1%7D%7B%5Csigma%28x%29%281-%5Csigma%28x%29%29%7Dy+-+%5Cfrac%7B1%7D%7B1-%5Csigma%28x%29%7D%5D%5Cbullet+%5Csigma%28x%29%281-%5Csigma%28x%29%29+%5C%5C+%3D+-%5Cfrac%7B1%7D%7Bn%7D+%5Csum_x%28y-a%29+%5C%5C)

所以有：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w%7D+%3D+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z%7D+%5Cfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+%3D+%28a-y%29x++%5C%5C)

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b%7D+%3D+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z%7D+%5Cfrac%7B%5Cpartial+z%7D%7B%5Cpartial+b%7D+%3D+%28a-y%29++%5C%5C)

所以参数更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=w+%3D+w+-+%5Ceta+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w%7D+%3D+w+-+%5Ceta+%28a-y%29x%5C%5C+b+%3D+b+-+%5Ceta+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b%7D+%3D+b+-+%5Ceta+%28a-y%29+%5C%5C)

可以看到参数更新公式中没有 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%27%28x%29) 这一项，权重的更新受 ![[公式]](https://www.zhihu.com/equation?tex=%28a-y%29) 影响，受到误差的影响，所以***当误差大的时候，权重更新快；当误差小的时候，权重更新慢\***。这是一个很好的性质。

所以当使用sigmoid作为激活函数的时候，常用**交叉熵损失函数**而不用**均方误差损失函数**。



