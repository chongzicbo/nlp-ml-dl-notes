# 谈谈自己对决策树的理解？

决策树算法，无论是哪种，其目的都是为了让模型的**不确定性降低**的越快越好，基于其评价指标的不同，主要是ID3算法，C4.5算法和CART算法，其中ID3算法的评价指标是**信息增益**，C4.5算法的评价指标是**信息增益率**，CART算法的评价指标是**基尼系数**。

# 谈谈对信息增益和信息增益率的理解？

- 要理解信息增益，**首先**要理解熵这个概念。从概率统计的角度看，熵是对随机变量不确定性的度量，也可以说是对随机变量的概率分布的一个衡量。**熵越大，随机变量的不确定性就越大。对同一个随机变量，当他的概率分布为均匀分布时，不确定性最大，熵也最大。对有相同概率分布的不同的随机变量，取值越多的随机变量熵越大**。**其次**，要理解条件熵的概念。正如熵是对随机变量不确定性的度量一样，**条件熵是指，有相关的两个随机变量X和Y**，**在已知随机变量X的条件下，随机变量Y的不确定性**。当熵和条件熵中概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别为**经验熵**与**经验条件熵**。

- 所谓信息增益，也叫**互信息**，就是指集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D∣A)之差。ID3算法在每一次对决策树进行分叉选取最优特征时，会选取信息增益最高的特征来作为分裂特征。

- 信息增益准则的问题（ID3算法存在的问题）？
  信息增益准则对那些特征的取值比较多的特征有所偏好，也就是说，采用信息增益作为判定方法，**会倾向于去选择特征取值比较多的特征作为最优特征**。那么，选择取值多的特征为甚就不好呢？参考[这篇博文](https://blog.csdn.net/u012351768/article/details/73469813)。

- 采用信息增益率的算法C4.5为什么可以解决ID3算法中存在的问题呢？
  信息增益率的公式如下：
$$
  g_R(D,A)=\frac{g(D,A)}{H_A(D)}
  $$
  其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$，n是特征*A*取值的个数。$H_A(D)$表示的就是特征*A*的纯度，如果A只含有少量的取值的话，那么A的纯度就比较高，$H_A(D)$就比较小；相反，如果A取值越多的话，那么A的纯度就越低，$H_A(D)$就比较大。这样就可以解决ID3算法中存在的问题了。

# 决策树出现过拟合的原因及其解决办法？

对训练数据预测效果很好，但是测试数据预测效果较差的现象称为过拟合。

- 原因：
  - 在决策树构建的过程中，对决策树的生长没有进行合理的限制（没有剪枝）；
  - 样本中有一些噪声数据，没有对噪声数据进行有效的剔除；(噪声数据影响)
  - *在构建决策树过程中使用了较多的输出变量，变量较多也容易产生过拟合*（特征太多）。
- 解决办法
  - 选择合理的参数进行剪枝，可以分为**预剪枝和后剪枝**，我们一般采用后剪枝的方法；
  - 利用**K−foldsK-folds*K*−*folds*交叉验证**，将训练集分为KK*K*份，然后进行KK*K*次交叉验证，每次使用K−1K-1*K*−1份作为训练样本数据集，另外一份作为测试集；
  - **减少特征**，*计算每一个特征和响应变量的相关性，常见得为皮尔逊相关系数，将相关性较小的变量剔除*（待解释！！！）；*当然还有一些其他的方法来进行特征筛选，比如基于决策树的特征筛选，通过正则化的方式来进行特征选取等*（决策的正则化，例如，L1和L2正则，具体是对谁的正则呢？怎样正则的呢？）。**面试官顺便会问L1和L2，一定要搞明白**

# 简单解释一下预剪枝和后剪枝，以及剪枝过程中可以参考的参数有哪些？

- 预剪枝：在决策树生成初期就已经设置了决策树的参数，决策树构建过程中，满足参数条件就提前停止决策树的生成。
- 后剪枝：后剪枝是一种**全局的优化方法**，它是在决策树完全建立之后再返回去对决策树进行剪枝。
- 参数：**树的高度、叶子节点的数目、最大叶子节点数、限制不纯度**、特征分裂时样本数量

# 决策树的优缺点

- 优点：
  - **计算简单、速度快**；
  - **可解释性强**；
  - **比较适合处理有缺失属性的样本**。
- 缺点：
  - **容易发生过拟合**（随机森林可以很大程度上减少过拟合）；
  - **忽略了数据之间的相关性**；
  - **对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征**（只要是使用了信息增益，都有这个缺点，如RF）。*对应的案例如下：有这么一个场景，在一个样本集中，其中有100个样本属于A，9900个样本属于B，用决策树算法实现对AB样本进行区分的时候，会发生欠拟合的现象。因为在这个样本集中，AB样本属于严重失衡状态，在建立决策树算法的过程中，模型会更多的偏倚到B样本的性质，对A样本的性质训练较差，不能很好的反映样本集的特征。*（待解释！！！）。

# 决策树是如何处理缺失值的？

推荐一篇博文，

决策树（decision tree）（四）——缺失值处理

，该博文对下述需处理缺失值的

三种情况

有详细的描述：

- 如何在训练样本属性缺失的情况下进行划分属性的选择？
- 给定划分属性，若样本在该属性上的值是缺失的，那么该如何对这个样本进行划分？
- 如何解决测试样本中属性有缺失值的情况？

# 决策树与逻辑回归的区别？

- 对于拥有**缺失值**的数据，决策树可以应对，而逻辑回归需要挖掘人员预先对缺失数据进行处理；（缺失值处理）
- 逻辑回归对数据**整体结构**的分析优于决策树，而决策树对**局部结构**的分析优于逻辑回归；（决策树由于采用分割的方法，所以能够深入数据内部，但同时失去了对全局的把握。一个分层一旦形成，它和别的层面或节点的关系就被切断了，以后的挖掘只能在局部中进行。同时由于切分，样本数量不断萎缩，所以无法支持对多变量的同时检验。而逻辑回归，始终着眼整个数据的拟合，所以对全局把握较好。但无法兼顾局部数据，或者说缺乏探查局部结构的内在机制。）（整体把握和局部）
- 逻辑回归擅长分析**线性关系**，而决策树对线性关系的把握较差。线性关系在实践中有很多优点：简洁，易理解，可以在一定程度上防止对数据的过度拟合。（我自己对线性的理解：1，逻辑回归应用的是样本数据线性可分的场景，输出结果是概率，即，输出结果和样本数据之间不存在直接的线性关系；2，线性回归应用的是样本数据和输出结果之间存在线性关系的场景，即，自变量和因变量之间存在线性关系。）（线性和非线性）
- 逻辑回归对**极值**比较敏感，容易受极端值的影响，而决策树在这方面表现较好。（极值敏感）
- 应用上的区别：**决策树的结果和逻辑回归相比略显粗糙**。逻辑回归原则上可以提供数据中每个观察点的概率，而决策树只能把挖掘对象分为有限的概率组群。比如决策树确定17个节点，全部数据就只能有17个概率，在应用上受到一定限制。就操作来说，决策树比较容易上手，需要的数据预处理较少，而逻辑回归则要去一定的训练和技巧。
- **执行速度**上：当数据量很大的时候，逻辑回归的执行速度非常慢，而决策树的运行速度明显快于逻辑回归。（执行速度）