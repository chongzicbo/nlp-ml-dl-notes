
![img](https://pic2.zhimg.com/v2-d6588457cc144c1bad2f87ec77081af1_r.jpg)





•横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。
•实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。

•**可以看出随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。**



产生这种现象的原因如下：

![img](https://gitee.com/chengbo123/images/raw/master/v2-677a5e08d5b55b3b4cb14f7ad6f8eb31_r.jpg)

•原因1：噪声、样本冲突，即错误的样本数据。

•原因2：特征即属性不能完全作为分类标准。

特殊的，还有↓

•原因3：巧合的规律性，数据量不够大。



这个时候，就需要对生成树进行修剪，也就是***剪枝***。

剪枝通常有两种做法：***预剪枝***和***后剪枝***。



***预剪枝***

•预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法:
(1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。
(2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。
(3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。

(4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。



***优点&缺点***

•由于预剪枝不必生成整棵决策树，且算法相对简单， 效率很高， 适合解决大规模问题。但是尽管这一方法看起来很直接， 但是 怎样精确地估计何时停止树的增长是相当困难的。

•预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。





***后剪枝***

后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。

这里主要介绍四种：

1、REP-错误率降低剪枝

2、PEP-悲观剪枝

3、CCP-代价复杂度剪枝

4、MEP-最小错误剪枝



***REP(Reduced Error Pruning)方法***

对于决策树T 的每棵非叶子树S , 用叶子替代这棵子树. 如果 S 被叶子替代后形成的新树关于D 的误差等于或小于S 关于 D 所产生的误差, 则用叶子替代子树S

![img](https://gitee.com/chengbo123/images/raw/master/v2-ff11945f2e5a8319d82ab53c363ef441_r.jpg)

***优点：***

•REP 是当前最简单的事后剪枝方法之一。

•它的计算复杂性是线性的。

•和原始决策树相比，修剪后的决策树对未来新事例的预测偏差较小。



***缺点：***

•但在数据量较少的情况下很少应用. REP方法趋于过拟合( overfitting) , 这是因为训练数据集中存在的特性在剪枝过程中都被忽略了, 当剪枝数据集比训练数据集小得多时 , 这个问题特别值得注意.



***PEP(Pessimistic Error Pruning)方法***

•为了克服 R EP 方法需要独立剪枝数据集的缺点而提出的, 它不需要分离的剪枝数据集，为了提高对未来事例的预测可靠性, PEP 方法对误差估计增加了连续性校正(continuity correction)。



![img](https://gitee.com/chengbo123/images/raw/master/v2-db6a733b52382964151850986b8af729_r.jpg)

![img](https://gitee.com/chengbo123/images/raw/master/v2-8aa168c3c980f569afcd1592c9822575_b.jpg)

![img](https://pic4.zhimg.com/v2-5b6048c323131815722f4aac33caf993_r.jpg)

e( t) 为节点 t 处误差; i 为覆盖 Tt 的叶子; Nt 为子树 T t 的叶子数; n( t) 为在节点 t 处训练事例的数.



前置知识：

在大样本条件下，样本比例

![img](https://pic3.zhimg.com/v2-75faaa12d748002e2a3dd37462f66b66_b.jpg)

近似正态分布，期望为

![img](https://pic4.zhimg.com/v2-a8c34400523438330feb59422b02c703_b.jpg)

，方差为



![img](https://pic1.zhimg.com/v2-51deea6aec0d19edda83cbabe6ace3ec_b.jpg)

，通过置信区间知识，可以得到

![img](https://pic3.zhimg.com/v2-0d6492914708c46e9d99ce7ff214f29e_b.jpg)







我个人就是这么理解公式的。



还有，公式里1/2是修正因子，突然冒出来看得很奇怪。我找了好久找到一张图。

我这里给出一种理解方式：当做二项分布的连续性校正（最后分类只有对和错两种情况。）

![img](https://pic2.zhimg.com/v2-2de0604fca77a169457ca306edc68c85_r.jpg)



实例：

![img](https://pic2.zhimg.com/v2-b1552444653519f843281f246d4dd12d_r.jpg)

![img](https://pic1.zhimg.com/v2-433eaf006437646241e831f72f6ea8a0_r.jpg)

举个计算例子：e’(t4)： 4+0.5=4.5；e’(Tt4) ： (1+0.5)+(2+0.5)= 4



***优点：***

•PEP方法被认为是当前决策树事后剪枝方法中精度较高的算法之一

•PEP 方法不需要分离的剪枝数据集, 这对于事例较少的问题非常有利

•它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 .



***缺点：***

PEP是唯一使用自顶向下剪枝策略的事后剪枝方法, 这种策略会带来与事前剪枝方法出 现的同样问题, 那就是树的某个节点会在该节点的子孙根据同样准则不需要剪裁时也会被剪裁。



***TIPS：***

个人认为，其实以时间复杂度和空间复杂度为代价，PEP是可以自下而上的，这并不是必然的。



***MEP(Minimum Error Pruning)方法***

MEP 方法的基本思路是采用自底向上的方式, 对于树中每个非叶节点, 首先计算该节点的误差 Er(t) . 然后, 计算该节点每个分枝的误差Er(Tt) , 并且加权相加, 权为每个分枝拥有的训练样本比例. 如果 Er(t) 大于 Er(Tt) , 则保留该子树; 否则, 剪裁它.

假设：***assumes that all the classes are equally likely.\***

n(t) 为节点 t 中的样本总数; n c( t )为 t 中主类的样本数目; k 为类数目 .

![img](https://pic1.zhimg.com/v2-bff66fcf2370f78b060d98f15deb0100_r.jpg)

（个人认为这种算法和PEP相似，也是引入了修正因子）

![img](https://pic4.zhimg.com/v2-60db82eb9b10070bd2d1032f79baa44f_r.jpg)

![img](https://pic1.zhimg.com/v2-9976ab0ca9ab401f597fa81a9f1f1914_r.jpg)

***优点：***

•MEP方法不需要独立的剪枝数据集, 无论是初始版本, 还是改进版本, 在剪枝过程中, 使用的信息都来自于训练样本集.•它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 .



***缺点：***

类别平均分配的前提假设现实几率不大&对K太过敏感

![img](https://pic3.zhimg.com/v2-5e7deee0ee978be2eec60328192affc6_r.jpg)



对此，也有改进算法，我没有深入研究。

![img](https://pic1.zhimg.com/v2-3ffc529242dbb52dcb4946e37fed92f0_r.jpg)





***CCP(Cost-Complexity Pruning)方法***

•CCP 方法就是著名的CART(Classificationand Regression Trees)剪枝算法，它包含两个步骤:
(1) 自底向上，通过对原始决策树中的修剪得到一系列的树 {T0,T1,T2,...,Tt}， 其中Tia 是由Ti中的一个或多个子树被替换所得到的，T0为未经任何修剪的原始树，几为只有一个结点的树。

(2) 评价这些树，根据真实误差率来选择一个最优秀的树作为最后被剪枝的树。



α的理解：

实际上, 当 1 棵树 T 在节点 t 处剪枝时, 它的误差增加直观上认为是R(t) - R (Tt) , 其中, R ( t) 为在节点 t 的子树被裁剪后节点t 的误差, R(Tt) 为在节点 t 的子树没被裁剪时子树Tt的误差. 然而, 剪枝后, T的叶子数减少了L(Tt) - 1, 其中, L(Tt) 为子树Tt的叶子数, 也就是说,T的复杂性减少了. 因此, 考虑树的复杂性因素, 树分枝被裁剪后误差增加率由 下式决定:

![img](https://pic1.zhimg.com/v2-0deb97089e868e7fdba622a840afdd20_r.jpg)

![img](https://pic2.zhimg.com/v2-cdc61b91d1606d5e5d501d35519d3731_r.jpg)

![img](https://pic4.zhimg.com/v2-60db82eb9b10070bd2d1032f79baa44f_r.jpg)

![img](https://pic4.zhimg.com/v2-be6ae6bb2d41a74387d4d1de709ed59b_r.jpg)

举个计算例子：α(t4)=(4-(1+2)/80)/2-1=0.0125



好的，现在我们有了一系列的子树，如何选择呢？

即根据预测精度， 如何从{To,T1,T2,...,Tt}中选择一棵最佳的树。

作者提供了两种截然不同的估计真实误差的方法。其一是***基于交叉验证\***， 另一种基于***独立剪枝数据集\***。

•独立数据集类似REP。如果使用独立的剪枝数据集，CCP 明显比 REP 方法存在一个缺点，它只能从{To,Ti,T2,...}中选择最佳决策树，而不是从原始树中所有可能的子树中获到 。

•k-cv，即k番交叉验证，即将数据集分成k分，前k-1生成树，最后1份选择最优树，重复K次，再从这K个子树中选取最优的子树。



***缺点：***

生成子树序列 T ( α) 所需要的时间和原决策树非叶节点的关系是二次的, 这就意味着如果非叶节点的数目随着训练例子记录数目线性增加, 则CCP方法的运行时间和训练数据记录数的关系也是二次的 . 这就比本文中将要介绍的其它剪枝方法所需要的时间长得多, 因为其它剪枝方法的运行时间和非叶节点的关系是线性的.





***对比四种方法***：



![img](https://pic2.zhimg.com/v2-87a2b4cf95df2e73ce6ba5e10162b8dd_r.jpg)







①MEP比PEP不准确，且树大。两者都不需要额外数据集，故当数据集小的时候可以用。对比公式，如果类（Label）多，则用MEP；PEP在数据集uncertain时错误多，不使用。

②REP最简单且精度高，但需要额外数据集；CCP精度和REP差不多，但树小。

③如果数据集多（REP&CCP←复杂但树小）

④如果数据集小（MEP←不准确树大&PEP←不稳定）



————————————————————————————————

资料来源：

《统计学习方法》

《An Empirical Comparison of Pruning Methods
for Decision Tree Induction》

《决策树学习及其剪枝算法研究》

《决策树剪枝方法的比较-魏红宁》



来源：谢小娇包教包会决策树之决策树剪枝 - 「已注销」的文章 - 知乎 https://zhuanlan.zhihu.com/p/30296061