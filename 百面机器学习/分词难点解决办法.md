# 1.未登录词处理

最原始的做法是用 UNK 标签表示所有未登录词，但是 UNK 的 embedding 一般不会用零向量。

第二种方法

我觉得最容易想到的方法，使用 sub-word level embedding。比如大名鼎鼎的 [fastText](https://fasttext.cc/)，通过 character n-gram 组合出 word embedding，不存在 OOV 的问题。官网能找到有很多语种的 pre-trained embedding，自己训练起来也不难，网上也有很多 blog、tutorial 可参考。而且感觉常用度不比 word2vec、GloVe 差。

- 论文：[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606) Bojanowsk et al.
- 代码：[facebookresearch/fastText](https://github.com/facebookresearch/fastText/)



第三种方法（**重点！**）

也是我看到题目时脑海中涌现的第一个想法，来源于论文 "Mimicking Word Embeddings using Subword RNNs." by Yuval Pinter, Robert Guthrie, and Jacob Eisenstein. 本文发表在 EMNLP 2017 上，专门解决 word embedding 的 OOV 问题。论文提出了 MIMICK 模型，在已有 embedding 上根据 subword 信息学习一个从拼写到 embedding 的 function 来为 OOV words 生成 embedding。 作者还在 [Universal Dependencies](https://universaldependencies.org/) (UD) 语料库的 23 种语言上测试了 MIMICK 在词性标注任务 (POS tagging) 中的准确率，并得到了不错的结果。而且 MIMICK 在中文上有着突出的表现。原因是 UD 的中文语料库包含了 >12000 的 token，可供 MIMICK 充分地学习出 embedding 的方法。另外 MIMICK 在低资源语言 (low-resource languages) 处理上有着重要的作用。通常情况下，用来训练的语料库资源有限，而 MIMICK 可以发挥自身优势，充分利用现有数据生成 word embedding ，完成下游任务。

- 论文：[Mimicking Word Embeddings using Subword RNNs](https://arxiv.org/abs/1707.06961)
- 代码：[yuvalpinter/Mimick](https://github.com/yuvalpinter/Mimick)



第四种方法

鉴于题主所研究的问题是 POS tagging，我还想到了一个更加 "end-to-end" 的方法，出自 ICLR 2016 的论文 "Segmental Recurrent Neural Network" by Lingpeng Kong, Chris Dyer, and Noah A. Smith. 本文提出一个 RNN 模型用于对输入的序列进行分割和标记。SRNN 可以对分割所得的片段学习出 segment embeddings 并在此基础上推断出片段标签。作者将 SRNN 在 [Penn Chinese Treebank](https://catalog.ldc.upenn.edu/LDC2013T21) 数据集上进行中文分词和词性标注任务的评估，并打败了作为 benchmark 的 BiRNN。当然 SRNN 应该只是众多可用于中文词性标注问题的模型之一，我相信现在一定有很多大更加强大的模型在中文词性标注上有更加优秀的表现。

- 论文：[Segmental Recurrent Neural Networks](https://arxiv.org/abs/1511.06018)
- 代码曾在 [DyNet](https://dynet.readthedocs.io/en/latest/) 老版本中作为 [example](https://github.com/clab/dynet/tree/master/examples/segmental-rnn) 出现过，现在 2.0 版本已经不支持了。如果有哪位大佬实现过本文的方法，求求你们不要学 Closed-AI 把代码开源吧！谢谢！



来源：https://www.zhihu.com/question/308543084



# 2.语义歧义消除

**语义消歧** 可以看作分类问题。一个词W有K个含义，对W消歧 就是确定W在特定句子中究竟使用了哪一个含义，即把W分到K类中的一个。分类的依据则是和W邻近的词，即W的上下文C。

歧义可以分为两类：一类是词的语义有多种，如“bank”，可以是银行，也可以是河岸；另一类是词本身的词性也是多样的，如predicate，既能作为名字，也能作为动词。对于前者，

可能需要与W相隔较远的其他词参与消歧，而对于后者，往往通过邻近的词汇就能确定W的词性了。



常用的消歧方法：

**一、有监督消歧**

   \1. 贝叶斯分类

​      s = arg max p(Sk|c), Sk 是W可能包含的语义，C是歧义词的上下文，而s是 使该概率最大的语义，即消歧后确定的语义。

   \2. 基于信息论的方法。以W包含2个语义为例，基本思想是最大化 互信息 I(P,Q)，P是W的语义集，Q是W的指示器取值集（指示器 即能区分W不同语义的关键邻近词）。

​     例：法语“ prendre”的含义是take或make，其指示器可以是 decision,note,example,measure。P划分为 p1 = {take，}和p2={make，}，

​         Q分为Q1 = {note,example,measure,}和Q2 = {decision}, 如果W的指示器为note，出现在Q1中，那么W对应的语义应该对应地出现在P1中，即take。

​          在这里，P和Q的集合划分的原则是最大化 I(P,Q)。

​     该方法感觉和贝叶斯分类本质上类似，还是基于邻近词，根据概率判决，只是具体的公式不一样。

​    

二、 **基于词典的消歧（本质上也是无监督消歧的一种**）

​     \1. 基于语义定义的消歧。如果词典中对W的 第i种定义 包含 词汇Ei，那么如果在一个包含W的句子中，同时也出现了Ei，那么就认为 在该句子中 W的语义应该取词典中的第i 

​       种定义。

​     \2. 基于类义辞典的消歧。 词的每个语义 都定义其对应的主题或范畴（如“网球”对应的主题是“运动”），多个语义即对应了多个主题。如果W的上下文C中的词汇包含多个主          题，则取其频率最高的主题，作为W的主题，确定了W的主题后，也就能确定其对应的语义。

​     \3. 基于双语对比的消歧。这种方法比较有创意，即把一种语言作为另一种语言的定义。例如，为了确定“interest”在英文句子A中的含义，可以利用句子A的中文表达，因为 

​       interest的不同语义在中文的表达是不同的。如果句子A对应中文包含“存款利率”，那么“interest”在句子A的语义就是“利率”。如果句子A的对应中文是“我对英语没有兴趣”，

​       那么其语义就是“兴趣”。



**三、无监督消歧**

​      主要是使用EM算法 对W的上下文C进行无监督地聚类，也就是对 W的语义进行了分类。（当然，该分类的结果不见得就是和词典中对该词的定义分类是匹配的）。