原文：[A Survey on Deep Learning for Named Entity Recognition](https://arxiv.org/abs/1812.09449)

## 1.介绍

**命名实体识别(NER)**旨在从文本中识别出特殊对象，这些对象的语义类别通常在识别前被预定义好，预定义类别如人、地址、组织等。命名实体识别不仅仅是独立的信息抽取任务，它在许多大型自然语言处理应用系统如信息检索、自动文本概要、问答任务、机器翻译以及知识建库（知识图谱）中也扮演了关键的角色。

### **1.1命名实体分类：**

1. 广义命名实体（人，地址等）
2. 领域命名实体（以生物领域为例：蛋白质，酶，基因等）

### 1.2方法分类：

1. 基于规则的方法
2. 无监督方法
3. 基于特征的监督学习方法
4. 深度学习方法

上述分类法并非泾渭分明的，比如某些深度学习方法也结合了一些研究者设计的特征来提高识别的准确率。

### 1.3形式化定义

给定标识符集合 ![[公式]](https://www.zhihu.com/equation?tex=s+%3D+%5Clangle+w_1%2Cw_2%2C...w_N%5Crangle) ，NER 输出一个三元组 ![[公式]](https://www.zhihu.com/equation?tex=%5Clangle+I_s%2CI_e%2Ct%5Crangle) 的列表，列表中的每个三元组代表 ![[公式]](https://www.zhihu.com/equation?tex=s) 中的一个命名实体。此处 ![[公式]](https://www.zhihu.com/equation?tex=I_s+%5Cin%5B1%2C+N%5D) ， ![[公式]](https://www.zhihu.com/equation?tex=I_e+%5Cin%5B1%2C+N%5D) ，分别为命名实体的起始索引以及结束索引；t 指代从预定义类别中选择的实体类型。例子如下：

![img](https://pic3.zhimg.com/80/v2-aab9cbbdde3b075cd4d87cef783e1012_720w.jpg)

### 1.4NER任务分类

1. 粗粒度的NER（实体种类少，每个命名实体对应一个实体类型）
2. 细粒度的NER（实体种类多，每个命名实体可能存在多个对应的实体类型）

值得一提的是，NER任务从粗到细的变化与标注数据集从小到大的发展密切相关

## 2.背景

### 2.1数据集

有监督方法的NER任务依赖标注数据集。2005 年之前，数据集主要通过标注新闻文章得到并且预定义的实体种类少，这些数据集适合用于粗粒度的NER任务; 2005 年之后，数据集来源越来越多，包括但不限于维基百科文章、对话、用户生成语料（如推特等社区的用户留言）等，并且预定义的实体类别也多了许多，以数据集 OneNotes 为例，其预定义的实体类别达到了89种之多。

所有数据集中，**最常见的数据集为 CoNLL03 和 OneNotes，**分别常见于粗粒度的NER任务和细粒度的NER任务。

常见的数据集列举如下：

![img](https://pic4.zhimg.com/v2-dd7d73f8ebcfed131b04eee523573433_r.jpg)

### 2.2工具

现成的NER工具来源于学界、工业界以及开源项目。列举如下：

![img](https://pic1.zhimg.com/80/v2-821e909ce55c059df342d20f6075907c_720w.jpg)

## 3.评估标准

通常通过与人类标注水平进行比较判断NER系统的优劣。评估分两种：

1. 精确匹配评估
2. 宽松匹配评估

### 3.1精确匹配评估

NER任务需要同时确定**实体边界**以及**实体类别。**在精确匹配评估中，只有当实体边界以及实体类别同时被精确标出时，实体识别任务才能被认定为成功。基于数据的 true positives（TP），false positives（FP），以及false negatives（FN），可以计算NER任务的精确率，召回率以及 F-score 用于评估任务优劣。对NER中的 true positives（TP），false positives（FP）与false negatives（FN）有如下解释：

- true positives（TP）：NER能正确识别实体
- false positives（FP）：NER能识别出实体但类别或边界判定出现错误
- false negatives（FN）：应该但没有被NER所识别的实体

**精确率**： ![[公式]](https://www.zhihu.com/equation?tex=Precision%3D%5Cfrac%7BTP%7D%7BTP%2BFP%7D)

**召回率**： ![[公式]](https://www.zhihu.com/equation?tex=Recall+%3D+%5Cfrac%7BTP%7D%7BTP%2BFN%7D)

F-score：F-score 是精确率和召回率的调和平均，最常用的 F-score 是： ![[公式]](https://www.zhihu.com/equation?tex=2%5Ctimes%5Cfrac%7BPrecision%5Ctimes+Recall%7D%7BPrecision%2BRecall%7D)

绝大多数的NER任务需要识别多种实体类别，需要对所有的实体类别评估NER的效果。基于这个思路，有两类评估指标：

1. 宏平均 F-score（macro-averaged F-score）：分别对每种实体类别分别计算对应类别的 F-score，再求整体的平均值（将所有的实体类别都视为平等的）
2. 微平均 F-score（micro-averaged F-score）：对整体数据求 F-score（将每个实体个体视为平等的）

### 3.2宽松匹配评估

[MUC-6](http://dl.acm.org/ft_gateway.cfm?id=992709&type=pdf) 定义了一种宽松匹配评估标准：只要实体的边界与实体真正所在的位置有重合（overlap）且实体类别识别无误，就可以认定实体类别识别正确；对实体边界的识别也不用考虑实体类别识别的正确与否。与精确匹配评估相比，宽松匹配评估的应用较少。

## 4.传统的NER方法

简单地介绍几类传统的NER方法。

### 4.1基于规则的方法

基于规则的NER系统依赖于人工制定的规则。规则的设计一般基于句法、语法、词汇的模式以及特定领域的知识等。当字典大小有限时，基于规则的NER系统可以达到很好的效果。由于特定领域的规则以及不完全的字典，这种NER系统的特点是高精确率与低召回率，并且类似的系统难以迁移应用到别的领域中去：基于领域的规则往往不通用，对新的领域而言，需要重新制定规则且不同领域字典不同。

### 4.2无监督学习方法

典型的无监督方法如聚类可以利用语义相似性，从聚集的组中抽取命名实体。其核心思路在于利用基于巨大语料得到的词汇资源、词汇模型、统计数据来推断命名实体的类别。

### 4.3基于特征的有监督学习方法

利用监督学习，NER任务可以被转化为多分类任务或者序列标注任务。根据标注好的数据，研究者应用领域知识与工程技巧设计复杂的特征来表征每个训练样本，然后应用机器学习算法，训练模型使其对数据的模式进行学习。

## 5.深度学习方法

### 5.1为什么使用深度学习方法

三个主要的优势：

1. NER可以利用深度学习非线性的特点，从输入到输出建立非线性的映射。相比于线性模型（如线性链式CRF、log-linear隐马尔可夫模型），深度学习模型可以利用巨量数据通过非线性激活函数学习得到更加复杂精致的特征。
2. 深度学习不需要过于复杂的特征工程。传统的基于特征的方法需要大量的工程技巧与领域知识；而深度学习方法可以从输入中自动发掘信息以及学习信息的表示，而且通常这种自动学习并不意味着更差的结果。
3. 深度NER模型是端到端的；端到端模型的一个好处在于可以避免流水线（pipeline）类模型中模块之间的误差传播；另一点是端到端的模型可以承载更加复杂的内部设计，最终产出更好的结果。

### 5.2模型分类法

文章针对现有的深度NER模型提出了一种新的归纳方法。这种归纳法将深度NER系统概括性的分为了三个阶段：

1. 输入的分布式表示（distributed representation）
2. 语境语义编码（context encoder）
3. 标签解码（tag decoder）

一个深度NER系统的结构示例如下：

![img](https://pic4.zhimg.com/80/v2-bcf63fd3ed42b9b8b9c3ce1e5b908adb_720w.jpg)

## 6.输入的分布式表示

**分布式语义表示**：一个单词的含义是由这个单词常出现的语境（上下文）所决定的

一种直接粗暴的单词表示方法为 one-hot 向量表示。这种方法通常向量的维度太大，极度稀疏，且任何两个向量都是正交的，无法用于计算单词相似度（见 [CS224N lecture1笔记](https://zhuanlan.zhihu.com/p/60177212)）。**分布式表示**使用低维度稠密实值向量表示单词，其中每个维度表示一个隐特征（此类特征由模型自动学习得到，而非人为明确指定，研究者往往不知道这些维度到底代表的是什么具体的特征）。这些分布式表示可以自动地从输入文本中学习得到重要的信息。深度NER模型主要用到了三类分布式表示：

1. 单词级别表示
2. 字符级别表示
3. 混合表示

### 6.1单词级别的分布式表示

通常经过训练，每个单词可以用一个低维度的实值向量表示。

常见的词嵌入如下：

1. [谷歌 word2vec](https://code.google.com/archive/p/word2vec/)
2. [斯坦福 GloVe](http://nlp.stanford.edu/projects/glove/)
3. [Facebook fastText](https://fasttext.cc/docs/en/english-vectors.html)
4. [SENNA](https://ronan.collobert.com/senna/)

作为后续阶段的输入，这些词嵌入向量既可以在预训练之后就固定，也可以根据具体应用场景进行调整。

**典型的应用**：

- [Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme](https://arxiv.org/abs/1706.05075) 利用word2vec 为端到端的联合抽取模型学习得到单词表示作为模型输入
- [Fast and accurate entity recognition with iterated dilated convolutions](https://arxiv.org/abs/1702.02098) 中整个系统的初始化部分中利用了 skip-n-gram 方法在 SENNA 语料上训练得到维度为 100 的词嵌入向量

### 6.2字符级别的表示

字符级别的表示能更有效地利用**次词级别**信息如前缀、后缀等。其另一个好处在于它可以很好地处理 out-of-vocabulary 问题。字符级别的表示可以对没有见过的（训练语料中未曾出现的）单词进行合理推断并给出相应的表示，并在语素层面上共享、处理信息（**语素**：最小的的音义结合体）。主流的抽取字符级别表示的结构分为：

1. 基于 CNN 的结构
2. 基于 RNN 的结构

结构示例如下：

![img](https://pic4.zhimg.com/v2-84be11c68ed01c1b3b73e0f7216a88bb_r.jpg)

**典型例子如下**：

- 基于 CNN 的方法：[Deep contextualized word representations(ELMo)](https://arxiv.org/abs/1802.05365)
- 基于 RNN 的方法：[CharNER](http://www.aclweb.org/anthology/C/C16/C16-1087.pdf)

### 6.3混合表示

某些单词表示研究还结合了一些其他信息，例如句法信息、词法信息、领域信息等。这些研究将这些附加的信息与单词表示或字符级别的表示相结合作为最终的单词表示，之后再作为输入输入到后续的语义编码结构当中。换而言之，这种方法的本质是将基于深度学习的单词表示与基于特征的方法相结合。这些额外的信息可能可以提升NER系统的性能，但是代价是可能会降低系统的通用性与可迁移性。

**典型例子如下：**

1. [BERT](http://arxiv.org/abs/1810.04805)

## 7.语义编码结构

基于深度学习的NER系统的第二步时利用输入表示学习语义编码。常见的语义编码结构有：

1. 卷积神经网络（convolutional neural network）
2. 循环神经网络（recurrent neural network）
3. 递归神经网络（recursive neural network）
4. 神经语言模型
5. transformer

**如无特殊说明，文中的 RNN 一般指循环（recurrent）神经网络。**

### 7.1卷积神经网络

如图是一个经典的基于 CNN 的句子处理方法：

![img](https://pic3.zhimg.com/80/v2-4f89427bbd086a8e14b907d6c3c0a012_720w.jpg)

输入表示阶段，输入序列中的每一个词都被嵌入一个 N 维的向量。在这之后，系统利用卷积神经网络来产生词间的局部特征，并且此时卷积神经网络的输出大小还与输入句子的大小有关。随后，通过对该局部特征施加极大池化（max pooling）或者平均池化（average pooling）操作，我们能得到大小固定且与输入相互独立的全局特征向量。这些长度大小固定的全局特征向量之后将会被导入标签解码结构中，分别对所有可能的标签计算相应的置信分数，完成对标签的预测。

### 7.2循环神经网络

循环神经网络在处理序列输入时效果优秀，它有两个最常见的变种：

1. GRU（gated recurrent unit）
2. LSTM（long-short term memory）

特别的，双向循环神经网络（bidirectional RNNs）能同时有效地利用过去的信息和未来的信息，即可以有效利用全局信息。因此，双向循环神经网络逐渐成为解决 NER 这类序列标注任务的标准解法。

**典型例子如下**：

- [Bidirectional lstm-crf models for sequence tagging](https://arxiv.org/abs/1508.01991) 这篇文章是第一个提出用双向 LSTM-CRF 结构来处理序列标注任务的，其结构如下：

![img](https://pic4.zhimg.com/80/v2-3aba410bddcfc9cf9ab8bc1e250b202b_720w.jpg)

### 7.3递归神经网络

递归神经网络是一种非线性自适应的模型，它可以学习得到输入的深度结构化信息。命名实体与某些语言成分联系十分紧密，如名词词组。传统的序列标注方法几乎忽略了句子的结构信息（成分间的结构），而递归神经网络能有效的利用这样的结构信息，从而得出更好的预测结果。

**典型例子如下**：

- Leveraging linguistic structures for named entity recognition with bidirectional recursive neural networks

![img](https://pic2.zhimg.com/80/v2-34dca193f71855e0659a641fcc179901_720w.jpg)

### 7.4神经语言模型

语言模型是一类描述序列生成的模型。给定符号序列 ![[公式]](https://www.zhihu.com/equation?tex=%28t_1%2Ct_2%2C...%2Ct_N%29) ，且已知符号 ![[公式]](https://www.zhihu.com/equation?tex=t_k) 之前的符号为 ![[公式]](https://www.zhihu.com/equation?tex=%28t_1%2C...%2Ct_%7Bk-1%7D%29) ，前向语言模型通过对符号 ![[公式]](https://www.zhihu.com/equation?tex=t_k) 的概率进行建模来计算整个序列的概率：

![[公式]](https://www.zhihu.com/equation?tex=p%28t_1%2C...%2Ct_N%29%3D%5Cprod_%7Bk%3D1%7D%5E%7BN%7Dp%28t_k+%5Cmid+t_1%2Ct_2%2C...%2Ct_k-1%29)

类似的，反向语言模型对整个序列的计算如下：

![[公式]](https://www.zhihu.com/equation?tex=p%28t_1%2C...%2Ct_N%29%3D%5Cprod_%7Bk%3D1%7D%5E%7BN%7Dp%28t_k+%5Cmid+t_%7Bk%2B1%7D%2Ct_%7Bk%2B2%7D%2C...%2Ct_N%29)

而双向语言模型（结合了前向和反向语言模型）同时利用了过去和未来的词的信息来计算当前符号的概率，因而可以很好的利用语境的语义与句法信息

**典型例子：**

- [Semisupervised sequence tagging with bidirectional language models](https://arxiv.org/abs/1705.00108) 文章认为，利用单词级别表示作为输入来产生上下文表示的循环神经网络往往是在相对较小的标注数据集上训练的。而神经语言模型可以在大型的无标注数据集上训练。文中模型同时使用词嵌入模型与神经语言模型对无监督的语料进行训练，得到两种单词表示；之后模型中省去了将输入向量转化为上下文相关向量的操作，直接结合前面得到的两类单词表示并用于有监督的序列标注任务，简化了模型的结构。示例图如下：

![img](https://pic2.zhimg.com/80/v2-b2086a8a750eaaef86fb3b815e2506a1_720w.jpg)

### 7.5深度Transformer

**例子如下**：

- [Attention is all you need](https://arxiv.org/abs/1706.03762)
- [Bert: Pretraining of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)

## 8.标签解码结构

标签解码是NER模型中的最后一个阶段。在得到了单词的向量表示并将它们转化为上下文相关的表示之后，标签解码模块以它们作为输入并对整个模型的输入预测相应的标签序列。主流的标签解码结构分为四类：

1. 多层感知器+Softmax
2. 条件随机场
3. 循环神经网络
4. pointer networks

### 8.1多层感知器+Softmax

利用这个结构可以将NER这类序列标注模型视为多类型分类问题。基于该阶段输入的上下文语义表示，每个单词的标签被独立地预测，与其邻居无关。

**例子如下**：

- [Fast and accurate entity recognition with iterated dilated convolutions](https://arxiv.org/abs/1702.02098)
- Leveraging linguistic structures for named entity recognition with bidirectional recursive neural networks

### 8.2条件随机场

条件随机场（conditional random fields）是一类概率图模型，在基于特征的有监督方法中应用广泛，近来的许多深度学习方法也使用条件随机场作为最后的标签标注结构。原文对其缺点描述如下：

> Cannot make full use of segment-level information because the inner properties of segments cannot be fully encoded with word-level representations.

**例子如下**：

- [Bidirectional lstm-crf models for sequence tagging](https://arxiv.org/abs/1508.01991)

### 8.3循环神经网络

一些研究使用 RNN 来预测标签。**例子**如下：

- [Deep active learning for named entity recognition](https://arxiv.org/abs/1707.05928) 这篇文章中提到，RNN 模型作为预测标签的解码器性能优于 CRF，并且当实体类型很多的时候训练速度更快

### 8.4Pointer Networks

- [Pointer Networks](http://arxiv.org/pdf/1506.03134v1.pdf) 首次提出此种结构
- [Neural Models for Sequence Chunking](http://arxiv.org/pdf/1701.04027) 第一篇将pointer networks结构应用到生成序列标签任务中的文章