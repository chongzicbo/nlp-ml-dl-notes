# 1.概述

BERT、RoBERTa、XLNET等模型参数数量大、训练时间过长，ALBERT最小的参数只有十几M,效果跟BERT相差不大。在模型参数量上减少明显，但是在速度上没有明显增快。最大的问题是这种方式没有减少计算量，也就是推理时间并没有减少，训练时间的减少也有待商榷。

# 2.模型详解

整体的模型结构跟BERT相似，采用Transformer和GELU激活函数。主要的创新有三点：

* 将Embedding的参数进行了因式分解

* 跨层的参数共享

* 抛弃NSP任务，使用SOP任务

  

  前两点目的是减少参数，第三点已经有很多工作发现BERT的NSP任务并没有积极的影响。参数共享对参数数量的减少影响比较大，同时也会影响模型的整体效果。

  

## 2.1 Embedding参数因式分解

原始的BERT模型以及各种依据transformer来搞的预训练语言模型在输入的地方我们会发现它的E是等于H的，其中E就是embedding size，H就是hidden size，也就是transformer的输入输出维度。这就会导致一个问题，当我们的hidden size提升的时候，embedding size也需要提升，这就会导致我们的embedding matrix维度的提升。所以这里作者将E和H进行了解绑，具体的操作其实就是在embedding后面加入一个矩阵进行维度变换。E是永远不变的，后面H提高了后，我们在E的后面进行一个升维操作，让E达到H的维度。这使得embedding参数的维度从O(V×H)到了O(V×E + E×H), 当E远远小于H的时候更加明显。

## 2.2 跨层参数共享

之前transformer的每一层参数都是独立的，包括self-attention 和全连接，这样的话当层数增加的时候，参数就会很明显的上升。之前有工作试过单独的将self-attention或者全连接进行共享，都取得了一些效果。这里作者尝试将所有的参数进行共享，这其实就导致多层的attention其实就是一层attention的叠加。同时作者通过实验还发现了，使用参数共享可以有效地提升模型的稳定程度。实验结果如下图：

![](https://pic3.zhimg.com/v2-85533260658fa30a0b527003208ba2f2_r.jpg)

## 2.3 SOP任务

这里作者使用了一个新的loss，其实就是更改了原来BERT的一个子任务NSP, 原来NSP就是来预测下一个句子的，也就是一个句子是不是另一个句子的下一个句子。这个任务的问题出在训练数据上面，正例就是用的一个文档里面连续的两句话，但是负例使用的是不同文档里面的两句话。这就导致这个任务包含了主题预测在里面，而主题预测又要比两句话连续性的预测简单太多。新的方法使用了sentence-order prediction(SOP), 正例的构建和NSP是一样的，不过负例则是将两句话反过来。实验的结果也证明这种方式要比之前好很多。但是这个这里应该不是首创了，百度的ERNIE貌似也采用了一个这种的。

# 3. 实验

![](https://pic3.zhimg.com/v2-7d7e084f92cb8b7a0590a945f3933506_r.jpg)

可以看到ALBERT的与参数的减少还是非常的显著的，同时我们也可以看出来，似乎参数共享对参数量减少的作用更大一点。

* 整体与BERT对比

  ![](https://pic1.zhimg.com/v2-30de5f162f71f7796995868923c3d7a8_r.jpg)

首先常规的提升我们都能看到，参数量巨大的减少以及效果的提升。但是我们可以看到ALBERT xxlarge只有233M的参数，而BERT xlarge足足有1270M的参数，但是在训练速度上之快了0.2。这个模型其实在计算效率上并没有很大的提升。

* Embedding参数因式分解的影响

  ![](https://pic4.zhimg.com/v2-f86f7861290f0eaa607e2b690977df6f_r.jpg)

对于参数不共享的版本，随着E的增加，效果是不断提升的。但是在参数共享的版本似乎不是这样的，效果最好的版本并不是E最大的版本。同时我们也可以发现参数共享对于效果可能带来1-2个点的下降

* 多层参数共享的影响

  ![](https://pic1.zhimg.com/v2-98fdb92b16b2351fcaa8f7936559e0c8_r.jpg)

可以看到对于E=768的版本，参数不共享的效果是最好的，但是对于E=128的版本，只共享attention确实最好的。这里作者只是指出了这个情况，但是没有分析具体的原因是为啥。

* SOP任务的影响

  ![](https://pic3.zhimg.com/v2-20554a144692d53a88ffedf4b54a524a_r.jpg)

* 网络深度和宽度的影响

  ![](https://pic4.zhimg.com/v2-70dc2b7013fb66eaedfd82131ac83e03_r.jpg)

可以看出来，随着网络的加深和加宽，模型效果是越来越好的。

# 4.结论

总体来看这个模型并没有那么的让人惊喜，虽然在各个数据及上有更好的效果而且参数量下降了。但是XXlarge版本的计算量还是非常大的，这就意味着其在训练和推理上要花更多的时间，作者也说了未来的工作就是提升模型的推理速度。

