## 模型评估方法

### Accuracy的局限

![Accuracy=\frac{N_{correct}}{N_{total}}](https://math.jianshu.com/math?formula=Accuracy%3D%5Cfrac%7BN_%7Bcorrect%7D%7D%7BN_%7Btotal%7D%7D)

当正负样本极度不均衡时存在问题！比如，正样本有99%时，分类器只要将所有样本划分为正样本就可以达到99%的准确率。但显然这个分类器是存在问题的。

当正负样本不均衡时，常用的评价指标为ROC曲线和PR曲线。

### ROC(receiver operating characteristic)曲线

ROC接受者操作特征曲线，其显示的是分类器的TPR和FPR之间的关系，如下图所示：



![img](https:////upload-images.jianshu.io/upload_images/10890732-0bd65b811589eb54.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/519/format/webp)

img

![TPR=\frac{TP}{P}=\frac{TP}{TP+FN}](https://math.jianshu.com/math?formula=TPR%3D%5Cfrac%7BTP%7D%7BP%7D%3D%5Cfrac%7BTP%7D%7BTP%2BFN%7D)

![FPR=\frac{FP}{N}=\frac{FP}{FP+TN}](https://math.jianshu.com/math?formula=FPR%3D%5Cfrac%7BFP%7D%7BN%7D%3D%5Cfrac%7BFP%7D%7BFP%2BTN%7D)

常用于二分类问题中的模型比较。具体方法是在不同的分类阈值 (threshold) 设定下分别以TPR和FPR为纵、横轴作图。**曲线越靠近左上角，意味着越多的正例优先于负例，模型的整体表现也就越好**。

#### 优点

- 兼顾正例和负例的权衡。因为**TPR聚焦于正例，FPR聚焦于与负例**，使其成为一个**比较均衡的评估方法**。
- ROC曲线选用的两个指标， **TPR和FPR，都不依赖于具体的类别分布**。

#### 缺点

- ROC曲线的**优点是不会随着类别分布的改变而改变**，但这在某种程度上也是其缺点。因为负例N增加了很多，而曲线却没变，这等于产生了大量FP。像信息检索中如果主要关心正例的预测准确性的话，这就不可接受了。
- 在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。ROC曲线的横轴采用FPR，根据公式 ，当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。结果是**虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来**。（当然也可以只分析ROC曲线左边一小段）

### PR（Precision - Recall）曲线

PR曲线与ROC曲线的**相同点是都采用了TPR (Recall)，都可以用AUC来衡量分类器的效果**。**不同点是ROC曲线使用了FPR，而PR曲线使用了Precision**，因此PR曲线的两个指标都聚焦于正例。类别不平衡问题中由于主要关心正例，所以在此情况下PR曲线被广泛认为优于ROC曲线。

![recall=\frac{TP}{TP+FN}](https://math.jianshu.com/math?formula=recall%3D%5Cfrac%7BTP%7D%7BTP%2BFN%7D)

![precision=\frac{TP}{TP+FP}](https://math.jianshu.com/math?formula=precision%3D%5Cfrac%7BTP%7D%7BTP%2BFP%7D)

![img](https:////upload-images.jianshu.io/upload_images/10890732-5fe8a89d01a5c843.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/686/format/webp)

img

### 使用场景

- ROC曲线由于兼顾正例与负例，所以适用于评估分类器的**整体性能**，相比而言PR曲线完全聚焦于正例。
- 如果有多份数据且存在不同的类别分布，比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为类别分布改变可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想测试不同类别分布下对分类器的性能的影响，则PR曲线比较适合。
- 如果想要评估在相同的类别分布下正例的预测情况，则宜选PR曲线。
- 类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线更好。
- 最后可以根据具体的应用，在曲线上找到最优的点，得到相对应的precision，recall，f1 score等指标，去调整模型的阈值，从而得到一个符合具体应用的模型。

![F1=\frac{2TP}{2TP+FN+FP}=\frac{2⋅Precision⋅Recall}{Precision+Recall}](https://math.jianshu.com/math?formula=F1%3D%5Cfrac%7B2TP%7D%7B2TP%2BFN%2BFP%7D%3D%5Cfrac%7B2%E2%8B%85Precision%E2%8B%85Recall%7D%7BPrecision%2BRecall%7D)

### AUC(Area Under the Curve)

可解读为：从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。

所以根据定义：我们最直观的有两种计算AUC的方法

1. 绘制ROC曲线，ROC曲线下面的面积就是AUC的值
2. 假设总共有（m+n）个样本，其中正样本m个，负样本n个，总共有mn个样本对，计数，正样本预测为正样本的概率值大于负样本预测为正样本的概率值记为1，累加计数，然后除以（mn）就是AUC的值

#### 编程实现AUC的计算，并指出复杂度？



```python
def get_roc(pos_prob,y_true):
    pos = y_true[y_true==1]
    neg = y_true[y_true==0]
    threshold = np.sort(pos_prob)[::-1]        # 按概率大小逆序排列
    y = y_true[pos_prob.argsort()[::-1]]
    tpr_all = [0] ; fpr_all = [0]
    tpr = 0 ; fpr = 0
    x_step = 1/float(len(neg))
    y_step = 1/float(len(pos))
    y_sum = 0                                  # 用于计算AUC
    for i in range(len(threshold)):
        if y[i] == 1:
            tpr += y_step
            tpr_all.append(tpr)
            fpr_all.append(fpr)
        else:
            fpr += x_step
            fpr_all.append(fpr)
            tpr_all.append(tpr)
            y_sum += tpr
    return tpr_all,fpr_all,y_sum*x_step         # 获得总体TPR，FPR和相应的AUC
```

排序复杂度：O(log2(P+N))

计算AUC的复杂度：O(P+N)

#### AUC指标有什么特点？放缩结果对AUC是否有影响？(待解答)

### 余弦距离与欧式距离有什么特点？

#### 余弦距离

余弦距离，也称为余弦相似度，是用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小的度量。如果两个向量的方向一致，即夹角接近零，那么这两个向量就相近。

![cos\theta=\frac{<x,y>}{\left \| x \right \|.\left \| y \right \|}](https://math.jianshu.com/math?formula=cos%5Ctheta%3D%5Cfrac%7B%3Cx%2Cy%3E%7D%7B%5Cleft%20%5C%7C%20x%20%5Cright%20%5C%7C.%5Cleft%20%5C%7C%20y%20%5Cright%20%5C%7C%7D)

#### 欧式距离

![d(x,y)=\sqrt{\sum_{i=0}^{N}(x_{i}-y_{i})^2}](https://math.jianshu.com/math?formula=d(x%2Cy)%3D%5Csqrt%7B%5Csum_%7Bi%3D0%7D%5E%7BN%7D(x_%7Bi%7D-y_%7Bi%7D)%5E2%7D)

余弦距离使用两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比欧氏距离，**余弦距离更加注重两个向量在方向上的差异**。

当对向量进行归一化后，欧式距离与余弦距离一致。

![euc=\sqrt{2-2.cos\theta}](https://math.jianshu.com/math?formula=euc%3D%5Csqrt%7B2-2.cos%5Ctheta%7D)

## 基本方法

### 如何划分训练集？如何选取验证集？

- 通常80%为训练集，20%为测试集
- 当数据量较小时（万级别及以下）的时候将训练集、验证集以及测试集划分为6：2：2；若是数据很大，可以将训练集、验证集、测试集比例调整为98：1：1
- 当数据量很小时，可以采用K折交叉验证
- 划分数据集时可采用随机划分法（当样本比较均衡时），分层采样法（当样本分布极度不均衡时）

随机采样



```python
import numpy as np
def split_train_test(data,test_ratio):
    #设置随机数种子，保证每次生成的结果都是一样的
    np.random.seed(42)
    #permutation随机生成0-len(data)随机序列
    shuffled_indices = np.random.permutation(len(data))
    #test_ratio为测试集所占的半分比
    test_set_size = int(len(data)) * test_ratio
    test_indices = shuffled_indices[:test_ratio]
    train_indices = shuffled_indices[test_set_size:]
    #iloc选择参数序列中所对应的行
    return data.iloc[train_indices],data.iloc[test_indices]

#测试
train_set,test_set = split_train_test(data,0.2)
print(len(train_set), "train +", len(test_set), "test")
```

### 什么是偏差和方差？

- 偏差：描述**预测值的期望与真实值之间的差别**，偏差越大说明模型的预测结果越差。

- 方差：描述**预测值的变化范围**。方差越大说明模型的预测越不稳定。

- **高方差过拟合，高偏差欠拟合。**

- 常用交叉验证来权衡模型的方差和偏差。

- 也可以比较均方误差

  

  ![img](https:////upload-images.jianshu.io/upload_images/10890732-e5e6f8884addec1f.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/720/format/webp)

  img

### 什么是过拟合？深度学习解决过拟合的方法有哪些？

过拟合是指模型拟合了训练样本中的噪声，导致**泛化能力差**。

解决方法如下：

- 增加训练数据
- 缩减模型表达能力
- Dropout
- 训练时提前终止
- 集成多种模型

### 解决欠拟合的方法有哪些？

- 增加模型复杂度
- 增加特征
- 减少数据
- 调整模型初始化方式
- 调整学习率
- 集成多种模型

### 深度模型参数调整的一般方法论？

- 学习率：遵循小->大->小原则
- 初始化：选择合适的初始化方式，有预训练模型更好
- 优化器选择：adam比较快，sgd较慢
- loss：回归问题选L2 loss，分类问题选交叉熵
- 可视化
- 从小数据大模型入手，先过拟合，再增加数据并根据需要调整模型复杂度

## 本节参考

- [https://zhuanlan.zhihu.com/p/34655990](https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F34655990)
- [https://www.zhihu.com/question/19640394](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F19640394)
- [https://zhuanlan.zhihu.com/p/48976706](https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F48976706)
- [https://www.zhihu.com/question/20448464](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F20448464)
- [https://www.zhihu.com/question/59201590](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F59201590)
- [https://zhuanlan.zhihu.com/p/29707029](https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F29707029)
- [https://www.zhihu.com/question/25097993](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F25097993)

## 优化方法

### 简述了解的优化器，发展综述？

深度学习优化算法经历了 SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam 这样的发展历程。

### 优化算法框架

首先定义：待优化参数：w，目标函数：f(w)，初始学习率![\theta](https://math.jianshu.com/math?formula=%5Ctheta)。

而后，开始进行迭代优化。在每个epoch t ：

1. 计算目标函数关于当前参数的梯度： ![g_{t}=\bigtriangledown f(w_{t})](https://math.jianshu.com/math?formula=g_%7Bt%7D%3D%5Cbigtriangledown%20f(w_%7Bt%7D))
2. 根据历史梯度计算一阶动量和二阶动量：![m_{t}=\phi(g_{1},g_{2},...gg_{t})，V_{t}=\varphi(g_{1},g_{2},...gg_{t})](https://math.jianshu.com/math?formula=m_%7Bt%7D%3D%5Cphi(g_%7B1%7D%2Cg_%7B2%7D%2C...gg_%7Bt%7D)%EF%BC%8CV_%7Bt%7D%3D%5Cvarphi(g_%7B1%7D%2Cg_%7B2%7D%2C...gg_%7Bt%7D))
3. 计算当前时刻的下降梯度： ![\eta_{t}=\alpha.\frac{m_{t}}{\sqrt{V_{t}}}](https://math.jianshu.com/math?formula=%5Ceta_%7Bt%7D%3D%5Calpha.%5Cfrac%7Bm_%7Bt%7D%7D%7B%5Csqrt%7BV_%7Bt%7D%7D%7D)
4. 根据下降梯度进行更新： ![w_{t+1}=w_{t}-\eta_{t}](https://math.jianshu.com/math?formula=w_%7Bt%2B1%7D%3Dw_%7Bt%7D-%5Ceta_%7Bt%7D)

- SGD：没有动量的概念
- SGDM：加入一阶动量![m_{t}=\beta_{1}.m_{t-1}+(1-\beta1).g_{t}](https://math.jianshu.com/math?formula=m_%7Bt%7D%3D%5Cbeta_%7B1%7D.m_%7Bt-1%7D%2B(1-%5Cbeta1).g_%7Bt%7D)
- NAG：加入牛顿加速 ![g_{t}=\bigtriangledown f(w_{t}-\alpha.m_{t-1}/\sqrt{V_{t-1}})](https://math.jianshu.com/math?formula=g_%7Bt%7D%3D%5Cbigtriangledown%20f(w_%7Bt%7D-%5Calpha.m_%7Bt-1%7D%2F%5Csqrt%7BV_%7Bt-1%7D%7D))
- AdaGrad：加入二阶动量![V_{t}=\sum_{t=1}^{t}g_{t}^2](https://math.jianshu.com/math?formula=V_%7Bt%7D%3D%5Csum_%7Bt%3D1%7D%5E%7Bt%7Dg_%7Bt%7D%5E2)
- AdaDelta：![V_{t}=\beta_{2}.V_{t-1}+(1-\beta_{2})g_{t}^2](https://math.jianshu.com/math?formula=V_%7Bt%7D%3D%5Cbeta_%7B2%7D.V_%7Bt-1%7D%2B(1-%5Cbeta_%7B2%7D)g_%7Bt%7D%5E2)
- Adam： 加入SGDM的一阶动量和AdaDelta二阶动量
- NAdam：Adam加上牛顿加速

#### SGD

- 下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点
- 精调参数，往往能取得更好的效果

#### Adam

- 收敛速度快
- 可能在**训练后期引起学习率的震荡**，导致模型无法收敛
- 自适应学习率算法可能会对**前期出现的特征过拟合**，后期才出现的特征很难纠正前期的拟合效果，可能**错过全局最优解**

### 常用的损失函数有哪些？分别适用于什么场景？

![img](https:////upload-images.jianshu.io/upload_images/10890732-252d9279da263120.png?imageMogr2/auto-orient/strip|imageView2/2/w/486/format/webp)

img

- 均方误差(MSE)是最常用的回归损失函数，计算方法是求预测值与真实值之间的距离的平方和。![MSE=\sum_{i=1}^{n}(y_{i}-y_{i}^p)^2](https://math.jianshu.com/math?formula=MSE%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D(y_%7Bi%7D-y_%7Bi%7D%5Ep)%5E2)

- 平均绝对值误差(MAE)是目标值和预测值之差的绝对值之和。![MAE=\sum_{i=1}^{n}|y_{i}-y_{i}^{p}|](https://math.jianshu.com/math?formula=MAE%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Cy_%7Bi%7D-y_%7Bi%7D%5E%7Bp%7D%7C)

- MSE计算简便，但**MAE对异常点具有更好的鲁棒性**。

- Huber损失，平滑的绝对平均误差，对异常点没有平方误差那么敏感。它在0也可微分。本质上是绝对误差，只是在误差很小时，就变为平方误差。误差降到多小时变为二次误差由超参数δ（delta）来控制。当Huber损失在[0-δ,0+δ]之间时，等价为MSE，而在[-∞,δ]和[δ,+∞]时为MAE。。

  ![img](https://image.jiqizhixin.com/uploads/editor/561e9469-150f-40dc-9586-0be6ec7483df/1529558774147.png)

- Log-Cosh损失，预测误差的双曲余弦对数。基本类似与均方误差，同时不易受异常点影响。![L(y,y^p)=\sum_{i=1}^{n}log(cosh(y_{i}^p-y_{i}))](https://math.jianshu.com/math?formula=L(y%2Cy%5Ep)%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dlog(cosh(y_%7Bi%7D%5Ep-y_%7Bi%7D))) 。![log(cosh(x))](https://math.jianshu.com/math?formula=log(cosh(x)))在x较小时约等于![(x^2)/2](https://math.jianshu.com/math?formula=(x%5E2)%2F2)，在x较大时约等于![abs(x)-log(2)](https://math.jianshu.com/math?formula=abs(x)-log(2))。

- 分位损失，更关注预测区间而不仅仅是预测点。当![\gamma](https://math.jianshu.com/math?formula=%5Cgamma)值小于0.5时，对高估的惩罚更大，使得预测值略低于中值。![L_{\gamma}(y,y_{p})=\sum_{i:y_{i}<y{i}^p}(1-\gamma)|y_{i}-y_{i}^p|+\sum_{i:y_{i}\geq y_{i}^p}\gamma|y_{i}-y_{i}^p|](https://math.jianshu.com/math?formula=L_%7B%5Cgamma%7D(y%2Cy_%7Bp%7D)%3D%5Csum_%7Bi%3Ay_%7Bi%7D%3Cy%7Bi%7D%5Ep%7D(1-%5Cgamma)%7Cy_%7Bi%7D-y_%7Bi%7D%5Ep%7C%2B%5Csum_%7Bi%3Ay_%7Bi%7D%5Cgeq%20y_%7Bi%7D%5Ep%7D%5Cgamma%7Cy_%7Bi%7D-y_%7Bi%7D%5Ep%7C)

### 梯度下降与拟牛顿法的异同？

- 参数更新模式相同
- 梯度下降法利用**误差的梯度**来更新参数，拟牛顿法利用**海塞矩阵的近似**来更新参数
- 梯度下降是**泰勒级数的一阶展开**，而拟牛顿法是**泰勒级数的二阶展开**
- SGD能保证收敛，但是**L-BFGS在非凸时不收敛**

### L1和L2正则分别有什么特点？为何L1稀疏？

- P-norm:![\left \|x \right \|_{p}:=(\sum_{i=1}^{n}|x_{i}|^p)^{\frac{1}{p}}](https://math.jianshu.com/math?formula=%5Cleft%20%5C%7Cx%20%5Cright%20%5C%7C_%7Bp%7D%3A%3D(%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Cx_%7Bi%7D%7C%5Ep)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D) 将向量投影到[0,)范围内，其中只有0向量的norm取到0

- p=1为L1-norm，p=2为L2-norm，用作正则项则对应为L1正则，L2正则

- **L1对异常值更鲁棒**

- L1在0点不可导，计算不方便

- L1没有唯一解

- L1输出稀疏，会把不重要的特征直接置零

- L2计算方便

- **L2对异常值敏感**

- **L2有唯一解**

  ![img](https:////upload-images.jianshu.io/upload_images/10890732-6f35d70eb7aaaf40.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/702/format/webp)

  img

在梯度更新时，不管 L1 的大小是多少（只要不是0）梯度都是1或者-1，所以每次更新时，它都是稳步向0前进。从而导致L1输出稀疏。

### 本节参考：

- [https://www.jiqizhixin.com/articles/2018-06-21-3](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.jiqizhixin.com%2Farticles%2F2018-06-21-3)
- [https://zhuanlan.zhihu.com/p/37524275](https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F37524275)
- [https://www.zhihu.com/question/46441403](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F46441403)
- [https://www.zhihu.com/question/26485586](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F26485586)

## 深度学习基础

### 以一层隐层的神经网络，relu激活，MSE作为损失函数推导反向传播

[http://galaxy.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html](https://links.jianshu.com/go?to=http%3A%2F%2Fgalaxy.agh.edu.pl%2F%7Evlsi%2FAI%2Fbackp_t_en%2Fbackprop.html)

[https://blog.csdn.net/login_sonata/article/details/76737482](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Flogin_sonata%2Farticle%2Fdetails%2F76737482)

### NN的权重参数能否初始化为0？

不能，可能导致模型无法收敛

### 什么是梯度消失和梯度爆炸？

[https://cloud.tencent.com/developer/article/1374163](https://links.jianshu.com/go?to=https%3A%2F%2Fcloud.tencent.com%2Fdeveloper%2Farticle%2F1374163)

### 常用的激活函数，导数？

#### sigmoid

![sigmoid(x)=\frac{1}{1+e^{-x}}](https://math.jianshu.com/math?formula=sigmoid(x)%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D)

#### tanh

![tanh(x)=\frac{sinh(x)}{cosh(x)}=\frac{\frac{e^z-e^(-z)}{2}}{\frac{e^z+e^{-z}}{2}}=\frac{e^z-e^{-z}}{e^z+e^{-z}}](https://math.jianshu.com/math?formula=tanh(x)%3D%5Cfrac%7Bsinh(x)%7D%7Bcosh(x)%7D%3D%5Cfrac%7B%5Cfrac%7Be%5Ez-e%5E(-z)%7D%7B2%7D%7D%7B%5Cfrac%7Be%5Ez%2Be%5E%7B-z%7D%7D%7B2%7D%7D%3D%5Cfrac%7Be%5Ez-e%5E%7B-z%7D%7D%7Be%5Ez%2Be%5E%7B-z%7D%7D)



#### ReLu

![f(x)=\left\{\begin{matrix}x & if&x\geq0 &\\ 0&if&x<0 \end{matrix}\right.](https://math.jianshu.com/math?formula=f(x)%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7Dx%20%26%20if%26x%5Cgeq0%20%26%5C%5C%200%26if%26x%3C0%20%5Cend%7Bmatrix%7D%5Cright.)

![{f(x)}'=\left\{\begin{matrix}1 & if&x\geq0 &\\ 0&if&x<0 \end{matrix}\right.](https://math.jianshu.com/math?formula=%7Bf(x)%7D'%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D1%20%26%20if%26x%5Cgeq0%20%26%5C%5C%200%26if%26x%3C0%20%5Cend%7Bmatrix%7D%5Cright.)

#### leaky ReLu

![f(x)=\left\{\begin{matrix}x & if&x\geq0 &\\ \lambda x&if&x<0 \end{matrix}\right.](https://math.jianshu.com/math?formula=f(x)%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7Dx%20%26%20if%26x%5Cgeq0%20%26%5C%5C%20%5Clambda%20x%26if%26x%3C0%20%5Cend%7Bmatrix%7D%5Cright.)
 ![{f(x)}'=\left\{\begin{matrix}1 & if&x\geq0 &\\ \lambda&if&x<0 \end{matrix}\right.](https://math.jianshu.com/math?formula=%7Bf(x)%7D'%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D1%20%26%20if%26x%5Cgeq0%20%26%5C%5C%20%5Clambda%26if%26x%3C0%20%5Cend%7Bmatrix%7D%5Cright.)
 [https://zhuanlan.zhihu.com/p/39673127](https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F39673127)

### relu的有优点？又有什么局限性？他们的系列改进方法是啥？

- 部分解决了梯度消失问题
- 收敛速度更快
- 在小于0部分相当于神经元死亡而且不会复活
- Leaky ReLU解决了神经元死亡问题

[https://zhuanlan.zhihu.com/p/31742800](https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F31742800)

### sigmoid和tanh为什么会导致梯度消失？

![img](https:////upload-images.jianshu.io/upload_images/10890732-a6d3b596b3a96ab8.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/375/format/webp)

sigmoid

![img](https:////upload-images.jianshu.io/upload_images/10890732-fa2edd755fa06e1b.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/390/format/webp)

tanh

### 一个隐层需要多少节点能实现包含n元输入的任意布尔函数？

[https://zhuanlan.zhihu.com/p/32579088](https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F32579088)

### 多个隐层实现包含n元输入的任意布尔函数，需要多少节点和网络层？

### dropout为何能防止过拟合？

- 相当于同时训练了多个网络，类似集成学习的效果
- 削弱了神经元之间的依赖性

### dropout和BN 在前向传播和反向传播阶段的区别？

drouput:

[https://blog.csdn.net/oBrightLamp/article/details/84105097](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2FoBrightLamp%2Farticle%2Fdetails%2F84105097)

BN:

[https://blog.csdn.net/silent_crown/article/details/78121270](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fsilent_crown%2Farticle%2Fdetails%2F78121270)

## CNN

### 给定卷积核的尺寸，特征图大小计算方法？

![featuresize=\frac{(inputsize-ksize+2*padsize)}{stride}+1](https://math.jianshu.com/math?formula=featuresize%3D%5Cfrac%7B(inputsize-ksize%2B2*padsize)%7D%7Bstride%7D%2B1)

### 网络容量计算方法(待解答)

### 共享参数有什么优点

- 削减参数量，压缩模型复杂度
- 实现平移不变性

### 常用的池化操作有哪些？有什么特点？

- 下采样
- 实现非线性
- 扩大感受野
- 实现不变性（平移、旋转和尺度）
- maxpooling：取最大值，加上index，在上采样时可以尽量还原信息，比如deep matting的上采样，必须用带有index的，才能保证抠出头发丝
- averagepooling：取平均值，计算简单

### CNN如何用于文本分类？

[https://zhuanlan.zhihu.com/p/28087321](https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F28087321)

### resnet提出的背景和核心理论是？

[https://www.zhihu.com/question/64494691](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F64494691)

背景：当模型深度增加到某个程度后，在增加深度，模型效果可能不升反降，出现退化现象。（不是过拟合也不是梯度爆炸或消失）

核心理论：恒等映射

### 空洞卷积是什么？有什么应用场景？

[https://www.zhihu.com/question/54149221](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F54149221)

在卷积图上注入空洞，增加感受野。注入空洞的数量由dilation rate确定。常规卷积的dilation rate为1。

多尺度检测，利于检测出小物体

语义分割中常用dilation rate。但是人像分割中无用，应该就是我们的应用场景没有特别小的物体。



作者：篱落秋风
链接：https://www.jianshu.com/p/bdaae0a46db9
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



## BN和Dropout在训练和测试时的差别

Batch Normalization
BN，Batch Normalization，就是在深度神经网络训练过程中使得每一层神经网络的输入保持相近的分布。

BN训练和测试时的参数是一样的嘛？

对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。

而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，这个可以通过移动平均法求得。

对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。

BN训练时为什么不用全量训练集的均值和方差呢？

因为用全量训练集的均值和方差容易过拟合，对于BN，其实就是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上能够增加模型的鲁棒性，也会在一定程度上减少过拟合。

也正是因此，BN一般要求将训练集完全打乱，并用一个较大的batch值，否则，一个batch的数据无法较好得代表训练集的分布，会影响模型训练的效果。

Dropout
Dropout 是在训练过程中以一定的概率的使神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。

Dropout 在训练和测试时都需要嘛？

Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。

而在测试时，应该用整个训练好的模型，因此不需要dropout。

Dropout 如何平衡训练和测试时的差异呢？

Dropout ，在训练时以一定的概率使神经元失活，实际上就是让对应神经元的输出为0

假设失活概率为 p ，就是这一层中的每个神经元都有p的概率失活，如下图的三层网络结构中，如果失活概率为0.5，则平均每一次训练有3个神经元失活，所以输出层每个神经元只有3个输入，而实际测试时是不会有dropout的，输出层每个神经元都有6个输入，这样在训练和测试时，输出层每个神经元的输入和的期望会有量级上的差异。

因此在训练时还要对第二层的输出数据除以（1-p）之后再传给输出层神经元，作为神经元失活的补偿，以使得在训练时和测试时每一层输入有大致相同的期望。



dropout部分参考：https://blog.csdn.net/program_developer/article/details/80737724

BN和Dropout共同使用时会出现的问题
BN和Dropout单独使用都能减少过拟合并加速训练速度，但如果一起使用的话并不会产生1+1>2的效果，相反可能会得到比单独使用更差的效果。

相关的研究参考论文：Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift

本论文作者发现理解 Dropout 与 BN 之间冲突的关键是网络状态切换过程中存在神经方差的（neural variance）不一致行为。试想若有图一中的神经响应 X，当网络从训练转为测试时，Dropout 可以通过其随机失活保留率（即 p）来缩放响应，并在学习中改变神经元的方差，而 BN 仍然维持 X 的统计滑动方差。这种方差不匹配可能导致数值不稳定（见下图中的红色曲线）。而随着网络越来越深，最终预测的数值偏差可能会累计，从而降低系统的性能。简单起见，作者们将这一现象命名为「方差偏移」。事实上，如果没有 Dropout，那么实际前馈中的神经元方差将与 BN 所累计的滑动方差非常接近（见下图中的蓝色曲线），这也保证了其较高的测试准确率。



作者采用了两种策略来探索如何打破这种局限。一个是在所有 BN 层后使用 Dropout，另一个就是修改 Dropout 的公式让它对方差并不那么敏感，就是高斯Dropout。

第一个方案比较简单，把Dropout放在所有BN层的后面就可以了，这样就不会产生方差偏移的问题，但实则有逃避问题的感觉。

第二个方案来自Dropout原文里提到的一种高斯Dropout，是对Dropout形式的一种拓展。作者进一步拓展了高斯Dropout，提出了一个均匀分布Dropout，这样做带来了一个好处就是这个形式的Dropout（又称为“Uout”）对方差的偏移的敏感度降低了，总得来说就是整体方差偏地没有那么厉害了。

该部分参考：

https://www.jiqizhixin.com/articles/2018-01-23-4

https://zhuanlan.zhihu.com/p/33101420



## 简述RNN，LSTM，GRU的区别和联系

* GRU和LSTM的性能在很多任务上不分伯仲。
* GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。
* 从结构上来说，GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将hidden state 传给下一个单元，而LSTM则用memory cell 把hidden state 包装起来。

## lstm中是否可以用relu作为激活函数？

LSTM到底该不该使用Relu函数作为其激活函数？在默认的情况下，LSTM使用tanh函数作为其激活函数。从一个帖子的讨论得出一些眉目：https://www.reddit.com/r/MachineLearning/comments/2t1rsp/lstm_question/
看到当LSTM组成的神经网络层数比较少的时候，才用其默认饿tanh函数作为激活函数比Relu要好很多。随着LSTM组成的网络加深，再继续使用tanh函数，就存在了梯度消失的的风险，导致一直徘徊在一个点无法搜索最优解，这种情况下，可以采用Relu函数进行调整，注意学习率需要变地更小一点防止进入死神经元。



## seq2seq在解码时候有哪些方法？

Seq2Seq在解码的时候最基础的算法是**贪心法**，即每次贪心的选择概率最大的结果。贪心算法的计算代价低，适合作为基准结果与其他方法比较。

**集束搜索**是一个常见的改进算法，它是一种启发式的算法。beam search每次维护beam_size个解，然后由这beam_size个解生成下一层的结果，之后将下一层取前beam_size个，不断迭代得到最后的结果。



## 决策树调参说明（DecisionTreeClassifier）

sklearn中决策树算法参数共有13个，如下：

 

* class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)

英文文档在这：决策树参数

* criterion：gini或者entropy,前者是基尼系数，后者是信息熵。两种算法差异不大对准确率无影响，信息墒运算效率低一点，因为它有对数运算.一般说使用默认的基尼系数”gini”就可以了，即CART算法。除非你更喜欢类似ID3, C4.5的最优特征选择方法。

* splitter： best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中，默认的”best”适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐”random” 。

* max_features：None（所有），log2，sqrt，N  特征小于50的时候一般使用所有的

* max_depth：  int or None, optional (default=None) 一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。常用来解决过拟合

* min_samples_split： 如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

* min_samples_leaf： 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值，大些如10W可是尝试下5

* min_weight_fraction_leaf： 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。

* max_leaf_nodes： 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到。

* class_weight： 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重，如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。

* min_impurity_split： 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 。

  ### 模型调参注意事项：

* 当样本少数量但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型
* 如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。
* 推荐多用决策树的可视化，同时先限制决策树的深度（比如最多3层），这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。
* 在训练模型先，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用* class_weight来限制模型过于偏向样本多的类别。
* 决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。
  如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csr_matrix稀疏化。