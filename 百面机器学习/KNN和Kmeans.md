# 1.简述一下K-means算法的原理和工作流程

K-means是划分方法中较经典的聚类算法之一。由于算法的效率高，所以在对大规模数据进行聚类时被广泛应用。目前，许多算法均围绕着该算法进行扩展和改进。

K-means算法以K为参数，把n个对象分成K个簇，使簇内具有较高的相似度，而簇间的相似度较低。

K-means算法的处理过程如下：首先，随机地选择K个对象，每个对象初始地代表了一个簇的平均值或中心；对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇；然后重新计算每个簇的平均值。这个过程不断重复，重复至准则函数收敛。

步骤：

1）从数据D中随机取k个元素，作为k个簇的各自的中心。

2）分别计算剩下的元素到K个簇中心的相异度，将这些元素分别划归到相异度最低的簇。

3）根据聚类结果，重新计算K个簇各自的中心，计算方法是取簇中所有元素各自维度的算术平均数。

4）将数据D中全部元素按照新的中心重新聚类。

5）重复第4）步，直到聚类结果不再变化为止。

6）将结果输出。



# 2.简述一下KNN算法的原理

KNN算法算法又称为K最近邻分类算法。所谓的K最近邻，就是指最接近的K个邻居，即每个样本都可以由它的K个邻居来表示。

KNN算法的思想是，在一个含未知样本的空间，可以根据离这个样本最邻近的K个样本的数据类型来确定样本的数据类型。

该算法涉及3个主要因素：**分类决策规则、距离与相似的衡量、K的大小**。

KNN做分类预测时，一般是选择**多数表决法**，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。而KNN做**回归时，一般是选择平均法**，即最近的K个样本的样本输出的平均值作为回归预测值。

对于距离的度量，我们有很多的距离度量方式，但是最常用的是欧式距离。

K值的选择，过小则容易过拟合，过大则容易欠拟合，可以使用交叉验证法选取K值。



# 3.KNN算法有哪些优点和缺点？

优点：

1）思想简单，理论成熟，既可以用来做分类也可以用来做回归；

2）可用于非线性分类；

3）训练时间复杂度为O(n)；

4)  准确度高，对数据没有假设，对离群值不敏感；

缺点：

1.计算量大；

2.样本不平衡问题（即有些类别的样本数量很多，而其他样本的数量很少）；

3.需要大量的内存；



# 4.不平衡的样本可以给KNN的预测结果造成哪些问题，有没有什么好的解决方式？



观察上面的图，可以看到对于样本X,通过KNN算法，显然可以得到X应属于红点，但对于样本Y,通过KNN算法似乎得到了Y应属于蓝点的结论，而这个结论直观来看并没有说服力。

由上面的例子可见：该算法在分类时有个严重的不足是，当样本不平衡时，即：一个类的样本容量很大，而其他类样本数量很小时，很有可能导致当输入一个未知样本时，该样本的K个邻居中大数量类的样本占多数。但是这类样本并不接近目标样本，而数量小的这类样本很接近目标样本。这个时候，我们有理由认为该位置样本属于数量小的样本所属的一类。但是，KNN却不关心这个问题，它只关心哪类样本的数量最多，而不去把距离远近考虑在内，因此，会导致预测结果的不准确。

我们可以采用权值的方法改进。和该样本距离小的邻居权值大，和该样本距离大的邻居权值相对较小，由此，将距离远近的因素也考虑在内，避免因为一个样本过大导致误判的情况。



# 5.为了解决KNN算法计算量过大的问题，可以使用分组的方式进行计算，简述一下该方式的原理。

KNN算法计算量较大，因为对每一个待分类的样本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。KNN算法的改进方法之一是分组快速搜索近邻法。其基本思想是：将样本集按近邻关系分解成组，给出每组质心的位置，以质心作为代表点，和未知样本计算距离，选出距离最近的一个或者若干个组，再在组的范围内应用一般的KNN算法。由于并不是将未知样本与所有样本计算距离，故该改进算法可以减少计算量，但并不能减少存储量。

# 6.什么是欧氏距离和曼哈顿距离？

欧式距离，最常见的两点或者多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，N维欧式空间中两点x1,x2间的距离公式：



曼哈顿距离是使用在几何度量空间的几何学用语，用以标明两个点在标准坐标系上的绝对轴距总和，曼哈顿距离的定义如下：



如图中两点：



图中红线代表曼哈顿距离，绿色代表欧式距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离。

# 7.KNN中的K如何选取的？

KNN中的K值选取对K近邻算法的结果会产生重大影响。

如果选择较小的值，就相当于用较小的领域的训练实例进行预测，“学习”近似误差会减少，只有与输入实例相近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减少就意味着整体模型变得复杂，容易发生过拟合；

如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的误差估计，但缺点是学习的近似误

差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体模型变得简单。

K=N,则完全不足取，因为此时无论输入实力是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。

在实际应用中,K值一般取一个较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。

 

 

 

# 8.什么是KD树？

KD树（K-dimension tree)是一种对K维空间中的实例点进行存储以便对其进行快速检索的树形结构。KD树是一种二叉树，表示对K维空间的一个划分，构造KD树相当于不断地用垂直于坐标轴的超平面将K维空间切分，构成一系列的K维超矩形区域。利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量，对于二维空间如图所示：



类比二分查找：给出一组数据：【9 1 4 7 2  5 0 3 8】，要查找8.如果挨个查找（线性扫描），那么将会把数据集都遍历一遍。而如果排一下序，那么数据集就变成【0 1 2 3 4 5 6 7 8 9】，按前一种方式进行了很多没有必要的查找，现在如果我们以5为分界点，那么数据集就被划分为了左右两个簇【0 1 2 3】和【6 7 8 9】。因此，根本没有必要进入第一个簇，可以直接进入第二个簇进行查找。把二分查找中的数据点换成K维数据点，这样的划分就变成了用超平面对K维空间的划分。空间划分就是对数据点进行分类。挨得近的数据点就在一个空间里面。

构造KD树的方法如下：构造根节点，使根结点对应于K维空间中包含所有实例点的超矩形区域；通过下面的递归方法，不断地对K维空间进行切分，生成子结点。在超矩形区域上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域分为左右两个子区域（子结点）；这时，实例被分到两个子区域，这个过程直到自区域内没有实例时终止（终止时的结点是叶节点）。在此过程中，将实例保存在相应的结点上，通常，循环的坐标轴对空间切分，选择实例点在坐标轴上的中位数为切分点，这样得到的kd树是平衡的。

构造平衡KD树算法：

输入：k维空间数据集T={},其中

输出：KD树

1）开始：构造根节点，根节点对应于包含T的K维空间的超矩形区域。选择x（1）为坐标轴，以T中所有实例的下（1）坐标的中位数为切分点，将根节点对应的超矩形区域切分为两个子区域。切分由通过切分点并于坐标轴下（1）垂直的超平面实现。由根结点生成深度为1的左右子结点：左子节点对应坐标下（1）小于切分点的子区域，右子节点对应坐标下（1）大于切分点的子区域。将落在切分超平面上的实例点保存在根节点。

2）重复。对深度为j的结点，选择x(l)为切分的坐标轴，l=j%k+1,以该结点的区域中所有实例的x（l）坐标的中位数为切分点，将该结点对应的超矩形区域切分成两个子区域。切分由通过切分点并与坐标轴X（l）垂直的超平面实现。由该结点生成深度为j+1的左右子结点：左子节点对应坐标下（1）小于切分点的子区域，右子节点对应坐标下（1）大于切分点的子区域。将落在切分超平面上的实例点保存在该结点。

搜索KD树：

利用kd树可以省去对大部分数据点的搜素，从而减少搜索的计算量

用kd树的最近邻搜索：

输入：已构造的kd树；目标点x;

输出：x的最近邻

1）在kd树中找出包含目标点x的叶节点：从根节点出发，递归的向下访问kd树。若目标点当前维的坐标值小于切分点的坐标值，则移动到左子节点，否则移动到右子节点。直到子结点为叶子节点为止。

2）以此叶子节点为“当前最近点”；

3）递归的向上回退，在每个结点进行以下操作：

   a)如果该结点保存的实例点比当前最近点距目标点更近，则以该实例点为“当前最近点”；

   b)当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父节点的另一个子结点对应的区域是否有更近的点。具体的，检查另一个子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距离目标更近的点，移动到另一个子结点。接着，递归地进行最近邻搜索。如果不相交，向上回退。

4）当回退到根结点时，搜索结束。最后的“当前最近点”即为x的最近邻点。该方法可以修改后用于k近邻搜索，即通过维护一个包含K个最近邻结点的队列来实现。

# 9.KD树建立过程中切分维度的顺序是否可以优化？

构建开始前，对比数据点在各维度的分布情况，数据点在某一唯独坐标值的方差越大分布越分散，方差越小分布越集中。从方差大的维度维度开始切分可以取得很好的切分效果及平衡性。

# 10.KD树每一次继续切分都要计算该子区间在需切分维度上的中值，计算量很大,有什么方法可以对其进行优化？

1）算法开始前，对原始数据点在所有维度进行一次排序，存储下来，然后在后续的中值选择中，无须每次都对其子集进行排序，提升了性能。

2）从原始数据点中随机选择固定数目的点，然后对其进行排序，每次从这些样本点中取中值，来作为分割超平面。

11.K-means中常用的到中心距离的度量有哪些？

K-means中比较常用的距离度量是欧几里得距离和余弦相似度。

第一种：欧几里得距离，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，N维欧式空间中两点和间的距离公式：



第二种：余弦相似度，余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或者长度上。下图表示余弦相似度的余弦是哪个角的余弦，A，B是三维空间中的两个向量，这两个点与三维空间原点连线形成的交，如果角度越小，说明这两个向量在方向上越接近，在聚类时就归成一类。



 

# 12.K-means中的k值如何选取?

1）场景选定法

K值有时需要根据应用场景选取

2）随机法

随机选取训练数据中的K个点作为起始点，当K值选定后，随机计算n次，取得到最优结果的K作为最优结果的K作为最终聚类结果，避免随机引起的局部最优解。

3）手肘法

手肘法的核心指标是SSE误差平方和：

其中，是第i个簇，p是的样本点，是的质心（中所有样本的均值），SSE是所有样本的聚类误差，代表了聚类效果的好坏。

手肘法的思想是：随着聚类数K的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当K小于真实聚类数时，由于K的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着K值的继续增大而趋于平缓，也就是说SSE和K的关系图是一个手肘的形状，而这个肘部对应的K值就是数据的真实聚类数。当然，这也是该方法被称为手肘法的原因。

4）轮廓系数法

使用轮廓系数法评估分类质量，选择分类质量最好的K值

5）稳定性方法

对一个数据集进行2次重采样产生2个数据子集，再用相同的聚类算法对2个数据自己进行聚类，产生2个具有K个聚类的聚类结果，计算2个聚类结果的相似度的分布情况。2个聚类结果具有高的相似度说明K个聚类反映了稳定的聚类结构，其相似度可以用来评估聚类个数。采用此方法试探多个K,找到合适的K值。

6）与层次聚类结合

首先采用层次聚类算法决定结果簇的数目，并找到一个初始聚类，然后用迭代重定位来改进该聚类。

7)Canopy Method

stage1:聚类最耗费计算的地方是计算对象相似性的时候，Canopy Method在第一阶段选择简单，计算代价较低的方法计算对象相似性，将相似的对象放在一个子集中，这个子集被叫做Canopy,通过一系列计算得到若干Canopy,Canopy之间是可以重叠的，但不会存在某个对象不属于任何Canopy的情况，可以把这一阶段看成是预处理。

stage2:在各个Canopy内使用传统的聚类方法（如K-means),不属于同一Canopy的对象之间不进行相似性计算。

从这个方法起码可以看出两点好处：首先，Canopy不要太大且Canopy之间重叠的不要太多的话会大大减少后续所需要计算相似性的对象的个数；其次，类似于K-means这样的聚类方法是需要人为指出K的值的，通过stage1得到额Canopy个数完全可以作为这个K值，一定程度上减少了这个选择k的盲目性。

# 13.K-means算法中初始点的选择对最终结果有影响吗？

有影响，k-means选择的初始点不同获得的最终分类结果也可能不同，随机选择的中心会导致K-means陷入局部最优解。

# 14.K-means聚类中每个类别中心的初始点如何选择？

1）随机法

最简单的确定初始类簇中心点的方法是随机选择K个点作为初始的类簇中心点。

2）选择各批次距离尽可能远的k个点，首先随机选择一个点作为第一个初始类簇中心点，然后选择距离该点最远的那个点作为第二个初始类簇中心点，然后再选择距离前两个点的最近距离最大的点作为第三个初始类簇的中心点，以此类推，直到选出k个初始类簇中心。

3）层次聚类或者Canopy预处理，选择中心点。选用层次聚类或者Canopy算法进行初始聚类，然后利用这些类簇的中心点作为Kmeans算法初始类簇中心点。

 

 

# 15.K-means中空聚类的处理

如果所有的点在指派步骤都未分配到某个簇，就会得到空簇。如果这种情况发生，则需要某种策略来选择一个替补质心，否则的话，平方误差将会偏大。一种方法是选择一个距离当前任何质心最远的点。这将消除当前对总平方误差影响最大的点。另一种方法是从具有最大SEE的簇中选择一个替补的质心。这将分裂簇并降低聚类的总SEE。如果有多个空簇，则该过程重复多次。另外编程实现时，要注意空簇可能导致的程序bug。

# 16.K-means是否会一直陷入选择质心的循环停不下来？

不会，有数学证明Kmeans一定会收敛，大概思路是利用SSE的概念（也就是误差平方和），即每个点到自身所归属质心的距离的平方和，这个平方和是一个凸函数，通过迭代一定可以到达它的局部最优解。（不一定是全局最优解）

# 17.如何快速收敛数据量超大的K-means？

K-means算法是常用的聚类算法，但其算法本身存在一定的问题。例如，在大数据量下的计算时间过长就是一个重要问题。

Mini Batch Kmeans使用了一种叫做Mini Batch（分批处理）的方法对数据点之间的距离进行计算。Mini Batch的好处是计算过程中不必使用所有的数据样本，而是从不同类别的样本中抽取一部分样本来代表各自类型进行计算。由于计算样本数量少，所以会相应的减少运行时间，但另一方面抽样页必然会带来准确度的下降。

该算法的迭代步骤有两步：

1）从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心

2）更新质心：与k均值算法相比，数据的更新是在每一个小的样本集上。对于每一个小批量，通过计算平均值得到更新质心，并把小批量里的数据分配给该质心，随着迭代次数的增加，这些质心的变化是逐渐减小的，直到质心稳定或者达到指定的迭代次数，停止计算。

# 18.K-means算法的优点和缺点是什么？

K-means算法试图找到使误差平方和最小的簇。当潜在的簇形状是凸面（即球形）的，簇与簇之间区别较明显，且簇大小相近时，其聚类结果较理想。对于处理大数据集合，该算法非常高效，且伸缩性较好。

但该算法除了要事先确定簇数K和对初始聚类中心敏感外，经常以局部最优结束，同时对“噪声”和孤立点敏感，并且该方法不适于发现非凸面形状的簇或者大小差别很大的簇。

# 19.如何对K-means聚类效果进行评估？

一种聚类算法效果评估方法称为轮廓系数，轮廓系数是类的密集和分散程度的评价指标。它会随着类的规模增大而增大。彼此相距很远，本身很密集的类，其轮廓系数较大，彼此集中，本身很大的类，其轮廓系数较小。轮廓系数是通过所有样本计算出来的。

对于第i个对象，计算它到所属簇中所有其他对象的平均距离，记(体现凝聚度）

对于第i个对象和不包括该对象的任意簇，计算该对象到给定簇中所有对象的平均距离，记(体现凝聚度）

第i个对象的轮廓系数为 

从上面可以看出，轮廓系数取值为【-1，1】，其值越大越好，且当值为负时，表明,样本被分配到错误的簇中，聚类结果不可接受。

# 20.K-Means与KNN有什么区别

1）KNN是分类算法，K-means是聚类算法；

2）KNN是监督学习，K-means是非监督学习

3）KNN喂给它的数据集是带Label的数据，已经是完全正确的数据，K-means喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序。

4）KNN没有明显的前期训练过程，K-means有明显的前期训练过程

5）K的含义 KNN来了一个样本x,要给它分类，即求出它的y,就从数据集中，在X附近找距离它最近的K个数据点，这K个数据点，类别C占的个数最多，就把x的label设为c.

K-means中K是人工固定好的数字，假设数据集合可以分为k个簇，由于是依靠人工定好，需要一些先验知识。





来源：https://blog.csdn.net/hua111hua/article/details/86556322