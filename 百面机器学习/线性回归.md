# 1.线性回归基本原理

线性回归是利用称为线性回归方程的最小平方函数对一个或者多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。

**线性回归的公式表达**：
$$
\hat y_i=\theta_0 + \theta_1 \bullet x_i
$$
其中,$\theta_0$和$x_i,y_i$均为向量，展开就是常见的$w_i *x_i+\ldots +w_0=y_1$，其中$x_1,x_2...$等就是特征，$y_1$是标签值。线性回归可以处理回归问题也可以处理分类问题，只不过针对分类问题的0-1这类的标签，线性回归很难收敛。

# 2.损失函数

$$
C=\sum_{i=1}^n(y_i-\hat y_i)
$$

为了方便求导，一般会在前面加$\frac{1}{2}$

## 2.1 为什么线性回归使用的是平方差形式的损失函数？（为什么要用最小二乘法计算）

使用平方形式的时候，使用的是“最小二乘法”的思想，这里的“二乘”指的是用平方来度量观测点与估计点的距离（远近），“最小”指的是参数值要保证各个观测点与估计点的距离的平方和达到最小。**最小二乘法以估计值与观测值的平方和作为损失函数，在误差服从正态分布的前提下（这一点容易被忽视），与极大似然估计的思想在本质上是相同。也就是说不是我们特意去选择mse作为线性回归的损失函数而是因为我们假设误差服从正态分布，使用极大似然法（最大化误差项目为εi的样本总体出现的概率最大）来求解参数，进一步经过推导之后得到的mse的公式而已，具体流程见下：**

我们设观测输出与预估数据之间的误差为：
$$
\epsilon_i=y_i-\hat y _i
$$
我们通常认为误差服从正太分布，即：
$$
f(\epsilon_i;u,\sigma ^2)=\frac{1}{\sigma \sqrt{2\pi}}\bullet exp[-\frac{(\epsilon_i-u)^2}{2\sigma^2}]
$$
我们求参数$\epsilon$的极大似然估计$(u,\sigma^2)$，即是说，在某个$(u,\sigma^2)$下，使得服从正太分布的$\epsilon$取得现有样本$\epsilon_i$的概率最大。也就是说实际上我们的最原始的目标是使得正态分布表达式的连乘的极大似然估计最大。

那么根据极大似然估计函数的定义，令：
$$
L(u,\sigma ^2)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi }\sigma}\bullet exp(-\frac{(\epsilon_i-u)^2}{2\sigma^2})
$$
取对数似然函数：
$$
log L(u,\sigma^2)=-\frac{n}{2}log\sigma^2-\frac{n}{2}log2\pi-\frac{\sum_{i=1}^n(\epsilon_i-u)^2}{2\sigma^2}
$$
分别求$(u,\sigma^2)$的偏导数，然后置0，最后求参数$(u,\sigma^2)$的极大似然估计为：
$$
u=\frac{1}{n}\sum_{i=1}^n \epsilon_i \\
\sigma^2=\frac{1}{n}\sum_{i=1}^n(\epsilon_i-u)^2
$$
我们在线性回归中要求的最佳拟合值线：
$$
\hat y_i=\theta_0+\theta_1 \bullet x_i
$$
实质上是求预估值$\hat y_i$与观测值$y_i$之间的误差最小的情况下$\theta$的值，而前面提到过,$\epsilon$是服从参数$(u,\sigma^2)$的正态分布，那最好是均值$u$和方差$\sigma$趋近于0或者越小越好，看表示就知道了，均值和方差越小，则对数似然函数越大。即：
$$
u=\frac{1}{n}(y_i-\hat y_i) \\
\sigma ^2=\frac{1}{n}(y_i-\hat y_i)^2
$$
而这与最前面构建的平方形式损失函数本质上是等价的。



#  3.下列关于线性回归说法错误的是（D）

- 在现有模型上，加入新的变量，所得到的R^2的值总会增加
- 线性回归的前提假设之一是残差必须服从独立正态分布
- 残差的方差无偏估计是SSE/(n-p)
- 自变量和残差不一定保持相互独立



选项1，R^2是拟合优度，R2=SSR/SST=1-SSE/SST，SST是总平方和，SSR是回归平方和，SSE是残差平方和，公式如下：

![img](https://pic1.zhimg.com/80/v2-14268c4224e6cc49fb1ab95c78b1e62c_720w.jpg)fi为回归预测结果（不包括残差项）

从公式上比较好理解，R^2=SSreg/SStot表示的是自变量引起的变动占总变动的百分比，值越大，说明残差的影响越不明显，则权重部分的预测效果越好。从公式上可以看出，因为SStot是不变的，加入新的变量，则对于SSreg，其中fi就增加了新的变量进来，比如原来是w1x1+w2x2+w3x3，如果新进的特征x4实在太差，再不济我们也可以让其权重w4=0从而训练出与加入x4之前的模型一样的模型（其实应该从pac理论来分析的，但是我不会。。。），显然所得到的R^2的值总会增加。

选项二，线性回归的前提假设之一是残差必须服从独立正态分布，线性回归的损失函数mse就是：在某个(u,σ^2)下，使得服从正态分布的ε取得现有样本εi的概率最大从而推算出来的损失函数的表达式。

选项三， 残差的方差无偏估计是SSE/(n-p)，其中p是特征的数量，别问为什么，这是公式你懂吗？是公式你懂吗？公式你懂吗？式你懂吗？你懂吗？懂吗？吗？？

选项四，错误，残差必须满足独立正态分布才符合线性回归的定义



# **4.在线性回归问题中，我们使用决定系数 (R-squared)来测量拟合优度。我们在线性回归模型中添加一个特征值，并保留相同的模型。下面说法正确的是（ C）**

- 如果R-Squared增大，这个变量是显著的
- 如果R-Squared减小，这个变量是显著的
- 单独观察R-Squared的变化趋势，无法判断这个变量是否显著
- 以上皆非

R-squared就是上面说的R^2，判断变量是否显著，需要根据变量对应的权重系数W，而与R^2无直接关系。

# 5.以下带正则的线性回归模型： 

![img](https://pic2.zhimg.com/v2-e6a7adcd6ae31e14d5f2403d43bbe825_r.jpg)



增加会对bias和variance带来什么影响(假设所有weights都大于1)?

- bias增加，variance增加
- bias增加，variance减小
- bias减小，variance增加
- bias减小，variance减小
- 信息不充分，无法判断

从公式上来看，因为weights都大于1，则p越大，对于目标函数的W的约束越大，作用类似于增大正则项的正则化系数，使得正则化系数整体偏小则偏差增大而方差减小。（正则项深挖涉及到先验分布假设还是挺复杂的后面会总结）

这里就不得不引出一个经典的问题也是很多面试官爱“顺便问一下”的问题：

# **6.为什么l1和l2正则化可以降低过拟合，本质原因是？**

过拟合现象有多种解释，经典的是从 bias-variance 角度来解释，这也是最常用的解释，“正则化能够增大偏差但是降低方差”，跟没说一样。


PAC-learning泛化界解释，正则化降低了模型的拟合能力，使得模型结构变得更加简单，导致了模型的VC维下降，根据PAC的期望误差与风险误差的关系公式（西瓜书上有），一般情况下，VC维越小则二者越接近。


Bayes先验解释，把正则变成先验，L1正则化是给参数引入了标准拉普拉斯先验，L2正则化是给参数引入了标准高斯分布的先验，通过贝叶斯后验概率最大化（这个时候不是用极大似然估计来求解了）可以推导出带L1或L2正则项的损失函数的表达式，引入先验相当于对参数进行了约束，使得参数必须落在标准拉普拉斯分布或者标准高斯分布的范围中，以投硬币的例子解释，假设我们投掷硬币5次均向上会得到硬币向上概率为100%的荒谬结论，如果引入先验知识 硬币向上的概率为0.5，并且令最终求解的概率为实际实验概率与先验概率的一个组合则可以一定程度上降低结论的偏差程度，比如0.3*0.5+0.7*1=0.85相对于原来的1正常一点。



奥卡姆剃刀原则，“保证性能差别不大的情况下，越是简单的模型泛化性能越好”，下面从从公式的角度理解更直观一些。（所谓简单，不同的模型对简单的定义不同，比如线性回归或者逻辑回归定义的简单是权重系数小，树的简单是树的深度浅或者叶子节点权重小等等）

下面是不加入正则项的线性回归的损失函数。

![img](https://pic2.zhimg.com/80/v2-df357b0da9b23f8da18a7220655d657d_720w.jpg)

这里m表示样本的数量为m个样本

使用梯度下降法迭代求解，得到参数更新的公式为。

![img](https://pic3.zhimg.com/80/v2-73ed3f52c97ec5f6b2144260e3e59ada_720w.jpg)

（忽略上面这个图左边的中括号。。。截图的）

然后是加入正则项的损失函数和参数更新公式。（以l2为例）

![img](https://pic2.zhimg.com/80/v2-c9ecfd604d02b385c2b958bdc0d9895d_720w.jpg)

![img](https://pic2.zhimg.com/80/v2-2521c23ef43271d4c6bb902587bdffa9_720w.jpg)

theta0单独列出来是因为正则项不对其进行惩罚。

比较一下加入正则项和不加入正则项的参数更新的公式，alpha是learning rate为正数，lambda是正则化系数为正数，m是样本数量，所以-alpha*lambda/m必为负数，因此相对于无正则化的参数更新公式实际上它的效果是较少参数 theta 的值，具体例子见下：

```text
from sklearn.datasets import load_boston
X=load_boston().x
y=load_boston().y
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
clf=LinearRegression()
clf.fit(X,y)
print(sum([abs(item) for item in clf.coef_]))
```

![img](https://pic3.zhimg.com/80/v2-f2751c178c455de74d8742222645b2ce_720w.png)



```text
clf=Ridge()
clf.fit(X,y)
print(sum([abs(item) for item in clf.coef_]))
```

![img](https://pic2.zhimg.com/80/v2-e720ed92ba6a92cf8adc77987b860985_720w.png)



确实是减少了整体的权重系数的大小。

那么问题来了，为什么降低了权重系数的大小能够降低过拟合呢？这又要涉及到过拟合的理解，为什么过拟合会发生？

本质原因是训练模型的数据和未知的数据之间的分布情况不同导致的而不是单纯因为模型的复杂度太高或者特征维度太高，只能说后者会将训练数据与未知数据之间分布不同带来的影响放大（比如模型复杂度太高而拟合了训练数据的特性而模型复杂度低则不容易拟合训练数据的特性而更可能的是共性等），举个例子，假设你的训练集全部都是人类的身体各项指标和身高的数据，而测试集是狗的身体各项指标和身高的数据，即使在人类的数据上交叉验证得到的结果非常优秀，复杂度非常合适的模型在测试集上也往往不会得到好的效果。（关于过拟合的更加深刻的阐述可见《data shift in machine learning》，有空翻译中文吧，关于样本工程这方面的的解释比较深刻的书确实中文的书不太多。）

所以，我们以一个非常简单的例子来描述一下线性回归的问题。

假设有一个训练完毕的未加入正则化的线性回归方程为 : **y=15\*x1+15\*x2+10**

假设加入正则项之后训练的结果为**：y=7\*x1+7\*x2+5**

那么当x1变动1正个单位，x2变动1个正单位的时候，未加入正则项线性回归方差的预测的变动为30个正单位，而加入正则项的线性回归方程变动为14个正单位，波动程度相对较小，预测结果相对更加稳定。则当未知分布的不同所带来的预测结果的波动，后者要比前者小，更不容易得到错误的预测结果或者说得到的预测结果的错误程度更低。



# **7.关于线性回归的描述,以下正确的有:**

- 基本假设包括随机干扰项是均值为0,方差为1的标准正态分布
- 基本假设包括随机干扰项是均值为0的同方差正态分布
- 在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量
- 在违背基本假设时,模型不再可以估计
- 可以用DW检验残差是否存在序列相关性
- 多重共线性会使得参数估计值方差减小

# **8.请写出在数据预处理过程中如何处理以下问题**

1) 为了预测摩拜每天订单数，我们建立了一个线性回归模型，其中有一个自变量为天气类型（分类变量），分为晴、阴、雾霾、沙尘暴、雨、雪等6种类型，请问如何处理这种变量

天气是离散变量，没有大小关系，并且类别只有6类，可以直接onehot展开

2) 仍然是1) 中的线性回归模型，其中有一个自变量为每天红包车的数量，但是这个变量有1/4的数据是缺失值，请写出至少两种处理缺失值的方法

这尼玛。。这哪是线性回归的面试题啊，这是缺失值处理的面试题。

1/4=25%的数据存在缺失值不能用简单的删除法或者是均值、中位数、众数插补的方法了因为都使用相同的数据进行这么大比例的插补太容易改变原始数据的分布情况了，所以比较好的方式有 1、通过模型插补；2、多重插补法等

3) 依然是1) 中的模型，其中自变量有4个，他们的相关系数矩阵如下：

![img](https://pic4.zhimg.com/80/v2-e67029bdb03aa94e95d66ed0e0fda9d7_720w.jpg)

变量2、3存在较强的负相关性，也就是存在共线性的问题，而共线性会导致模型的效果变差，所以需要根据业务知识来进行处理，合并，或者删除等再观察处理之后的效果再决定采用哪一种处理方式（需要注意的是我们这里是针对仅仅使用线性回归的情况，如果是使用gbdt这类算法则不需要处理）

# **9.线性回归的求解方法有哪些？**

这里也总结一下吧免得老忘记shit，首先线性回归的解法使用的是最小二乘法，所谓就小二乘法就是通过最小化（y_pred*-y_*true）误差的平方来求解模型的权重系数。

**最简单的求法，矩阵求逆计算解析解：**

![img](https://pic3.zhimg.com/80/v2-65642cbc8e7d6ebb0e3cf80545dcf69a_720w.png)

![img](https://pic4.zhimg.com/80/v2-2b64499952f9fad79bcf9d4f1d0781b7_720w.png)

从矩阵的角度很直观的得到原方程的解析解。但是存在以下缺陷：

1、首先就是 （X‘X）必然是一个n*n的方阵，因为涉及计到求逆矩阵的操作，则![[公式]](https://www.zhihu.com/equation?tex=X%5ETX)必须是满秩矩阵（满秩矩阵是判断一个矩阵是否可逆的充分必要条件）（补充：用[初等](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%88%9D%E7%AD%89)行变换将[矩阵](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E7%9F%A9%E9%98%B5)A化为[阶梯形矩阵](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E9%98%B6%E6%A2%AF%E5%BD%A2%E7%9F%A9%E9%98%B5), 则矩阵中非零行的个数就定义为这个[矩阵的秩](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E7%9A%84%E7%A7%A9), 记为r（A），对于n*n的方阵（X‘X），如果其秩等于n则为满秩矩阵 [https://baike.baidu.com/item/%E6%BB%A1%E7%A7%A9%E7%9F%A9%E9%98%B5/10017113](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E6%BB%A1%E7%A7%A9%E7%9F%A9%E9%98%B5/10017113)）

然而实际情况中常常不满足满秩的条件；

2、矩阵的转置与求逆都是很耗费时间和空间的，如果X非常大很容易存在内存不足的问题。



**所以我们常常会使用一种在机器学习领域常见的最优化方法，梯度下降法**

根据损失函数计算出待求参数的梯度更新公式，然后随机初始化，根据梯度更新公式不断更新待求参数直到所有待求参数都到达要求的收敛条件最终得到我们需要的结果，这是一种近似求解的方法。

实际上最优化方法还有不少呢，lbfgs，newton-cg，sag，saga，liblinear（这些都是sklearn里使用到的解法，lbfgs在工业界很常见）等等，我们单独放在《常见最优化方法面经》里总结好了。



# **10.线性回归（包括逻辑回归）模型如何增强模型的表达能力？（就是说遇到非线性的问题咋整）**

连续特征的离散化，特征交叉，gbdt交叉特征提取等；



# **11.为什么线性回归和逻辑回归要用对特征进行离散化？**

[https://blog.csdn.net/Tomcater321/article/details/81562896blog.csdn.net](https://link.zhihu.com/?target=https%3A//blog.csdn.net/Tomcater321/article/details/81562896)

0、 离散特征的增加和减少都很容易，易于模型的快速迭代。(离散特征的增加和减少，模型也不需要调整，重新训练是必须的，相比贝叶斯推断方法或者树模型方法迭代快)

1、稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；

2、离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；离散化后年龄300岁也只对应于一个权重，如果训练数据中没有出现特征"年龄-300岁"，那么在LR模型中，其权重对应于0，所以，即使测试数据中出现特征"年龄-300岁",也不会对预测结果产生影响。特征离散化的过程，比如特征A，如果当做连续特征使用，在LR模型中，A会对应一个权重w,如果离散化，那么A就拓展为特征A-1，A-2，A-3...,每个特征对应于一个权重，如果训练样本中没有出现特征A-4，那么训练的模型对于A-4就没有权重，如果测试样本中出现特征A-4,该特征A-4也不会起作用。相当于无效。但是，如果使用连续特征，在LR模型中，y = w*a,a是特征，w是a对应的权重,比如a代表年龄，那么a的取值范围是[0..100]，如果测试样本中,出现了一个测试用例，a的取值是300，显然a是异常值，但是w*a还是有值，而且值还非常大，所以，异常值会对最后结果产生非常大的影响。

3、逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；在LR模型中，特征A作为连续特征对应的权重是Wa。A是线性特征，因为y = Wa*A,y对于A的导数就是Wa,如果离散化后，A按区间离散化为A_1,A_2,A_3。那么y = w_1*A_1+w_2*A_2+w_3*A_3.那么y对于A的函数就相当于分段的线性函数，y对于A的导数也随A的取值变动，所以，相当于引入了非线性。

4、 离散化后可以进行特征交叉，加入特征A 离散化为M个值，特征B离散为N个值，那么交叉之后会有M*N个变量，进一步引入非线性，提升表达能力；

5、特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；按区间离散化，划分区间是非常关键的。

6、特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。(当使用连续特征时，一个特征对应于一个权重，那么，如果这个特征权重较大，模型就会很依赖于这个特征，这个特征的一个微小变化可能会导致最终结果产生很大的变化，这样子的模型很危险，当遇到新样本的时候很可能因为对这个特征过分敏感而得到错误的分类结果，也就是泛化能力差，容易过拟合。而使用离散特征的时候，一个特征变成了多个，权重也变为多个，那么之前连续特征对模型的影响力就被分散弱化了，从而降低了过拟合的风险。)



# 12.线性回归的基本假设？

给这个大佬博主跪了，五大假设以及假设检验方法讲解的非常详细！

[https://blog.csdn.net/Noob_daniel/article/details/76087829blog.csdn.net](https://link.zhihu.com/?target=https%3A//blog.csdn.net/Noob_daniel/article/details/76087829)

总结起来就是：

1、假设特征与标签之间满足线性关系

2、误差项（ε）之间应相互独立。（比如时间序列数据常常发生误差项不是相互独立的情况，比如今天的数据会收到昨天和前天的数据的影响）

3、自变量之间应相互独立

4、误差项（ε）的方差应为常数。

5、误差项（ε）应呈正态分布**。**



# **13.线性回归效果不好的原因？：**

很难拟合复杂的非线性关系，实际情况中基础假设难以得到满足（比如残差符合正态分布）从而影响最终模型的精度；



# **14.线性回归解析解的推导（三种方法）**

[https://blog.csdn.net/promisejia/article/details/80159619#%E4%B8%89-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BCblog.csdn.net](https://link.zhihu.com/?target=https%3A//blog.csdn.net/promisejia/article/details/80159619%23%E4%B8%89-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC)

最常用的莫过于通过矩阵求逆的方式来推导，没想到还有另外两种方法，我就想知道这什么公司怎么会出这种毒题啊。。。。下次面试官要这么问我我就让他现场给我写一遍，给你能的。





# 15.逻辑回归 和 线性回归的区别？

两者都属于广义线性模型。

线性回归优化目标函数用的最小二乘法，而逻辑回归用的是最大似然估计。逻辑回归只是在线性回归的基础上，将加权之和通过 sigmoid 函数，映射到 0−1范围内空间。

线性回归在整个实数范围内进行预测，敏感度一致，而分类范围需要在 [0,1] 。**逻辑回归就是一种减小预测范围，将预测值限定为 [0,1] 间的一种回归模型。**

逻辑曲线在 z=0 时，十分敏感，在 z>>0或 z<<0处，都不敏感，将预测值限定为 (0,1)。逻辑回归的鲁棒性比线性回归要好。（这主要是因为sigmoid的性质，看sigmoid的图像就知道了在0点附近变化函数值很大，越趋近于正负无穷大函数值变化越小）



# **16.简要介绍一下线性回归原理，处理步骤，怎么确定因变量与自变量间线性关系，什么情况下可停止迭代，怎么避免过拟合情况？**

原理部分不介绍了吧，前面写过了，一般来说缺失值处理、类别变量数值化，异常值处理，连续特征离散化等等，当两次迭代所有参数的变化量小于事先给定的阈值时，或者达到事先设定的最大迭代次数，则停止迭代过程，过拟合没法避免只能说是尽量降低过拟合的影响，通过l1、l2正则化、减少特征的数量、增大样本的数量等等



来源：https://zhuanlan.zhihu.com/p/30535220