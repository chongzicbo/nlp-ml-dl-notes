{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "知乎笔记：attention机制详解.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTctzkx1z3lCBnrshv5GDc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/nlp-ml-dl-notes/blob/master/%E7%9F%A5%E4%B9%8E%E7%AC%94%E8%AE%B0%EF%BC%9Aattention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quxrmlad_7M7",
        "colab_type": "text"
      },
      "source": [
        "##1.RNN结构的局限\n",
        "&emsp;&emsp;以机器翻译为例，通常使用encoder-decoder结构，即encoder首先将输入的句子转换为一个定长的向量，然后decoder再将这个向量翻译成目标语言的文字。encoder和decoder一般都采用LSTM或者GRU等。\n",
        "![替代文字](https://pic1.zhimg.com/80/v2-60645bb5a5777b7bcee90c78de34eb00_720w.jpg)\n",
        "\n",
        "&emsp;&emsp;如上图所示，encoder端RNN将输入语句换换到最后一个hidden vector中，并将其作为decoder端的初始hidden vector，然后利用decoder解码成相应语言的文字。\n",
        "&emsp;&emsp;上图结构存在的问题：\n",
        "* 当输入序列太长，RNN结构存在梯度消失的问题\n",
        "* 将输入的序列转化为定长向量以难以有效的保存信息，随着输入序列的增长，效果会更差。\n",
        "\n",
        "## 2. attention机制\n",
        "\n",
        "![替代文字](https://pic4.zhimg.com/80/v2-a5f8a19c6d89422fe7d8a74087088b37_720w.jpg)\n",
        "\n",
        "* 首先得到encoder端的hidden state ($h_1,h_2,\\ldots,h_T$)\n",
        "* 计算注意力得分\n",
        "假设当前decoder的hidden state是$s_{t-1}$，计算每一个输入位置$j$与当前输出位置的关联性$e_{tj}=a(s_{t-1},h_j)$,相应的向量形式为：$\\overrightarrow{e_t}=(a(s_{t-1},h_1),\\ldots,a(s_{t-1,h_T}))$。其中，a是一种相关性运算符，例如常见的有点乘形式$\\overrightarrow{e_t}=\\overrightarrow{s_{t-1}}^T\\overrightarrow{h}$、加权点乘\n",
        "$\\overrightarrow{e_t}=\\overrightarrow{s_{t-1}}^T W \\overrightarrow{h}$、加和$\\overrightarrow{e_t}=\\overrightarrow{v}^Ttanh(W_1\\overrightarrow{h}+W_2\\overrightarrow{s_{t-1}})$等等。\\\n",
        "对$\\overrightarrow{s_t}$进行softmax操作，得到attention的分布，$\\overrightarrow \\alpha _t=softmax(\\overrightarrow{s_t})$,展开形式为:$\\alpha _{tj}=\\frac{exp(e_{tj})}{\\sum _{k=1}^Texp(e_{tk})}$\n",
        "\n",
        "* 利用$\\overrightarrow{\\alpha _t}$计算context vector:\n",
        "$\\overrightarrow{c _t}=\\sum_{j=1}^T \\alpha _{tj}h_j$\n",
        "\n",
        "* 根据$s_{t-1},y_{t-1},c_t$计算decoder的下一个hidden state以及输出：\n",
        "$$s_t=f(s_{t-1},y_{t-1},c_t)$$\n",
        "$$p(y_t|y_1,\\dots,y_{t-1},x)=g(y_{i-1},s_i,c_i)$$\n",
        "\n",
        "## 3. attention机制总结\n",
        "\n",
        "* 关键操作是计算encoder与decoder state之间的关联性的权重，得到attention分布，从而对于当前输出位置得到比较重要的输入位置的权重，在预测输出时相应的会占较大的比重。\n",
        "* attention机制打破了只能利用encoder最终单一固定结果向量的限制，从而模型可以集中在所有对于下一个目标单词重要的输入信息上，使模型效果可以得到极大的改善。另外一个优点是可以可视化观察attention权重矩阵的变化，可以更好地知道哪部分翻译对应哪部分源文字，有助于更好地理解模型工作机制。"
      ]
    }
  ]
}