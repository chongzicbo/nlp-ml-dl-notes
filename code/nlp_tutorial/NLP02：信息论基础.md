# 1.熵

熵用于描述信息的不确定性，定义为：
$$
H(X)=-\sum_xp(x)log_2p(x)
$$
其中，X代表离散型随机变量，当对数函数以2为底时，熵的单位是比特(bit),以e为底时单位是奈特(nat)。熵的大小不取决于$X$的实际取值，而仅依赖其概率分布。

# 2.联合熵和条件熵

当有两个随机变量$(X,Y)$时，如果其联合分布为$p(x,y)$,则其**联合熵**$H(X,Y)$定义为
$$
H(X,Y)=-\sum_x\sum_yp(x,y)log_2p(x,y)
$$
联合熵体现随机变量$X$和$Y$共同携带的信息量，它的值大于或等于任何一个随机变量单独的熵，同时也小于或等于二者熵的和。

**条件熵**描述了在已知一个随机变量值的前提下，对另一个随机变量的熵的计算。基于条件$X$的$Y$的信息熵表示为$H(Y|X)$，则有：
$$
H(Y|X)=\sum_x\sum_yp(x,y)log_2p(y|x)
$$
条件熵、联合熵之间的关系是
$$
H(Y|X)=H(X,Y)-H(X)
$$

# 3.相对熵与互信息

相对熵是两个随机分布之间距离的度量。比如一个样本集中的两个概率分布为$p$和$q$，其中$p$为真实分布，$q$为假定分布，如果按照真实分布来衡量识别一个样本所需要的编码长度的期望为$H(p)=\sum_ip(i)log_2\frac {1}{p(i)}$，如果按照假定分布$q$来表示真实分布$p$的平均编码长度，则有$H(p,q)=\sum_ip(i)log_2\frac {1}{q(i)}$，称$H(p,q)$为交叉熵。因为$H(p)$是最小编码长度，$H(p,q)-H(p)$是冗余的信息量，这个差值被称为相对熵，也叫KL散度。

$p(x)$和$q(x)$两个概率密度函数之间的相对熵定义为
$$
D(p||q)=\sum_{x \in \chi}p(x)log_x\frac{p(x)}{q(x)} \\
=E_plog_2 \frac {p(X)}{q(X)}
$$
相对熵可以用来衡量在相同事件空间中两个事件的相似程度，而互信息则用来衡量在不同事件空间中两个信息的相关性。

**定义**：设两个随机变量为$X$和$Y$，它们的联合概率密度函数为$p(x,y)$,其边缘概率密度函数分布为$p(x)$和$p(y)$，则$X$和$Y$的互信息记作：
$$
I(X,Y)=\sum_x\sum_yp(x,y)log_2 \frac{p(x,y)}{p(x)p(y)}\\
D(p(x,y)||p(x)p(y))
$$
熵与互信息的关系是
$$
I(X,Y)=H(X)-H(X|Y) \geq0
$$
互信息就是随机变量$X$的熵$H(X)$在给定随机变量$Y$的条件熵$H(X|Y)$后的缩减量

