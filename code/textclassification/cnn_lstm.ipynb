{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_lstm.ipynb",
      "provenance": [],
      "mount_file_id": "1RMiGWACc9Mx_KE3iJyEgetzOra6MCbhF",
      "authorship_tag": "ABX9TyPTDSYEXXhJC9+vAhSTSP44",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/nlp-ml-dl-notes/blob/master/code/textclassification/cnn_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv5X3vv1ygz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation,Embedding,LSTM,Conv1D,MaxPooling1D\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHf02KoTfh03",
        "colab_type": "text"
      },
      "source": [
        "# 1.参数设置"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYTbKD4Sy_53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#embedding 参数\n",
        "maxlen=100\n",
        "embedding_size=200\n",
        "\n",
        "#卷积参数\n",
        "kernel_size=5\n",
        "filters=128\n",
        "pool_size=4\n",
        "\n",
        "#LSTM参数\n",
        "lstm_output_size=100\n",
        "\n",
        "#训练参数\n",
        "batch_size=128\n",
        "epochs=2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMGTcN0Yfl_7",
        "colab_type": "text"
      },
      "source": [
        "# 2.数据预处理及训练数据准备"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNPYbvXn4FLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def textToChars(filePath):\n",
        "  \"\"\"\n",
        "  读取文本文件并进行处理\n",
        "  :param filePath:文件路径\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  lines = []\n",
        "  df=pd.read_excel(filePath,header=None)\n",
        "  df.columns=['content']\n",
        "  for index, row in df.iterrows():\n",
        "    row=row['content']\n",
        "    row = re.sub(\"[^\\u4e00-\\u9fa5]\", \"\", str(row))  # 只保留中文\n",
        "    lines.append(list(str(row)))\n",
        "  return lines\n",
        "\n",
        "\n",
        "def getWordIndex(vocabPath):\n",
        "  \"\"\"\n",
        "  获取word2Index,index2Word\n",
        "  :param vocabPath:词汇文件\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  word2Index = {}\n",
        "  with open(vocabPath, encoding=\"utf-8\") as f:\n",
        "    for line in f.readlines():\n",
        "      word2Index[line.strip()] = len(word2Index)\n",
        "  index2Word = dict(zip(word2Index.values(), word2Index.keys()))\n",
        "  return word2Index, index2Word\n",
        "\n",
        "\n",
        "def lodaData(posFile, negFile, word2Index):\n",
        "  \"\"\"\n",
        "  获取训练数据\n",
        "  :param posFile:正样本文件\n",
        "  :param negFile:负样本文件\n",
        "  :param word2Index:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  posLines = textToChars(posFile)\n",
        "  negLines = textToChars(negFile)\n",
        "  textLines=posLines+negLines\n",
        "  print(\"正样本数量%d,负样本数量%d\"%(len(posLines),len(negLines)))\n",
        "  posIndexLines = [[word2Index[word] if word2Index.get(word) else 0 for word in line] for line in posLines]\n",
        "  negIndexLines = [[word2Index[word] if word2Index.get(word) else 0 for word in line] for line in negLines]\n",
        "  lines = posIndexLines + negIndexLines\n",
        "  print(\"训练样本和测试样本共：%d 个\"%(len(lines)))\n",
        "  # lens = [len(line) for line in lines]\n",
        "  labels = [1] * len(posIndexLines) + [0] * len(negIndexLines)\n",
        "  padSequences = sequence.pad_sequences(lines, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
        "  X_train,X_test,y_train,y_test=train_test_split(padSequences,np.array(labels),test_size=0.2,random_state=42)\n",
        "  return (textLines,labels),(X_train,X_test,y_train,y_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InQv-eRR4KSk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "954e739c-4a1d-4acb-af1b-5fff0bc26a44"
      },
      "source": [
        "vocabPath=\"/content/drive/My Drive/data/vocab.txt\"\n",
        "negFilePath=\"/content/drive/My Drive/data/text_classify/sentiment/neg.xls\"\n",
        "posFilePath=\"/content/drive/My Drive/data/text_classify/sentiment/pos.xls\"\n",
        "word2Index, index2Word=getWordIndex(vocabPath)\n",
        "(textLines,labels),(X_train,X_test,y_train,y_test)=lodaData(posFile=posFilePath,negFile=negFilePath,word2Index=word2Index)\n",
        "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "正样本数量10677,负样本数量10428\n",
            "训练样本和测试样本共：21105 个\n",
            "(16884, 100) (4221, 100) (16884,) (4221,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bB6oIYg5OV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "dab96dfb-492c-42c1-c79f-80a9b0e8aefc"
      },
      "source": [
        "X_train[0],y_train[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([6821, 3221,  671, 3315, 2769,  702,  782, 6371,  711, 2523, 2141,\n",
              "        4500, 4638,  741,  852, 1079, 2159, 3300,  763, 5042, 1296,  679,\n",
              "        6814, 2828,  809, 1184, 1762, 2110, 3413, 7027, 2110, 6814, 4638,\n",
              "        5632, 3131, 4638, 4761, 6399, 7028, 3946,  749, 7390, 4708, 2399,\n",
              "        7977, 1469, 4852,  833, 4384, 1862, 2128, 6871, 5632, 3131, 4638,\n",
              "        2692, 6399, 6632, 3341, 6632, 2483,  749, 4684, 1168, 1724, 2335,\n",
              "        1765, 7448,  782, 5102, 1762, 5632, 4197,  704, 3221, 1963, 3634,\n",
              "        4638, 3953, 2207, 5445, 5546, 2483, 2769,  812, 3187, 1213, 2850,\n",
              "        2834,  852, 3221, 1963, 3362, 2769,  812, 2958, 2995,  671,  763,\n",
              "        5632], dtype=int32), 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSsvH0x0ftdA",
        "colab_type": "text"
      },
      "source": [
        "# 3.网络结构搭建及模型训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krVDgS-80bX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "9efbf419-6379-499c-f0f6-004b8135552c"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(len(word2Index),embedding_size,input_length=maxlen))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters,kernel_size,padding=\"valid\",activation=\"relu\",strides=1))\n",
        "model.add(MaxPooling1D(pool_size))\n",
        "model.add(LSTM(lstm_output_size))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print(\"开始训练\")\n",
        "model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_test,y_test))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "开始训练\n",
            "Epoch 1/2\n",
            "132/132 [==============================] - 8s 57ms/step - loss: 0.5012 - accuracy: 0.7406 - val_loss: 0.3162 - val_accuracy: 0.8778\n",
            "Epoch 2/2\n",
            "132/132 [==============================] - 7s 53ms/step - loss: 0.2644 - accuracy: 0.9006 - val_loss: 0.2633 - val_accuracy: 0.8965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f64b93bd518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOMCAugv1zrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "7b60e1a4-e8ab-4a6f-8e74-0ec5ec382c95"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 200)          4225600   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100, 200)          0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 96, 128)           128128    \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 24, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               91600     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 4,445,429\n",
            "Trainable params: 4,445,429\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99BsIchNhKi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_one(sentence,model,word2Index):\n",
        "  sentence=re.sub(\"[^\\u4e00-\\u9fa5]\", \"\", str(sentence))  # 只保留中文\n",
        "  # print(sentence)\n",
        "  sentence=[word2Index[word] if word2Index.get(word) else 0 for word in sentence]\n",
        "  sentence=sentence+[0]*(maxlen-len(sentence)) if len(sentence)<maxlen else sentence[0:300]\n",
        "  # print(sentence)\n",
        "  sentence=np.reshape(np.array(sentence),(-1,len(sentence))) \n",
        "  pred_prob=model.predict(sentence)\n",
        "  label = 1 if pred_prob[0][0]>0.5 else 0\n",
        "  print(label)\n",
        "  return label\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXVD1BxU9YWd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6068832-83d4-4aa7-c7b0-76702ca57d7e"
      },
      "source": [
        "sentence=\"一次很不爽的购物，页面上说是第二天能到货，结果货是从陕西发出的，卖家完全知道第二天根本到不了货。多处提到送货入户还有100%送货入户也没有兑现，与客服联系多日，还是把皮球踢到快递公司。算是一个教训吧。\"\n",
        "predict_one(sentence,model,word2Index)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvwqj6Vb9b-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}