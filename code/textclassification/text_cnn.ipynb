{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1JCgBvrteetnvN9wCCfPIHXdn4sfy3Ee8",
      "authorship_tag": "ABX9TyM4ufcVAlIdK3TV3mZvp2VE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/nlp-ml-dl-notes/blob/master/code/textclassification/text_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm5SzLas6Eya",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 基于TextCnn的中文文本分类\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp7YafFb6RZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing import sequence"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx5qm59I8ZVJ",
        "colab_type": "text"
      },
      "source": [
        "# 1.数据预处理及准备训练和测试数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbdAAKOsmehk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen=100\n",
        "batch_size=128"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc5ncWSemaNv",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 文本预处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNI2O_JijxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def textToChars(filePath):\n",
        "    \"\"\"\n",
        "    读取文本文件并进行处理\n",
        "    :param filePath:文件路径\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    df=pd.read_excel(filePath,header=None)\n",
        "    df.columns=['content']\n",
        "    for index, row in df.iterrows():\n",
        "      row=row['content']\n",
        "      row = re.sub(\"[^\\u4e00-\\u9fa5]\", \"\", str(row))  # 只保留中文\n",
        "      lines.append(list(row))\n",
        "    return lines\n",
        "\n",
        "\n",
        "def getWordIndex(vocabPath):\n",
        "    \"\"\"\n",
        "    获取word2Index,index2Word\n",
        "    :param vocabPath:词汇文件\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    word2Index = {}\n",
        "    with open(vocabPath, encoding=\"utf-8\") as f:\n",
        "        for line in f.readlines():\n",
        "            word2Index[line.strip()] = len(word2Index)\n",
        "    index2Word = dict(zip(word2Index.values(), word2Index.keys()))\n",
        "    return word2Index, index2Word\n",
        "\n",
        "\n",
        "def lodaData(posFile, negFile, word2Index):\n",
        "  \"\"\"\n",
        "  获取训练数据\n",
        "  :param posFile:正样本文件\n",
        "  :param negFile:负样本文件\n",
        "  :param word2Index:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  posLines = textToChars(posFile)\n",
        "  negLines = textToChars(negFile)\n",
        "  posIndexLines = [[word2Index[word] if word2Index.get(word) else 0 for word in line] for line in posLines]\n",
        "  negIndexLines = [[word2Index[word] if word2Index.get(word) else 0 for word in line] for line in negLines]\n",
        "  lines = posIndexLines + negIndexLines\n",
        "  print(\"训练样本和测试样本共：%d 个\"%(len(lines)))\n",
        "  # lens = [len(line) for line in lines]\n",
        "  labels = [1] * len(posIndexLines) + [0] * len(negIndexLines)\n",
        "  padSequences = sequence.pad_sequences(lines, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
        "  X_train,X_test,y_train,y_test=train_test_split(padSequences,np.array(labels),test_size=0.2,random_state=42)\n",
        "  return X_train,X_test,y_train,y_test"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44V6PzFMkvx_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "41ad2730-7f32-49c2-be36-3726201bfdcc"
      },
      "source": [
        "vocabPath=\"/content/drive/My Drive/data/vocab.txt\"\n",
        "negFilePath=\"/content/drive/My Drive/data/text_classify/sentiment/neg.xls\"\n",
        "posFilePath=\"/content/drive/My Drive/data/text_classify/sentiment/pos.xls\"\n",
        "word2Index, index2Word=getWordIndex(vocabPath)\n",
        "X_train,X_test,y_train,y_test=lodaData(posFile=posFilePath,negFile=negFilePath,word2Index=word2Index)\n",
        "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "训练样本和测试样本共：21105 个\n",
            "(16884, 100) (4221, 100) (16884,) (4221,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9V_3kJ5m3Qe",
        "colab_type": "text"
      },
      "source": [
        "## 1.2  自定义Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9hDvinn8VSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(self,features,labels):\n",
        "    \"\"\"\n",
        "    features:文本向量化后的特征\n",
        "    labels:标签向量 \n",
        "    \"\"\"\n",
        "    self.features=features\n",
        "    self.labels=labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.features.shape[0]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.features[index],self.labels[index]\n",
        "\n",
        "    \n",
        "train_dataset=MyDataset(X_train,y_train)\n",
        "test_dataset=MyDataset(X_test,y_test)\n",
        "train_dataloader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdoBfhY4o0lD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3e91ccb-4e6d-4295-b228-84018584cbbd"
      },
      "source": [
        "for features,labels in train_dataloader:\n",
        "  print(features.shape,labels.shape)\n",
        "  break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 100]) torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZjMTsZI8RGy",
        "colab_type": "text"
      },
      "source": [
        "# 2.网络搭建"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWEA0bWv5SBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextCnn(nn.Module):\n",
        "  def __init__(self, param: dict):\n",
        "    super(TextCnn, self).__init__()\n",
        "    input_channel = 1  # input channel size\n",
        "    output_channel = param[\"output_channel\"]  # output channel size\n",
        "    kernel_size = param[\"kernel_size\"]\n",
        "    vocab_size = param[\"vocab_size\"]\n",
        "    embedding_dim = param[\"embedding_dim\"]\n",
        "    dropout = param[\"dropout\"]\n",
        "    class_num = param[\"class_num\"]\n",
        "    self.param = param\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim,padding_idx=0)\n",
        "    self.conv1 = nn.Conv2d(input_channel, output_channel, (kernel_size[0], embedding_dim))\n",
        "    self.conv2 = nn.Conv2d(input_channel, output_channel, (kernel_size[1], embedding_dim))\n",
        "    self.conv3 = nn.Conv2d(input_channel, output_channel, (kernel_size[2], embedding_dim))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc1 = nn.Linear(len(kernel_size) * output_channel, class_num)\n",
        "\n",
        "  def init_embedding(self, embedding_matrix):\n",
        "    self.embedding.weight = nn.Parameter(torch.Tensor(embedding_matrix))\n",
        "\n",
        "  @staticmethod\n",
        "  def conv_pool(x, conv):\n",
        "    \"\"\"\n",
        "    卷积+池化\n",
        "    :param x:[batch_size,1,sequence_length,embedding_dim]\n",
        "    :param conv:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    x = conv(x)  # 卷积， [batch_size,output_channel,h_out,1]\n",
        "    x = F.relu((x.squeeze(3)))  # 去掉最后一维,[batch_size,output_channel,h_out]\n",
        "    x = F.max_pool1d(x, x.size(2)).squeeze(2)  # [batch_size,output_channel]\n",
        "    return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    前向传播\n",
        "    :param x:[batch_size,sequence_length]\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    x = self.embedding(x)  # [batch_size,sequence_length,embedding_dim]\n",
        "    x = x.unsqueeze(1)  # 增加一个channel维度 [batch_size,1,sequence_length,embedding_dim]\n",
        "    x1 = self.conv_pool(x, self.conv1)  # [batch_size,output_channel]\n",
        "    x2 = self.conv_pool(x, self.conv2)  # [batch_size,output_channel]\n",
        "    x3 = self.conv_pool(x, self.conv3)  # [batch_size,output_channel]\n",
        "    x = torch.cat((x1, x2, x3), 1)  # [batch_size,output_channel*3]\n",
        "    x = self.dropout(x)\n",
        "    logit = F.log_softmax(self.fc1(x), dim=1)\n",
        "    return logit\n",
        "\n",
        "  def init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        if m.bias is not None:\n",
        "          m.bias.data.zero_()\n",
        "\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        m.weight.data.normal_(0, 0.01)\n",
        "        m.bias.data.zero_()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1XYBiWXintx",
        "colab_type": "text"
      },
      "source": [
        "# 3.模型训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q2_ISso8PA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textCNNParams={\n",
        "    \"vocab_size\":len(word2Index),\n",
        "    \"embedding_dim\":200,\n",
        "    \"class_num\":2,\n",
        "    \"output_channel\":16,\n",
        "    \"kernel_size\":[3,4,5],\n",
        "    \"dropout\":0.2\n",
        "}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uAtvtG6nrCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net=TextCnn(textCNNParams)\n",
        "net.init_weights()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzLfybsqoDeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "2da4e065-65d1-42cf-e90e-40a6b1d45b28"
      },
      "source": [
        "net.cuda()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextCnn(\n",
              "  (embedding): Embedding(21128, 200, padding_idx=0)\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(3, 200), stride=(1, 1))\n",
              "  (conv2): Conv2d(1, 16, kernel_size=(4, 200), stride=(1, 1))\n",
              "  (conv3): Conv2d(1, 16, kernel_size=(5, 200), stride=(1, 1))\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (fc1): Linear(in_features=48, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPMOhQB7o0oD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer=torch.optim.Adam(net.parameters(),lr=0.01)\n",
        "criterion=nn.NLLLoss()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc1EECdloKBV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "718b39c4-f19a-4303-c043-dedea5599d34"
      },
      "source": [
        "for epoch in range(50):\n",
        "  total_train_loss=[]\n",
        "  net.train()\n",
        "  for i,(feature,label) in enumerate(train_dataloader):\n",
        "    feature=feature.cuda()\n",
        "    label=label.cuda()\n",
        "    y_pred=net(feature.long()) #前向计算\n",
        "    loss=criterion(y_pred,label) #计算损失\n",
        "    optimizer.zero_grad() #清除梯度\n",
        "    loss.backward() #计算梯度，误差回传\n",
        "    optimizer.step() #更新参数\n",
        "    total_train_loss.append(loss.data.item())\n",
        "  total_valid_loss=[]\n",
        "  pred_true_labels=0\n",
        "  net.eval()\n",
        "  for i,(feature_test,label_test) in enumerate(test_dataloader):\n",
        "    feature_test=feature_test.cuda()\n",
        "    label_test=label_test.cuda()\n",
        "    with torch.no_grad():\n",
        "      pred_test=net(feature_test.long())\n",
        "      test_loss=criterion(pred_test,label_test)\n",
        "      total_valid_loss.append(test_loss.data.item())\n",
        "      # accu=torch.sum((torch.argmax(pred_test,dim=1)==label_test)).data.item()/feature_test.shape[0]\n",
        "      pred_true_labels+=torch.sum(torch.argmax(pred_test,dim=1)==label_test).data.item()\n",
        "      \n",
        "  print(\"epoch:{},  train_loss:{:.3f},  test_loss:{:.3f},  test accuracy:{:.3f}\".format(epoch,np.mean(total_train_loss),np.mean(total_valid_loss),pred_true_labels/len(test_dataset)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0,  train_loss:0.475,  test_loss:0.299,  test accuracy:0.879\n",
            "epoch:1,  train_loss:0.299,  test_loss:0.265,  test accuracy:0.894\n",
            "epoch:2,  train_loss:0.211,  test_loss:0.326,  test accuracy:0.889\n",
            "epoch:3,  train_loss:0.162,  test_loss:0.273,  test accuracy:0.905\n",
            "epoch:4,  train_loss:0.116,  test_loss:0.353,  test accuracy:0.905\n",
            "epoch:5,  train_loss:0.105,  test_loss:0.362,  test accuracy:0.910\n",
            "epoch:6,  train_loss:0.097,  test_loss:0.347,  test accuracy:0.911\n",
            "epoch:7,  train_loss:0.079,  test_loss:0.487,  test accuracy:0.904\n",
            "epoch:8,  train_loss:0.089,  test_loss:0.489,  test accuracy:0.896\n",
            "epoch:9,  train_loss:0.076,  test_loss:0.778,  test accuracy:0.878\n",
            "epoch:10,  train_loss:0.096,  test_loss:0.650,  test accuracy:0.900\n",
            "epoch:11,  train_loss:0.077,  test_loss:0.635,  test accuracy:0.901\n",
            "epoch:12,  train_loss:0.079,  test_loss:0.698,  test accuracy:0.903\n",
            "epoch:13,  train_loss:0.063,  test_loss:0.780,  test accuracy:0.896\n",
            "epoch:14,  train_loss:0.072,  test_loss:0.764,  test accuracy:0.907\n",
            "epoch:15,  train_loss:0.058,  test_loss:0.878,  test accuracy:0.895\n",
            "epoch:16,  train_loss:0.061,  test_loss:0.889,  test accuracy:0.898\n",
            "epoch:17,  train_loss:0.088,  test_loss:0.923,  test accuracy:0.900\n",
            "epoch:18,  train_loss:0.084,  test_loss:0.825,  test accuracy:0.901\n",
            "epoch:19,  train_loss:0.058,  test_loss:0.968,  test accuracy:0.892\n",
            "epoch:20,  train_loss:0.074,  test_loss:1.026,  test accuracy:0.897\n",
            "epoch:21,  train_loss:0.101,  test_loss:1.170,  test accuracy:0.887\n",
            "epoch:22,  train_loss:0.089,  test_loss:1.090,  test accuracy:0.895\n",
            "epoch:23,  train_loss:0.068,  test_loss:0.999,  test accuracy:0.892\n",
            "epoch:24,  train_loss:0.068,  test_loss:1.288,  test accuracy:0.899\n",
            "epoch:25,  train_loss:0.076,  test_loss:1.241,  test accuracy:0.893\n",
            "epoch:26,  train_loss:0.091,  test_loss:1.597,  test accuracy:0.892\n",
            "epoch:27,  train_loss:0.123,  test_loss:1.397,  test accuracy:0.893\n",
            "epoch:28,  train_loss:0.084,  test_loss:1.297,  test accuracy:0.892\n",
            "epoch:29,  train_loss:0.068,  test_loss:1.492,  test accuracy:0.889\n",
            "epoch:30,  train_loss:0.096,  test_loss:1.599,  test accuracy:0.879\n",
            "epoch:31,  train_loss:0.102,  test_loss:1.635,  test accuracy:0.895\n",
            "epoch:32,  train_loss:0.073,  test_loss:1.729,  test accuracy:0.897\n",
            "epoch:33,  train_loss:0.085,  test_loss:1.984,  test accuracy:0.892\n",
            "epoch:34,  train_loss:0.082,  test_loss:1.662,  test accuracy:0.892\n",
            "epoch:35,  train_loss:0.068,  test_loss:1.750,  test accuracy:0.894\n",
            "epoch:36,  train_loss:0.073,  test_loss:1.984,  test accuracy:0.892\n",
            "epoch:37,  train_loss:0.074,  test_loss:2.106,  test accuracy:0.888\n",
            "epoch:38,  train_loss:0.062,  test_loss:1.886,  test accuracy:0.892\n",
            "epoch:39,  train_loss:0.073,  test_loss:2.113,  test accuracy:0.896\n",
            "epoch:40,  train_loss:0.089,  test_loss:1.906,  test accuracy:0.897\n",
            "epoch:41,  train_loss:0.089,  test_loss:2.058,  test accuracy:0.892\n",
            "epoch:42,  train_loss:0.063,  test_loss:1.959,  test accuracy:0.896\n",
            "epoch:43,  train_loss:0.041,  test_loss:1.940,  test accuracy:0.894\n",
            "epoch:44,  train_loss:0.058,  test_loss:2.363,  test accuracy:0.893\n",
            "epoch:45,  train_loss:0.073,  test_loss:2.524,  test accuracy:0.898\n",
            "epoch:46,  train_loss:0.070,  test_loss:2.435,  test accuracy:0.892\n",
            "epoch:47,  train_loss:0.086,  test_loss:2.566,  test accuracy:0.892\n",
            "epoch:48,  train_loss:0.103,  test_loss:2.639,  test accuracy:0.890\n",
            "epoch:49,  train_loss:0.110,  test_loss:2.842,  test accuracy:0.890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVodLhGytLZt",
        "colab_type": "text"
      },
      "source": [
        "# 4.模型测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XPdAelYmkX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_one(sentence,net,word2Index):\n",
        "  sentence=re.sub(\"[^\\u4e00-\\u9fa5]\", \"\", str(sentence))  # 只保留中文\n",
        "  print(sentence)\n",
        "  sentence=[word2Index[word] if word2Index.get(word) else 0 for word in sentence]\n",
        "  sentence=sentence+[0]*(maxlen-len(sentence)) if len(sentence)<maxlen else sentence[0:300]\n",
        "  print(sentence)\n",
        "  sentence=torch.tensor(np.array(sentence)).view(-1,len(sentence)).cuda()\n",
        "  label=torch.argmax(net(sentence),dim=1).data.item()\n",
        "  print(label)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swS9CG11p7el",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "75cae6f2-afe6-4394-d7d3-cbaf53fb17ad"
      },
      "source": [
        "sentence=\"一次很不爽的购物，页面上说是第二天能到货，结果货是从陕西发出的，卖家完全知道第二天根本到不了货。多处提到送货入户还有100%送货入户也没有兑现，与客服联系多日，还是把皮球踢到快递公司。算是一个教训吧。\"\n",
        "predict_one(sentence,net,word2Index)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "一次很不爽的购物页面上说是第二天能到货结果货是从陕西发出的卖家完全知道第二天根本到不了货多处提到送货入户还有送货入户也没有兑现与客服联系多日还是把皮球踢到快递公司算是一个教训吧\n",
            "[671, 3613, 2523, 679, 4272, 4638, 6579, 4289, 7552, 7481, 677, 6432, 3221, 5018, 753, 1921, 5543, 1168, 6573, 5310, 3362, 6573, 3221, 794, 7362, 6205, 1355, 1139, 4638, 1297, 2157, 2130, 1059, 4761, 6887, 5018, 753, 1921, 3418, 3315, 1168, 679, 749, 6573, 1914, 1905, 2990, 1168, 6843, 6573, 1057, 2787, 6820, 3300, 6843, 6573, 1057, 2787, 738, 3766, 3300, 1050, 4385, 680, 2145, 3302, 5468, 5143, 1914, 3189, 6820, 3221, 2828, 4649, 4413, 6677, 1168, 2571, 6853, 1062, 1385, 5050, 3221, 671, 702, 3136, 6378, 1416, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBFoL2s0qgHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}