{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1JCgBvrteetnvN9wCCfPIHXdn4sfy3Ee8",
      "authorship_tag": "ABX9TyNZ13fnkDCxhwU6ZMRgFsSi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/nlp-ml-dl-notes/blob/master/code/textclassification/text_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm5SzLas6Eya",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 基于TextCnn的中文文本分类\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp7YafFb6RZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing import sequence"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx5qm59I8ZVJ",
        "colab_type": "text"
      },
      "source": [
        "# 1.数据预处理及准备训练和测试数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbdAAKOsmehk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen=300\n",
        "batch_size=128"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc5ncWSemaNv",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 文本预处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNI2O_JijxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def textToChars(filePath):\n",
        "    \"\"\"\n",
        "    读取文本文件并进行处理\n",
        "    :param filePath:文件路径\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    df = pd.read_excel(filePath)\n",
        "    for index, row in df.iterrows():\n",
        "        row = re.sub(\"[^\\u4e00-\\u9fa5]\", \"\", str(row))  # 只保留中文\n",
        "        lines.append(list(row))\n",
        "    return lines\n",
        "\n",
        "\n",
        "def getWordIndex(vocabPath):\n",
        "    \"\"\"\n",
        "    获取word2Index,index2Word\n",
        "    :param vocabPath:词汇文件\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    word2Index = {}\n",
        "    with open(vocabPath, encoding=\"utf-8\") as f:\n",
        "        for line in f.readlines():\n",
        "            word2Index[line.strip()] = len(word2Index)\n",
        "    index2Word = dict(zip(word2Index.values(), word2Index.keys()))\n",
        "    return word2Index, index2Word\n",
        "\n",
        "\n",
        "def lodaData(posFile, negFile, word2Index):\n",
        "    \"\"\"\n",
        "    获取训练数据\n",
        "    :param posFile:正样本文件\n",
        "    :param negFile:负样本文件\n",
        "    :param word2Index:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    posLines = textToChars(posFile)\n",
        "    negLines = textToChars(negFile)\n",
        "    posIndexLines = [[word2Index[word] if word2Index.get(word) else 0 for word in line] for line in posLines]\n",
        "    negIndexLines = [[word2Index[word] if word2Index.get(word) else 0 for word in line] for line in negLines]\n",
        "    lines = posIndexLines + negIndexLines\n",
        "    print(\"训练样本和测试样本共：%d 个\"%(len(lines)))\n",
        "    # lens = [len(line) for line in lines]\n",
        "    labels = [1] * len(posIndexLines) + [0] * len(negIndexLines)\n",
        "    padSequences = sequence.pad_sequences(lines, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
        "    X_train,X_test,y_train,y_test=train_test_split(padSequences,np.array(labels),test_size=0.2,random_state=42)\n",
        "    return X_train,X_test,y_train,y_test"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44V6PzFMkvx_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ab74262a-a8ec-4e27-9efe-568ef8eff68c"
      },
      "source": [
        "vocabPath=\"/content/drive/My Drive/data/vocab.txt\"\n",
        "negFilePath=\"/content/drive/My Drive/data/text_classify/sentiment/neg.xls\"\n",
        "posFilePath=\"/content/drive/My Drive/data/text_classify/sentiment/pos.xls\"\n",
        "word2Index, index2Word=getWordIndex(vocabPath)\n",
        "X_train,X_test,y_train,y_test=lodaData(posFile=posFilePath,negFile=negFilePath,word2Index=word2Index)\n",
        "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "训练样本和测试样本共：21103 个\n",
            "(16882, 300) (4221, 300) (16882,) (4221,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9V_3kJ5m3Qe",
        "colab_type": "text"
      },
      "source": [
        "## 1.2  自定义Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9hDvinn8VSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(self,features,labels):\n",
        "    \"\"\"\n",
        "    features:文本向量化后的特征\n",
        "    labels:标签向量 \n",
        "    \"\"\"\n",
        "    self.features=features\n",
        "    self.labels=labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.features.shape[0]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.features[index],self.labels[index]\n",
        "\n",
        "    \n",
        "train_dataset=MyDataset(X_train,y_train)\n",
        "test_dataset=MyDataset(X_test,y_test)\n",
        "train_dataloader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdoBfhY4o0lD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6df2e27c-25e5-4de0-a55f-77192b076984"
      },
      "source": [
        "for features,labels in train_dataloader:\n",
        "  print(features.shape,labels.shape)\n",
        "  break"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 300]) torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZjMTsZI8RGy",
        "colab_type": "text"
      },
      "source": [
        "# 2.网络搭建"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWEA0bWv5SBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextCnn(nn.Module):\n",
        "    def __init__(self, param: dict):\n",
        "        super(TextCnn, self).__init__()\n",
        "        input_channel = 1  # input channel size\n",
        "        output_channel = param[\"output_channel\"]  # output channel size\n",
        "        kernel_size = param[\"kernel_size\"]\n",
        "        vocab_size = param[\"vocab_size\"]\n",
        "        embedding_dim = param[\"embedding_dim\"]\n",
        "        dropout = param[\"dropout\"]\n",
        "        class_num = param[\"class_num\"]\n",
        "        self.param = param\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim,padding_idx=0)\n",
        "        self.conv1 = nn.Conv2d(input_channel, output_channel, (kernel_size[0], embedding_dim))\n",
        "        self.conv2 = nn.Conv2d(input_channel, output_channel, (kernel_size[1], embedding_dim))\n",
        "        self.conv3 = nn.Conv2d(input_channel, output_channel, (kernel_size[2], embedding_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(len(kernel_size) * output_channel, class_num)\n",
        "\n",
        "    def init_embedding(self, embedding_matrix):\n",
        "        self.embedding.weight = nn.Parameter(torch.Tensor(embedding_matrix))\n",
        "\n",
        "    @staticmethod\n",
        "    def conv_pool(x, conv):\n",
        "        \"\"\"\n",
        "        卷积+池化\n",
        "        :param x:[batch_size,1,sequence_length,embedding_dim]\n",
        "        :param conv:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        x = conv(x)  # 卷积， [batch_size,output_channel,h_out,1]\n",
        "        x = F.relu((x.squeeze(3)))  # 去掉最后一维,[batch_size,output_channel,h_out]\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)  # [batch_size,output_channel]\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        :param x:[batch_size,sequence_length]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)  # [batch_size,sequence_length,embedding_dim]\n",
        "        x = x.unsqueeze(1)  # 增加一个channel维度 [batch_size,1,sequence_length,embedding_dim]\n",
        "        x1 = self.conv_pool(x, self.conv1)  # [batch_size,output_channel]\n",
        "        x2 = self.conv_pool(x, self.conv2)  # [batch_size,output_channel]\n",
        "        x3 = self.conv_pool(x, self.conv3)  # [batch_size,output_channel]\n",
        "        x = torch.cat((x1, x2, x3), 1)  # [batch_size,output_channel*3]\n",
        "        x = self.dropout(x)\n",
        "        logit = F.log_softmax(self.fc1(x), dim=1)\n",
        "        return logit\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1XYBiWXintx",
        "colab_type": "text"
      },
      "source": [
        "# 3.模型训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q2_ISso8PA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textCNNParams={\n",
        "    \"vocab_size\":len(word2Index),\n",
        "    \"embedding_dim\":100,\n",
        "    \"class_num\":2,\n",
        "    \"output_channel\":4,\n",
        "    \"kernel_size\":[3,4,5],\n",
        "    \"dropout\":0.2\n",
        "}"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uAtvtG6nrCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net=TextCnn(textCNNParams)\n",
        "# net.init_weights()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzLfybsqoDeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "5abd0d76-997a-4f5f-8116-97a7ff457cd3"
      },
      "source": [
        "net.cuda()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextCnn(\n",
              "  (embedding): Embedding(21128, 100, padding_idx=0)\n",
              "  (conv1): Conv2d(1, 4, kernel_size=(3, 100), stride=(1, 1))\n",
              "  (conv2): Conv2d(1, 4, kernel_size=(4, 100), stride=(1, 1))\n",
              "  (conv3): Conv2d(1, 4, kernel_size=(5, 100), stride=(1, 1))\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (fc1): Linear(in_features=12, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPMOhQB7o0oD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer=torch.optim.SGD(net.parameters(),lr=0.01)\n",
        "criterion=nn.NLLLoss()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc1EECdloKBV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "e524ada7-0440-4567-822a-0babc325efcb"
      },
      "source": [
        "for epoch in range(10):\n",
        "  total_train_loss=[]\n",
        "  net.train()\n",
        "  for i,(feature,label) in enumerate(train_dataloader):\n",
        "    feature=feature.cuda()\n",
        "    label=label.cuda()\n",
        "    y_pred=net(feature.long()) #前向计算\n",
        "    loss=criterion(y_pred,label) #计算损失\n",
        "    optimizer.zero_grad() #清除梯度\n",
        "    loss.backward() #计算梯度，误差回传\n",
        "    optimizer.step() #更新参数\n",
        "    total_train_loss.append(loss.data.item())\n",
        "  total_valid_loss=[]\n",
        "  pred_true_labels=0\n",
        "  net.eval()\n",
        "  for i,(feature_test,label_test) in enumerate(test_dataloader):\n",
        "    feature_test=feature_test.cuda()\n",
        "    label_test=label_test.cuda()\n",
        "    with torch.no_grad():\n",
        "      pred_test=net(feature_test.long())\n",
        "      test_loss=criterion(pred_test,label_test)\n",
        "      total_valid_loss.append(test_loss.data.item())\n",
        "      # accu=torch.sum((torch.argmax(pred_test,dim=1)==label_test)).data.item()/feature_test.shape[0]\n",
        "      pred_true_labels+=torch.sum(torch.argmax(pred_test,dim=1)==label_test).data.item()\n",
        "      \n",
        "  print(\"epoch:{},train_loss:{},test_loss:{},accuracy:{}\".format(epoch,np.mean(total_train_loss),np.mean(total_valid_loss),pred_true_labels/len(test_dataset)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0,train_loss:0.11022299110437885,test_loss:0.005435514842357599,accuracy:1.0\n",
            "epoch:1,train_loss:0.025626315403673234,test_loss:0.0016127865140636761,accuracy:1.0\n",
            "epoch:2,train_loss:0.01653201541877493,test_loss:0.0007606070474580382,accuracy:1.0\n",
            "epoch:3,train_loss:0.014247172505058574,test_loss:0.00043912876263317287,accuracy:1.0\n",
            "epoch:4,train_loss:0.01097160065370245,test_loss:0.0002832797701668107,accuracy:1.0\n",
            "epoch:5,train_loss:0.010533176803481623,test_loss:0.00022185035579075867,accuracy:1.0\n",
            "epoch:6,train_loss:0.007795979900313823,test_loss:0.0001584696825981761,accuracy:1.0\n",
            "epoch:7,train_loss:0.006695934636588914,test_loss:0.00011782544364299004,accuracy:1.0\n",
            "epoch:8,train_loss:0.0052681467575232755,test_loss:9.00564286205212e-05,accuracy:1.0\n",
            "epoch:9,train_loss:0.00692182230660833,test_loss:8.227612234997761e-05,accuracy:1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVodLhGytLZt",
        "colab_type": "text"
      },
      "source": [
        "# 4.模型测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XPdAelYmkX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_one(sentence,net,word2Index):\n",
        "  sentence=re.sub(\"[^\\u4e00-\\u9fa5]\", \"\", str(sentence))  # 只保留中文\n",
        "  print(sentence)\n",
        "  sentence=[word2Index[word] if word2Index.get(word) else 0 for word in sentence]\n",
        "  sentence=sentence+[0]*(maxlen-len(sentence)) if len(sentence)<maxlen else sentence[0:300]\n",
        "  print(sentence)\n",
        "  sentence=torch.tensor(np.array(sentence)).view(-1,len(sentence)).cuda()\n",
        "  label=torch.argmax(net(sentence),dim=1).data.item()\n",
        "  print(label)\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swS9CG11p7el",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a74f7c68-451c-4d4c-cc6c-784360980e62"
      },
      "source": [
        "sentence=\"一次很不爽的购物，页面上说是第二天能到货，结果货是从陕西发出的，卖家完全知道第二天根本到不了货。多处提到送货入户还有100%送货入户也没有兑现，与客服联系多日，还是把皮球踢到快递公司。算是一个教训吧。\"\n",
        "predict_one(sentence,net,word2Index)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "一次很不爽的购物页面上说是第二天能到货结果货是从陕西发出的卖家完全知道第二天根本到不了货多处提到送货入户还有送货入户也没有兑现与客服联系多日还是把皮球踢到快递公司算是一个教训吧\n",
            "[671, 3613, 2523, 679, 4272, 4638, 6579, 4289, 7552, 7481, 677, 6432, 3221, 5018, 753, 1921, 5543, 1168, 6573, 5310, 3362, 6573, 3221, 794, 7362, 6205, 1355, 1139, 4638, 1297, 2157, 2130, 1059, 4761, 6887, 5018, 753, 1921, 3418, 3315, 1168, 679, 749, 6573, 1914, 1905, 2990, 1168, 6843, 6573, 1057, 2787, 6820, 3300, 6843, 6573, 1057, 2787, 738, 3766, 3300, 1050, 4385, 680, 2145, 3302, 5468, 5143, 1914, 3189, 6820, 3221, 2828, 4649, 4413, 6677, 1168, 2571, 6853, 1062, 1385, 5050, 3221, 671, 702, 3136, 6378, 1416, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBFoL2s0qgHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}