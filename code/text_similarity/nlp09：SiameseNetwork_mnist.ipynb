{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp09ï¼šSiameseNetwork_mnist.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZeRl0co1PKoAb2EQ2z+l8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/nlp-ml-dl-notes/blob/master/code/text_similarity/nlp09%EF%BC%9ASiameseNetwork_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geZaapTS4af0",
        "outputId": "46b12e4a-2f88-4cb6-9bdc-d004a210a1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1XxDoxGwpnr"
      },
      "source": [
        "import codecs\n",
        "import errno\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets.mnist\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGg3Uzq34iEe"
      },
      "source": [
        "do_learn=True\n",
        "save_frequency=2\n",
        "batch_size=64\n",
        "lr=0.001\n",
        "num_epochs=10\n",
        "weight_decay=0.0001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLCzi8Xl3SiC"
      },
      "source": [
        "def get_int(b):\n",
        "  return int(codecs.encode(b,\"hex\"),16)\n",
        "\n",
        "def read_label_file(path):\n",
        "  with open(path,\"rb\")as f:\n",
        "    data=f.read()\n",
        "  assert get_int(data[:4])==2049\n",
        "  length=get_int(data[4:8])\n",
        "  parsed=np.frombuffer(data,dtype=np.uint8,offset=8)\n",
        "  return torch.from_numpy(parsed).view(length).long()\n",
        "\n",
        "def read_image_file(path):\n",
        "  with open(path,\"rb\") as f:\n",
        "    data=f.read()\n",
        "\n",
        "  assert get_int(data[:4])==2051\n",
        "  length=get_int(data[4:8])    \n",
        "  num_rows=get_int(data[8:12])\n",
        "  num_cols=get_int(data[12:16])\n",
        "  images=[]\n",
        "  parsed=np.frombuffer(data,dtype=np.uint8,offset=16)\n",
        "  return torch.from_numpy(parsed).view(length,num_rows,num_cols)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esH_JXJaS4Io"
      },
      "source": [
        "class BalancedMNISTPair(torch.utils.data.Dataset):\n",
        "  \"\"\"Dataset that on each iteration provides two random pairs of\n",
        "  MNIST images. One pair is of the same number (positive sample), one\n",
        "  is of two different numbers (negative sample).\n",
        "  \"\"\"\n",
        "  urls = [\n",
        "    'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
        "    'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
        "    'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
        "    'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
        "  ]\n",
        "  raw_folder = 'raw'\n",
        "  processed_folder = 'processed'\n",
        "  training_file = 'training.pt'\n",
        "  test_file = 'test.pt'\n",
        "  \n",
        "  def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "    self.root = os.path.expanduser(root)\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.train = train # training set or test set\n",
        "    \n",
        "    if download:\n",
        "        self.download()\n",
        "        \n",
        "    if not self._check_exists():\n",
        "        raise RuntimeError('Dataset not found.' + ' You can use download=True to download it')\n",
        "        \n",
        "    if self.train:\n",
        "        self.train_data, self.train_labels = torch.load(\n",
        "          os.path.join(self.root, self.processed_folder, self.training_file))\n",
        "        \n",
        "        train_labels_class = []\n",
        "        train_data_class = []\n",
        "        for i in range(10):\n",
        "          indices = torch.squeeze((self.train_labels == i).nonzero())\n",
        "          train_labels_class.append(torch.index_select(self.train_labels, 0, indices))\n",
        "          train_data_class.append(torch.index_select(self.train_data, 0, indices))\n",
        "          \n",
        "        # generate balanced pairs\n",
        "        self.train_data = []\n",
        "        self.train_labels = []\n",
        "        lengths = [x.shape[0] for x in train_labels_class]\n",
        "        for i in range(10):\n",
        "          for j in range(500): # create 500 pairs\n",
        "              rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
        "              if rnd_cls >= i:\n",
        "                rnd_cls = rnd_cls + 1\n",
        "\n",
        "              rnd_dist = random.randint(0, 100)\n",
        "                \n",
        "              self.train_data.append(torch.stack([train_data_class[i][j], train_data_class[i][j+rnd_dist], train_data_class[rnd_cls][j]]))\n",
        "              self.train_labels.append([1,0])\n",
        "\n",
        "        self.train_data = torch.stack(self.train_data)\n",
        "        self.train_labels = torch.tensor(self.train_labels)\n",
        "              \n",
        "    else:\n",
        "        self.test_data, self.test_labels = torch.load(\n",
        "          os.path.join(self.root, self.processed_folder, self.test_file))\n",
        "        \n",
        "        test_labels_class = []\n",
        "        test_data_class = []\n",
        "        for i in range(10):\n",
        "          indices = torch.squeeze((self.test_labels == i).nonzero())\n",
        "          test_labels_class.append(torch.index_select(self.test_labels, 0, indices))\n",
        "          test_data_class.append(torch.index_select(self.test_data, 0, indices))\n",
        "          \n",
        "        # generate balanced pairs\n",
        "        self.test_data = []\n",
        "        self.test_labels = []\n",
        "        lengths = [x.shape[0] for x in test_labels_class]\n",
        "        for i in range(10):\n",
        "          for j in range(500): # create 500 pairs\n",
        "              rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
        "              if rnd_cls >= i:\n",
        "                rnd_cls = rnd_cls + 1\n",
        "\n",
        "              rnd_dist = random.randint(0, 100)\n",
        "                \n",
        "              self.test_data.append(torch.stack([test_data_class[i][j], test_data_class[i][j+rnd_dist], test_data_class[rnd_cls][j]]))\n",
        "              self.test_labels.append([1,0])\n",
        "\n",
        "        self.test_data = torch.stack(self.test_data)\n",
        "        self.test_labels = torch.tensor(self.test_labels)\n",
        "        \n",
        "  def __getitem__(self, index):\n",
        "    if self.train:\n",
        "        imgs, target = self.train_data[index], self.train_labels[index]\n",
        "    else:\n",
        "        imgs, target = self.test_data[index], self.test_labels[index]\n",
        "        \n",
        "    img_ar = []\n",
        "    for i in range(len(imgs)):\n",
        "        img = Image.fromarray(imgs[i].numpy(), mode='L')\n",
        "        if self.transform is not None:\n",
        "          img = self.transform(img)\n",
        "        img_ar.append(img)\n",
        "        \n",
        "    if self.target_transform is not None:\n",
        "        target = self.target_transform(target)\n",
        "        \n",
        "    return img_ar, target\n",
        "  \n",
        "  def __len__(self):\n",
        "    if self.train:\n",
        "        return len(self.train_data)\n",
        "    else:\n",
        "        return len(self.test_data)\n",
        "    \n",
        "  def _check_exists(self):\n",
        "    return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
        "        os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
        "  \n",
        "  def download(self):\n",
        "    \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
        "    from six.moves import urllib\n",
        "    import gzip\n",
        "\n",
        "    if self._check_exists():\n",
        "        return\n",
        "\n",
        "    # download files\n",
        "    try:\n",
        "        os.makedirs(os.path.join(self.root, self.raw_folder))\n",
        "        os.makedirs(os.path.join(self.root, self.processed_folder))\n",
        "    except OSError as e:\n",
        "        if e.errno == errno.EEXIST:\n",
        "          pass\n",
        "        else:\n",
        "          raise\n",
        "\n",
        "    for url in self.urls:\n",
        "        print('Downloading ' + url)\n",
        "        data = urllib.request.urlopen(url)\n",
        "        filename = url.rpartition('/')[2]\n",
        "        file_path = os.path.join(self.root, self.raw_folder, filename)\n",
        "        with open(file_path, 'wb') as f:\n",
        "          f.write(data.read())\n",
        "        with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
        "              gzip.GzipFile(file_path) as zip_f:\n",
        "          out_f.write(zip_f.read())\n",
        "        os.unlink(file_path)\n",
        "\n",
        "    # process and save as torch files\n",
        "    print('Processing...')\n",
        "\n",
        "    training_set = (\n",
        "        read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
        "        read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
        "    )\n",
        "    test_set = (\n",
        "        read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
        "        read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
        "    )\n",
        "    with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
        "        torch.save(training_set, f)\n",
        "    with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
        "        torch.save(test_set, f)\n",
        "\n",
        "    print('Done!')\n",
        "\n",
        "  def __repr__(self):\n",
        "    fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "    fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "    tmp = 'train' if self.train is True else 'test'\n",
        "    fmt_str += '    Split: {}\\n'.format(tmp)\n",
        "    fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "    tmp = '    Transforms (if any): '\n",
        "    fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "    tmp = '    Target Transforms (if any): '\n",
        "    fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "    return fmt_str"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQdzp2b6Sue8"
      },
      "source": [
        "# å®šä¹‰æ¨¡åž‹ç»“æž„"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK6F71R5TI9K"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1=nn.Conv2d(1,64,7)\n",
        "    self.pool1=nn.MaxPool2d(2)\n",
        "    self.conv2=nn.Conv2d(64,128,5)\n",
        "    self.conv3=nn.Conv2d(128,256,5)\n",
        "    self.linear1=nn.Linear(2304,512)\n",
        "    self.linear2=nn.Linear(512,2)\n",
        "\n",
        "  def forward(self,data):\n",
        "    res=[]\n",
        "    for i in range(2):\n",
        "      x=data[i]\n",
        "      x=self.conv1(x)\n",
        "      x=F.relu(x)\n",
        "      x=self.pool1(x)\n",
        "      x=self.conv2(x)\n",
        "      x=F.relu(x)\n",
        "      x=self.conv3(x)\n",
        "      x=F.relu(x)\n",
        "\n",
        "      x=x.view(x.shape[0],-1)\n",
        "      x=self.linear1(x)\n",
        "      res.append(F.relu(x))\n",
        "\n",
        "    res=torch.abs(res[1]-res[0])\n",
        "    res=self.linear2(res)\n",
        "    return res"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tDub2QiSjUH",
        "outputId": "203dd9ff-a045-479f-9803-54012dba49c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train(model, device, train_loader, epoch, optimizer):\n",
        "   model.train()\n",
        "   \n",
        "   for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      for i in range(len(data)):\n",
        "         data[i] = data[i].to(device)\n",
        "         \n",
        "      optimizer.zero_grad()\n",
        "      output_positive = model(data[:2])\n",
        "      output_negative = model(data[0:3:2])\n",
        "      \n",
        "      target = target.type(torch.LongTensor).to(device)\n",
        "      target_positive = torch.squeeze(target[:,0])\n",
        "      target_negative = torch.squeeze(target[:,1])\n",
        "      \n",
        "      loss_positive = F.cross_entropy(output_positive, target_positive)\n",
        "      loss_negative = F.cross_entropy(output_negative, target_negative)\n",
        "      \n",
        "      loss = loss_positive + loss_negative\n",
        "      loss.backward()\n",
        "      \n",
        "      optimizer.step()\n",
        "      if batch_idx % 10 == 0:\n",
        "         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx*batch_size, len(train_loader.dataset), 100. * batch_idx*batch_size / len(train_loader.dataset),\n",
        "            loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "   model.eval()\n",
        "   \n",
        "   with torch.no_grad():\n",
        "      accurate_labels = 0\n",
        "      all_labels = 0\n",
        "      loss = 0\n",
        "      for batch_idx, (data, target) in enumerate(test_loader):\n",
        "         for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "            \n",
        "         output_positive = model(data[:2])\n",
        "         output_negative = model(data[0:3:2])\n",
        "            \n",
        "         target = target.type(torch.LongTensor).to(device)\n",
        "         target_positive = torch.squeeze(target[:,0])\n",
        "         target_negative = torch.squeeze(target[:,1])\n",
        "            \n",
        "         loss_positive = F.cross_entropy(output_positive, target_positive)\n",
        "         loss_negative = F.cross_entropy(output_negative, target_negative)\n",
        "            \n",
        "         loss = loss + loss_positive + loss_negative\n",
        "            \n",
        "         accurate_labels_positive = torch.sum(torch.argmax(output_positive, dim=1) == target_positive).cpu()\n",
        "         accurate_labels_negative = torch.sum(torch.argmax(output_negative, dim=1) == target_negative).cpu()\n",
        "            \n",
        "         accurate_labels = accurate_labels + accurate_labels_positive + accurate_labels_negative\n",
        "         all_labels = all_labels + len(target_positive) + len(target_negative)\n",
        "      \n",
        "      accuracy = 100. * accurate_labels / all_labels\n",
        "      print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))\n",
        "   \n",
        "def oneshot(model, device, data):\n",
        "   model.eval()\n",
        "\n",
        "   with torch.no_grad():\n",
        "      for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "      \n",
        "      output = model(data)\n",
        "      return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()\n",
        "\n",
        "def main():\n",
        "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "   trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "   \n",
        "   model = Net().to(device)\n",
        "   \n",
        "   if do_learn: # training mode\n",
        "      train_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=True, download=True, transform=trans), batch_size=batch_size, shuffle=True)\n",
        "      test_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=False, download=True, transform=trans), batch_size=batch_size, shuffle=False)\n",
        "      \n",
        "      optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "      for epoch in range(num_epochs):\n",
        "         train(model, device, train_loader, epoch, optimizer)\n",
        "         test(model, device, test_loader)\n",
        "         if epoch & save_frequency == 0:\n",
        "            torch.save(model, 'siamese_{:03}.pt'.format(epoch))\n",
        "   else: # prediction\n",
        "      prediction_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=False, download=True, transform=trans), batch_size=1, shuffle=True)\n",
        "      model.load_state_dict(torch.load(load_model_path))\n",
        "      data = []\n",
        "      data.extend(next(iter(prediction_loader))[0][:3:2])\n",
        "      same = oneshot(model, device, data)\n",
        "      if same > 0:\n",
        "         print('These two images are of the same number')\n",
        "      else:\n",
        "         print('These two images are not of the same number')\n",
        "         \n",
        "if __name__ == '__main__':\n",
        "   main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done!\n",
            "Train Epoch: 0 [0/5000 (0%)]\tLoss: 1.386388\n",
            "Train Epoch: 0 [640/5000 (13%)]\tLoss: 1.236525\n",
            "Train Epoch: 0 [1280/5000 (26%)]\tLoss: 0.974270\n",
            "Train Epoch: 0 [1920/5000 (38%)]\tLoss: 1.038586\n",
            "Train Epoch: 0 [2560/5000 (51%)]\tLoss: 0.915066\n",
            "Train Epoch: 0 [3200/5000 (64%)]\tLoss: 0.700401\n",
            "Train Epoch: 0 [3840/5000 (77%)]\tLoss: 0.711285\n",
            "Train Epoch: 0 [4480/5000 (90%)]\tLoss: 0.799151\n",
            "Test accuracy: 8155/10000 (81.550%)\tLoss: 63.527763\n",
            "Train Epoch: 1 [0/5000 (0%)]\tLoss: 0.840174\n",
            "Train Epoch: 1 [640/5000 (13%)]\tLoss: 0.407506\n",
            "Train Epoch: 1 [1280/5000 (26%)]\tLoss: 0.525354\n",
            "Train Epoch: 1 [1920/5000 (38%)]\tLoss: 0.751752\n",
            "Train Epoch: 1 [2560/5000 (51%)]\tLoss: 0.527764\n",
            "Train Epoch: 1 [3200/5000 (64%)]\tLoss: 0.678261\n",
            "Train Epoch: 1 [3840/5000 (77%)]\tLoss: 0.632383\n",
            "Train Epoch: 1 [4480/5000 (90%)]\tLoss: 0.462189\n",
            "Test accuracy: 8974/10000 (89.740%)\tLoss: 41.585396\n",
            "Train Epoch: 2 [0/5000 (0%)]\tLoss: 0.333985\n",
            "Train Epoch: 2 [640/5000 (13%)]\tLoss: 0.529698\n",
            "Train Epoch: 2 [1280/5000 (26%)]\tLoss: 0.270961\n",
            "Train Epoch: 2 [1920/5000 (38%)]\tLoss: 0.215222\n",
            "Train Epoch: 2 [2560/5000 (51%)]\tLoss: 0.463632\n",
            "Train Epoch: 2 [3200/5000 (64%)]\tLoss: 0.250592\n",
            "Train Epoch: 2 [3840/5000 (77%)]\tLoss: 0.273420\n",
            "Train Epoch: 2 [4480/5000 (90%)]\tLoss: 0.263825\n",
            "Test accuracy: 9274/10000 (92.740%)\tLoss: 31.573954\n",
            "Train Epoch: 3 [0/5000 (0%)]\tLoss: 0.132995\n",
            "Train Epoch: 3 [640/5000 (13%)]\tLoss: 0.156947\n",
            "Train Epoch: 3 [1280/5000 (26%)]\tLoss: 0.105598\n",
            "Train Epoch: 3 [1920/5000 (38%)]\tLoss: 0.112060\n",
            "Train Epoch: 3 [2560/5000 (51%)]\tLoss: 0.230849\n",
            "Train Epoch: 3 [3200/5000 (64%)]\tLoss: 0.175800\n",
            "Train Epoch: 3 [3840/5000 (77%)]\tLoss: 0.137684\n",
            "Train Epoch: 3 [4480/5000 (90%)]\tLoss: 0.158168\n",
            "Test accuracy: 9384/10000 (93.840%)\tLoss: 27.706055\n",
            "Train Epoch: 4 [0/5000 (0%)]\tLoss: 0.104505\n",
            "Train Epoch: 4 [640/5000 (13%)]\tLoss: 0.110305\n",
            "Train Epoch: 4 [1280/5000 (26%)]\tLoss: 0.055767\n",
            "Train Epoch: 4 [1920/5000 (38%)]\tLoss: 0.105167\n",
            "Train Epoch: 4 [2560/5000 (51%)]\tLoss: 0.096558\n",
            "Train Epoch: 4 [3200/5000 (64%)]\tLoss: 0.040930\n",
            "Train Epoch: 4 [3840/5000 (77%)]\tLoss: 0.027496\n",
            "Train Epoch: 4 [4480/5000 (90%)]\tLoss: 0.029876\n",
            "Test accuracy: 9313/10000 (93.130%)\tLoss: 35.062977\n",
            "Train Epoch: 5 [0/5000 (0%)]\tLoss: 0.047546\n",
            "Train Epoch: 5 [640/5000 (13%)]\tLoss: 0.294049\n",
            "Train Epoch: 5 [1280/5000 (26%)]\tLoss: 0.193511\n",
            "Train Epoch: 5 [1920/5000 (38%)]\tLoss: 0.044427\n",
            "Train Epoch: 5 [2560/5000 (51%)]\tLoss: 0.110589\n",
            "Train Epoch: 5 [3200/5000 (64%)]\tLoss: 0.045106\n",
            "Train Epoch: 5 [3840/5000 (77%)]\tLoss: 0.162295\n",
            "Train Epoch: 5 [4480/5000 (90%)]\tLoss: 0.092252\n",
            "Test accuracy: 9391/10000 (93.910%)\tLoss: 29.587965\n",
            "Train Epoch: 6 [0/5000 (0%)]\tLoss: 0.105038\n",
            "Train Epoch: 6 [640/5000 (13%)]\tLoss: 0.036711\n",
            "Train Epoch: 6 [1280/5000 (26%)]\tLoss: 0.093073\n",
            "Train Epoch: 6 [1920/5000 (38%)]\tLoss: 0.039443\n",
            "Train Epoch: 6 [2560/5000 (51%)]\tLoss: 0.029748\n",
            "Train Epoch: 6 [3200/5000 (64%)]\tLoss: 0.053526\n",
            "Train Epoch: 6 [3840/5000 (77%)]\tLoss: 0.066336\n",
            "Train Epoch: 6 [4480/5000 (90%)]\tLoss: 0.069534\n",
            "Test accuracy: 9510/10000 (95.100%)\tLoss: 26.962774\n",
            "Train Epoch: 7 [0/5000 (0%)]\tLoss: 0.022964\n",
            "Train Epoch: 7 [640/5000 (13%)]\tLoss: 0.042354\n",
            "Train Epoch: 7 [1280/5000 (26%)]\tLoss: 0.058485\n",
            "Train Epoch: 7 [1920/5000 (38%)]\tLoss: 0.028813\n",
            "Train Epoch: 7 [2560/5000 (51%)]\tLoss: 0.020319\n",
            "Train Epoch: 7 [3200/5000 (64%)]\tLoss: 0.045812\n",
            "Train Epoch: 7 [3840/5000 (77%)]\tLoss: 0.187601\n",
            "Train Epoch: 7 [4480/5000 (90%)]\tLoss: 0.005830\n",
            "Test accuracy: 9451/10000 (94.510%)\tLoss: 31.653055\n",
            "Train Epoch: 8 [0/5000 (0%)]\tLoss: 0.034416\n",
            "Train Epoch: 8 [640/5000 (13%)]\tLoss: 0.012986\n",
            "Train Epoch: 8 [1280/5000 (26%)]\tLoss: 0.036925\n",
            "Train Epoch: 8 [1920/5000 (38%)]\tLoss: 0.079469\n",
            "Train Epoch: 8 [2560/5000 (51%)]\tLoss: 0.010183\n",
            "Train Epoch: 8 [3200/5000 (64%)]\tLoss: 0.055634\n",
            "Train Epoch: 8 [3840/5000 (77%)]\tLoss: 0.013442\n",
            "Train Epoch: 8 [4480/5000 (90%)]\tLoss: 0.022214\n",
            "Test accuracy: 9447/10000 (94.470%)\tLoss: 30.076372\n",
            "Train Epoch: 9 [0/5000 (0%)]\tLoss: 0.015019\n",
            "Train Epoch: 9 [640/5000 (13%)]\tLoss: 0.008702\n",
            "Train Epoch: 9 [1280/5000 (26%)]\tLoss: 0.009446\n",
            "Train Epoch: 9 [1920/5000 (38%)]\tLoss: 0.009299\n",
            "Train Epoch: 9 [2560/5000 (51%)]\tLoss: 0.018761\n",
            "Train Epoch: 9 [3200/5000 (64%)]\tLoss: 0.008480\n",
            "Train Epoch: 9 [3840/5000 (77%)]\tLoss: 0.014903\n",
            "Train Epoch: 9 [4480/5000 (90%)]\tLoss: 0.045853\n",
            "Test accuracy: 9507/10000 (95.070%)\tLoss: 31.426634\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}