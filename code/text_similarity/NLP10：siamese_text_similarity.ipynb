{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP10：siamese_text_similarity",
      "provenance": [],
      "mount_file_id": "14oVUVINsJ3sOA1GESzAaMMbmaKmsHMxo",
      "authorship_tag": "ABX9TyNu/fs29DaenHZdMVKOVBMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/nlp-ml-dl-notes/blob/master/code/text_similarity/NLP10%EF%BC%9Asiamese_text_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUFc3cxna5rm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "  \"\"\"\n",
        "  把字符串数组转换为字符数值索引数组\n",
        "  :param X:string 数组\n",
        "  :param word_to_index:\n",
        "  :param max_len:最长的序列长度\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  m = X.shape[0]\n",
        "  X_indices = np.zeros((m, max_len))\n",
        "  for i in range(m):\n",
        "    # split字符串\n",
        "    sentence_words = X[i].split(\" \")\n",
        "    for j, w in enumerate(sentence_words):\n",
        "      if j >= max_len:\n",
        "        break\n",
        "      X_indices[i, j] = word_to_index[w]\n",
        "  return X_indices\n",
        "\n",
        "\n",
        "def load_dataset(data_dir, max_seq_len, embed_dim, word_level=True):\n",
        "    \"\"\"\n",
        "    读取数据，对数据进行预处理，并生成embed_matrix\n",
        "    :param data_dir:数据目录\n",
        "    :param max_seq_len:\n",
        "    :param embed_dim:词向量维度\n",
        "    :param word_level:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    question_path = os.path.join(data_dir, \"question.csv\")\n",
        "    train_path = os.path.join(data_dir, \"train.csv\")\n",
        "    if word_level:\n",
        "        embed_path = os.path.join(data_dir, \"word_embed.txt\")  # 词向量\n",
        "    else:\n",
        "        embed_path = os.path.join(data_dir, \"char_embed.txt\")  # 字符向量\n",
        "\n",
        "    # 读取数据\n",
        "    question = pd.read_csv(question_path)\n",
        "    train = pd.read_csv(train_path)\n",
        "\n",
        "    # 把train里面的问题id匹配到句子\n",
        "    train = pd.merge(train, question, left_on=[\"q1\"], right_on=[\"qid\"], how=\"left\")  # 匹配第一个问题\n",
        "    train = pd.merge(train, question, left_on=[\"q2\"], right_on=[\"qid\"], how=\"left\")  # 匹配第二个问题\n",
        "\n",
        "    if word_level:\n",
        "        train = train[[\"label\", \"words_x\", \"words_y\"]]\n",
        "    else:\n",
        "        train = train[[\"label\", \"chars_x\", \"chars_y\"]]\n",
        "    train.columns = [\"label\", \"q1\", \"q2\"]\n",
        "\n",
        "    word_to_vec_map = pd.read_csv(embed_path, sep=\" \", header=None, index_col=0)\n",
        "    word = word_to_vec_map.index.values\n",
        "\n",
        "    # word2id,id2word\n",
        "    word_to_index = dict([(word[i], i+1) for i in range(len(word))])\n",
        "    index_to_word = dict([(i+1, word[i]) for i in range(len(word))])\n",
        "\n",
        "    train_q1_indices = sentences_to_indices(train.q1.values, word_to_index, max_seq_len)\n",
        "    train_q2_indices = sentences_to_indices(train.q2.values, word_to_index, max_seq_len)\n",
        "    label = train.label.values\n",
        "\n",
        "    vocab_len = len(word_to_index)+1\n",
        "    embed_matrix = np.zeros((vocab_len, embed_dim))\n",
        "    for word, index in word_to_index.items():\n",
        "        embed_matrix[index, :] = word_to_vec_map.loc[word].values\n",
        "\n",
        "    return train_q1_indices, train_q2_indices, label, embed_matrix, word_to_index, index_to_word\n",
        "\n",
        "\n",
        "def load_test_data(data_dir, max_seq_len, word_level=True):\n",
        "    \"\"\"\n",
        "    读取测试数据\n",
        "    :param max_seq_len:\n",
        "    :param word_level:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    question_path = os.path.join(data_dir, \"question.csv\")\n",
        "    test_path = os.path.join(data_dir, \"test.csv\")\n",
        "    if word_level:\n",
        "        embed_path = os.path.join(data_dir, \"word_embed.txt\")\n",
        "    else:\n",
        "        embed_path = os.path.join(data_dir, \"char_embed.txt\")\n",
        "\n",
        "    # 读取数据\n",
        "    question = pd.read_csv(question_path)\n",
        "    test = pd.read_csv(test_path)\n",
        "\n",
        "    test = pd.merge(test, question, left_on=[\"q1\"], right_on=[\"qid\"], how=\"left\")\n",
        "    test = pd.merge(test, question, left_on=[\"q2\"], right_on=[\"qid\"], how=\"left\")\n",
        "\n",
        "    if word_level:\n",
        "        test = test[[\"words_x\", \"words_y\"]]\n",
        "    else:\n",
        "        test = test[[\"chars_x\", \"chars_y\"]]\n",
        "    test.columns = [\"q1\", \"q2\"]\n",
        "    word_to_vec_map = pd.read_csv(embed_path, sep=\" \", header=None, index_col=0)\n",
        "    word = word_to_vec_map.index.values\n",
        "\n",
        "    # word2id,id2word\n",
        "    word_to_index = dict([(word[i], i+1) for i in range(len(word))])\n",
        "    index_to_word = dict([(i+1, word[i]) for i in range(len(word))])\n",
        "\n",
        "    test_q1_indices = sentences_to_indices(test.q1.values, word_to_index, max_seq_len)\n",
        "    test_q2_indices = sentences_to_indices(test.q2.values, word_to_index, max_seq_len)\n",
        "    return test_q1_indices, test_q2_indices\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciBxhhESbODD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, GaussianNoise, \\\n",
        "    Input, Dropout, LSTM, Activation, BatchNormalization, concatenate, Subtract, Dot, Multiply, Bidirectional, Lambda\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.callbacks as kcallbacks\n",
        "\n",
        "np.random.seed(1)\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 15  # 20 for character level and 15 for word level\n",
        "EMBEDDING_DIM = 300\n",
        "lstm_num = 64\n",
        "lstm_drop = 0.5\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "\n",
        "def trainLSTM(train_q1, train_q2, train_label, embed_matrix):\n",
        "    question1 = Input(shape=(MAX_SEQUENCE_LENGTH,), batch_size=BATCH_SIZE)\n",
        "    question2 = Input(shape=(MAX_SEQUENCE_LENGTH,), batch_size=BATCH_SIZE)\n",
        "    embed_layer = Embedding(embed_matrix.shape[0], EMBEDDING_DIM,weights=[embed_matrix]) #\n",
        "    q1_embed = embed_layer(question1)\n",
        "    q2_embed = embed_layer(question2)\n",
        "\n",
        "    shared_lstm1 = LSTM(lstm_num, return_sequences=True)\n",
        "    shared_lstm2 = LSTM(lstm_num)\n",
        "\n",
        "    q1 = shared_lstm1(q1_embed)\n",
        "    q1 = Dropout(lstm_drop)(q1)\n",
        "    q1 = BatchNormalization()(q1)\n",
        "    q1 = shared_lstm2(q1)\n",
        "\n",
        "    q2 = shared_lstm1(q2_embed)\n",
        "    q2 = Dropout(lstm_drop)(q2)\n",
        "    q2 = BatchNormalization()(q2)\n",
        "    q2 = shared_lstm2(q2)\n",
        "\n",
        "    # 求distance (batch_size,lstm_num)\n",
        "    d = Subtract()([q1, q2])\n",
        "    distance = Multiply()([d, d])\n",
        "    # 求angle (batch_size,lstm_num)\n",
        "    angle = Multiply()([q1, q2])\n",
        "    merged = concatenate([distance, angle])\n",
        "    merged = Dropout(0.3)(merged)\n",
        "    merged = BatchNormalization()(merged)\n",
        "\n",
        "    merged = Dense(256, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.3)(merged)\n",
        "    merged = BatchNormalization()(merged)\n",
        "\n",
        "    merged = Dense(64, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.3)(merged)\n",
        "    merged = BatchNormalization()(merged)\n",
        "\n",
        "    res = Dense(1, activation=\"sigmoid\")(merged)\n",
        "    model = Model(inputs=[question1, question2], outputs=res)\n",
        "    model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "\n",
        "    hist = model.fit([train_q1, train_q2],train_label,epochs=30, batch_size=BATCH_SIZE, validation_split=0.2,shuffle=True)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZEcnLCxbTFA",
        "outputId": "ad51b9c0-7f3d-4afc-9bd2-a4244eb4ddce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "\n",
        "train_q1_indices, train_q2_indices, train_label, embed_matrix, word_to_index, index_to_word = load_dataset(\"/content/drive/My Drive/data/text_similarity/data\", MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, False)\n",
        "print('train_q1: ', train_q1_indices.shape)\n",
        "print('train_q2: ', train_q2_indices.shape)\n",
        "print('train_label: ', tf.one_hot(train_label,depth=2).shape)\n",
        "print('embed_matrix: ', embed_matrix.shape)\n",
        "\n",
        "# 加载test 数据\n",
        "test_q1, test_q2 = load_test_data(\"/content/drive/My Drive/data/text_similarity/data\", MAX_SEQUENCE_LENGTH, word_level=False)\n",
        "print('test_q1: ', test_q1.shape)\n",
        "print('test_q2: ', test_q2.shape)\n",
        "print(\"word_to_index len:\",len(word_to_index))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_q1:  (254386, 15)\n",
            "train_q2:  (254386, 15)\n",
            "train_label:  (254386, 2)\n",
            "embed_matrix:  (3049, 300)\n",
            "test_q1:  (172956, 15)\n",
            "test_q2:  (172956, 15)\n",
            "word_to_index len: 3048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdw5VfWiw7Ro",
        "outputId": "55532b9d-d70e-460c-bfd7-4345cc303510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainLSTM(train_q1_indices[:243000], train_q2_indices[:243000], train_label[:243000], embed_matrix) #数据数量无法整除BATCH_SIZE时会报错"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(100, 15)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(100, 15)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (100, 15, 300)       914700      input_7[0][0]                    \n",
            "                                                                 input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (100, 15, 64)        93440       embedding_3[0][0]                \n",
            "                                                                 embedding_3[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (100, 15, 64)        0           lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (100, 15, 64)        0           lstm_6[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (100, 15, 64)        256         dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (100, 15, 64)        256         dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (100, 64)            33024       batch_normalization_6[0][0]      \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "subtract (Subtract)             (100, 64)            0           lstm_7[0][0]                     \n",
            "                                                                 lstm_7[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (100, 64)            0           subtract[0][0]                   \n",
            "                                                                 subtract[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_4 (Multiply)           (100, 64)            0           lstm_7[0][0]                     \n",
            "                                                                 lstm_7[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (100, 128)           0           multiply_3[0][0]                 \n",
            "                                                                 multiply_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (100, 128)           0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (100, 128)           512         dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (100, 256)           33024       batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (100, 256)           0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (100, 256)           1024        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (100, 64)            16448       batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (100, 64)            0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (100, 64)            256         dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (100, 1)             65          batch_normalization_10[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 1,093,005\n",
            "Trainable params: 1,091,853\n",
            "Non-trainable params: 1,152\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "1944/1944 [==============================] - 48s 25ms/step - loss: 0.4684 - accuracy: 0.7779 - val_loss: 0.3507 - val_accuracy: 0.8452\n",
            "Epoch 2/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.3606 - accuracy: 0.8415 - val_loss: 0.3184 - val_accuracy: 0.8599\n",
            "Epoch 3/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.3290 - accuracy: 0.8561 - val_loss: 0.2952 - val_accuracy: 0.8710\n",
            "Epoch 4/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.3111 - accuracy: 0.8642 - val_loss: 0.2853 - val_accuracy: 0.8785\n",
            "Epoch 5/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.2993 - accuracy: 0.8706 - val_loss: 0.2772 - val_accuracy: 0.8824\n",
            "Epoch 6/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.2896 - accuracy: 0.8752 - val_loss: 0.2753 - val_accuracy: 0.8810\n",
            "Epoch 7/30\n",
            "1944/1944 [==============================] - 46s 24ms/step - loss: 0.2808 - accuracy: 0.8798 - val_loss: 0.2737 - val_accuracy: 0.8827\n",
            "Epoch 8/30\n",
            "1944/1944 [==============================] - 46s 24ms/step - loss: 0.2752 - accuracy: 0.8826 - val_loss: 0.2719 - val_accuracy: 0.8858\n",
            "Epoch 9/30\n",
            "1944/1944 [==============================] - 46s 24ms/step - loss: 0.2696 - accuracy: 0.8855 - val_loss: 0.2665 - val_accuracy: 0.8889\n",
            "Epoch 10/30\n",
            "1944/1944 [==============================] - 46s 24ms/step - loss: 0.2662 - accuracy: 0.8861 - val_loss: 0.2666 - val_accuracy: 0.8868\n",
            "Epoch 11/30\n",
            "1944/1944 [==============================] - 46s 24ms/step - loss: 0.2616 - accuracy: 0.8890 - val_loss: 0.2592 - val_accuracy: 0.8927\n",
            "Epoch 12/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.2583 - accuracy: 0.8898 - val_loss: 0.2564 - val_accuracy: 0.8921\n",
            "Epoch 13/30\n",
            "1944/1944 [==============================] - 46s 24ms/step - loss: 0.2578 - accuracy: 0.8906 - val_loss: 0.2600 - val_accuracy: 0.8912\n",
            "Epoch 14/30\n",
            "1944/1944 [==============================] - 45s 23ms/step - loss: 0.2546 - accuracy: 0.8930 - val_loss: 0.2558 - val_accuracy: 0.8925\n",
            "Epoch 15/30\n",
            "1944/1944 [==============================] - 46s 23ms/step - loss: 0.2512 - accuracy: 0.8930 - val_loss: 0.2560 - val_accuracy: 0.8928\n",
            "Epoch 16/30\n",
            "1944/1944 [==============================] - 45s 23ms/step - loss: 0.2519 - accuracy: 0.8934 - val_loss: 0.2521 - val_accuracy: 0.8949\n",
            "Epoch 17/30\n",
            "1944/1944 [==============================] - 46s 24ms/step - loss: 0.2492 - accuracy: 0.8943 - val_loss: 0.2507 - val_accuracy: 0.8954\n",
            "Epoch 18/30\n",
            "1944/1944 [==============================] - 45s 23ms/step - loss: 0.2484 - accuracy: 0.8950 - val_loss: 0.2555 - val_accuracy: 0.8929\n",
            "Epoch 19/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.2478 - accuracy: 0.8951 - val_loss: 0.2474 - val_accuracy: 0.8957\n",
            "Epoch 20/30\n",
            "1944/1944 [==============================] - 45s 23ms/step - loss: 0.2460 - accuracy: 0.8967 - val_loss: 0.2495 - val_accuracy: 0.8959\n",
            "Epoch 21/30\n",
            "1944/1944 [==============================] - 46s 24ms/step - loss: 0.2438 - accuracy: 0.8969 - val_loss: 0.2473 - val_accuracy: 0.8973\n",
            "Epoch 22/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.2415 - accuracy: 0.8980 - val_loss: 0.2572 - val_accuracy: 0.8922\n",
            "Epoch 23/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.2405 - accuracy: 0.8999 - val_loss: 0.2455 - val_accuracy: 0.8977\n",
            "Epoch 24/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.2377 - accuracy: 0.9001 - val_loss: 0.2495 - val_accuracy: 0.8971\n",
            "Epoch 25/30\n",
            "1944/1944 [==============================] - 48s 24ms/step - loss: 0.2359 - accuracy: 0.9006 - val_loss: 0.2479 - val_accuracy: 0.8973\n",
            "Epoch 26/30\n",
            "1944/1944 [==============================] - 46s 24ms/step - loss: 0.2361 - accuracy: 0.9002 - val_loss: 0.2451 - val_accuracy: 0.8978\n",
            "Epoch 27/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.2346 - accuracy: 0.9014 - val_loss: 0.2480 - val_accuracy: 0.8981\n",
            "Epoch 28/30\n",
            "1944/1944 [==============================] - 46s 24ms/step - loss: 0.2345 - accuracy: 0.9014 - val_loss: 0.2458 - val_accuracy: 0.8982\n",
            "Epoch 29/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.2325 - accuracy: 0.9028 - val_loss: 0.2504 - val_accuracy: 0.8964\n",
            "Epoch 30/30\n",
            "1944/1944 [==============================] - 47s 24ms/step - loss: 0.2316 - accuracy: 0.9029 - val_loss: 0.2468 - val_accuracy: 0.8981\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}