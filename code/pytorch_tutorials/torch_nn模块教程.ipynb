{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch.nn模块教程.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqC28xVq1lImmDLVfzto0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/nlp-ml-dl-notes/blob/master/pytorch_tutorials/torch_nn%E6%A8%A1%E5%9D%97%E6%95%99%E7%A8%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLRnoWPS7Wom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import  torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp1nycup7kQ5",
        "colab_type": "text"
      },
      "source": [
        "##01. torch.nn.EmbeddingBag()\n",
        "\n",
        "class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean')\\\n",
        "计算一个'bags'里的embeddings的均值或和，不用实例化中间的embeddings。默认计算均值。\\\n",
        "对于固定长度的bags:\n",
        "* nn.EmbeddingBag和mode=sum相当于nn.Embedding()之后的torch.sum(dim=1)\n",
        "* 其与mode=mean相当于nn.Embedding()之后的torch.mean(dim=1)\n",
        "\n",
        "然而，nn.EmbeddingBag在时间和内存上更加高效。\\\n",
        "参数：\n",
        "* num_embeddings (int)：embeddings字典的大小\n",
        "* embedding_dim （int）:每个embedding向量的大小(维度)\n",
        "* max_norm (float,可选):如果给出，则重新归一化embeddings，使其范数小于该值\n",
        "* norm_type(float,可选):为max_norm参数计算p范数时的P.\n",
        "* scale_grad_by_freq (boolean, 可选) – 如果给出, 会根据 words 在 mini-batch 中的频率缩放梯度\n",
        "* mode (string, 可选) – ‘sum’ | ‘mean’. 指定减少 bag 的方式. 默认: ‘mean’\n",
        "\n",
        "###Inputs: input, offsets\n",
        "\n",
        "* input (N or BxN): LongTensor, 包括要提取的 embeddings 的索引, 当 input 是形状为 N 的 1D 张量时, 一个给出的 offsets 张量中包括: mini-batch 中每个新序列的起始位置\n",
        "* offsets (B or None): LongTensor, 包括一个 mini-batch 的可变长度序列中的每个新样本的起始位置 如果 input 是 2D (BxN) 的, offset 就不用再给出; 如果 input 是一个 mini-batch 的固定长度的序列, 每个序列的长度为 N\n",
        "###形状：\n",
        "\n",
        "* 输入：LongTensor N, N = 要提取的 embeddings 的数量,\n",
        "或者是 LongTensor BxN, B = mini-batch 中序列的数量, N = 每个序列中 embeddings 的数量\n",
        "\n",
        "* Offsets: LongTensor B, B = bags 的数量, 值为每个 bag 中 input 的 offset, i.e. 是长度的累加. Offsets 不会给出, 如果 Input是 2D 的BxN 张量, 输入被认为是固定长度的序列\n",
        "输出：(B, embedding_dim)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGv4pUiD7smc",
        "colab_type": "code",
        "outputId": "f1864e77-3f78-4a32-8c55-0a740b6122fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "embedding_sum=torch.nn.EmbeddingBag(10,3,mode='sum')\n",
        "input=torch.autograd.Variable(torch.LongTensor([1,2,4,5,4,3,2,9]))#将indice为[1,2,4,5]的embedding进行sum，[4,3,2,9]的embedding也进行相加\n",
        "offsets=torch.autograd.Variable(torch.LongTensor([0,4]))\n",
        "embedding_sum(input,offsets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.5947,  2.4057,  1.2095],\n",
              "        [ 1.4345, -0.5633,  0.2688]], grad_fn=<EmbeddingBagBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQNLhSPJALZK",
        "colab_type": "code",
        "outputId": "68909f90-baf2-45fc-a715-44c670b7b50a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "input=torch.autograd.Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))#将indice为[1,2,4,5]的embedding进行sum，[4,3,2,9]的embedding也进行相加\n",
        "# offsets=torch.autograd.Variable(torch.LongTensor([0,4]))\n",
        "embedding_sum(input)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.5947,  2.4057,  1.2095],\n",
              "        [ 1.4345, -0.5633,  0.2688]], grad_fn=<EmbeddingBagBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZjDNBNRB6mc",
        "colab_type": "text"
      },
      "source": [
        "## 02. torch.nn.Linear()\n",
        "\n",
        "* class torch.nn.Linear(in_features, out_features, bias=True)\n",
        "对输入数据做线性变换\n",
        "###参数：\n",
        "* in_features:每个输入样本的大小(输入的特征数，神经元数量),如(batch_size,in_features)\n",
        "* out_features:每个输出样本的大小（输出神经元数量）,如(batch_size,out_features)\n",
        "* bias:若设置为False,这层不会学习偏置，默认为True。\n",
        "\n",
        "### 形状：\n",
        "* 输入：(N,in_features)\n",
        "* 输出：(N,outfeatures)\n",
        "\n",
        "### 变量\n",
        "* weights:形状为(out_features,in_features)的可学习的矩阵\n",
        "* bias:形状为(out_features)的可学习的向量\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SuXkIulVTFk",
        "colab_type": "code",
        "outputId": "5822674e-3129-4d95-d924-6a0502ed72c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "m=torch.nn.Linear(20,30)\n",
        "input=torch.autograd.Variable(torch.randn(128,20))\n",
        "output=m(input)\n",
        "print(output.size())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 30])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuHOcDUfVxS7",
        "colab_type": "code",
        "outputId": "26b9a6e9-f95b-43e5-ed83-ef7f0c1463ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "m.weight.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4zHjiveV4lh",
        "colab_type": "text"
      },
      "source": [
        "## 03 torch.nn.Embedding()\n",
        "\n",
        "* torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)\n",
        "\n",
        "&emsp;&emsp;一个简单的查找表，用于存储固定词典和大小的嵌入。通常用于存储单词嵌入并使用索引检索它们。该模块的输入是索引列表，输出是相应的词嵌入。\n",
        "\n",
        "### 参数\n",
        "* num_embeddings：int。嵌入字典的大小。即词汇表单词数量。\n",
        "* embedding_dim:int。词嵌入维度。\n",
        "* padding_idx：int,可选。指定用以padding的索引位置。所谓padding，就是在将不等长的句子组成一个batch时，对那些空缺的位置补0，以形成一个统一的矩阵。\n",
        "* max_norm：float,可选。如果给定，则将范数大于max_norm的每个嵌入向量重新规范化为范数max_norm。\n",
        "* norm_type:float,可选。为max_norm选项计算p范数的p，默认为2即计算L2范数。\n",
        "* scale_grad_by_freq:boolean,可选。如果给定，将按mini-batch中单词频率的倒数来缩放梯度。默认为False。\n",
        "* sparse：bool，可选。如果为True，则梯度权重矩阵将是稀疏张量。\n",
        "\n",
        "### 变量\n",
        "\n",
        "* Embedding.weight:形状为(num_embeddings,embedding_dim)的可学习权重，初始化为标准正太分布$N(0,1)$\n",
        "\n",
        "### 形状\n",
        "\n",
        "* input（*）:任意形状的LongTensor,包含要提取的索引。\n",
        "* output （*，H）：*表示输入的shape，H=embedding_dim\n",
        "\n",
        "注意：只有少数optimizer支持稀疏梯度，包括：SGD(CUDA和CPU)，SparseAdam(CUDA和CPU),Adagrad（CPU）.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-mSdEAvcWwt",
        "colab_type": "code",
        "outputId": "28be6dcf-b617-49d4-c60a-d04f8054cccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "embedding=torch.nn.Embedding(10,3)\n",
        "input=torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
        "embedding(input)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.6370, -0.5164, -0.1187],\n",
              "         [-1.3355,  0.6742,  1.4405],\n",
              "         [-0.5242,  0.6519,  0.6431],\n",
              "         [-0.6023,  0.9551,  0.1716]],\n",
              "\n",
              "        [[-0.5242,  0.6519,  0.6431],\n",
              "         [ 0.7513,  0.6707, -0.3475],\n",
              "         [-1.3355,  0.6742,  1.4405],\n",
              "         [-1.0698, -1.2547,  0.1406]]], grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZepwEPOcjfK",
        "colab_type": "code",
        "outputId": "578f0bba-ca91-4f35-c42e-a55695b0ac89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding.weight.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-lY2yDUcoPl",
        "colab_type": "code",
        "outputId": "3a5c1e46-1234-465b-a429-e892a144db1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "embedding=torch.nn.Embedding(10,3,padding_idx=0)\n",
        "input=torch.LongTensor([[0,2,0,5]])\n",
        "embedding(input)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
              "         [-0.3680,  1.4680,  0.3064],\n",
              "         [ 0.0000,  0.0000,  0.0000],\n",
              "         [-0.3258,  0.5864, -0.2532]]], grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UlfXE0udAIz",
        "colab_type": "text"
      },
      "source": [
        "## 03. torch.nn.GRU()\n",
        "\n",
        "* torch.nn.GRU(*args,**kwargs)\n",
        "\n",
        "&emsp;&emsp; 在输入序列上应用多层门控循环神经网络(GRU)。对于输入序列的每个元素，每层计算下列函数：\n",
        "\n",
        "$$\n",
        "r_t=\\sigma(W_{ir}x_t+b_{ir}+W_{hr}h_{(h-1)}+b_{hr})\n",
        "$$\n",
        "\n",
        "$$\n",
        "z_t=\\sigma(W_{iz}x_t+b_{iz}+W_{hz}h_{(t-1)}+b_{hz})\n",
        "$$\n",
        "\n",
        "$$\n",
        "n_t=tanh(W_{in}x_t+b_{in}+r_t(W_{hn}h_{(t-1)}+b_{hn}))\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_t=(1-z_t)n_t+z_th_{(t-1)}\n",
        "$$\n",
        "\n",
        "&emsp;&emsp;其中，$h_t$是$t$时刻的隐藏状态,$x_t$是$t$时刻的输入，$h_{t-1}$是$t-$时刻的隐藏状态。$r_t,z_t,n_t$是重置门、更新门和新门。$\\sigma$是sigmoid函数。\n",
        "&emsp;&emsp;在一个多层GRU中，第$l(l >=2)$层的输入$i_t^{(l)}$是前一层的隐状态$h_t^{(l-1)}$乘以dropout$\\sigma _t ^{(l)}$,其中$\\sigma _t ^{(l)}$是一个伯努利随机变量。\n",
        "\n",
        "### 参数：\n",
        "\n",
        "* input_size:输入的特征数量 x\n",
        "* hidden_size:隐状态h的特征数量\n",
        "* num_layers:循环层的数量。例如:num_layers=2意味着堆叠两个GRU层行成一个stacked GRU.然后，第二层GRU会使用第一层GRU的输出作为输入并计算最终结果。默认为1。\n",
        "* bias:如果为False,则层不会使用偏置$b_{ih}$和$b_{hh}$。默认为True。\n",
        "* batch_first:如果为True，则输入和输出张量使用如下形式提供：(batch,seq,feature)。默认为False。\n",
        "* dropout:如果为非0，则除了最后一层外均在每个GRU层的输出上添加一个Dropout层，使用概率dropout。默认为0。\n",
        "* bidirectional:如果为True,则成为双向GRU。默认为False。\n",
        "\n",
        "\n",
        "### 输入：input,h_0\n",
        "\n",
        "* input:形如(seq_len,batch,input_size)的张量，包含输入序列的特征。输入也可以是一个叠加的可变长度的序列。看sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence()\n",
        "* h_0:形如(num_layers*num_directions,batch,hidden_size)的张量，包含批次中每个元素的初始隐藏状态。如果没提供，则默认为0.如果RNN是双向的，num_directions=2,否则为1。\n",
        "\n",
        "### 输出：output,h_n\n",
        "\n",
        "* output:形如(seq_len,batch,num_directions*hidden_size)的张量，对于每个时刻$t$包含来自于GRU的最后一层的输出特征$h_t$。如果使用torch.nn.utils.rnn.PackedSequence作为输入，输出则是packed的序列。对于unpacked的情况，可以使用output.view（seq_len，batch，num_directions，hidden_​​size）分隔方向，前向和后向分别是方向0和1。同样的，在pack的例子中，方向也能被分割。\n",
        "\n",
        "* h_n:形如(num_layers * num_directions, batch, hidden_size)的张量，包含t=seq_len时刻的隐状态。就像输出一样，可是使用h_n.view(num_layers, num_directions, batch, hidden_size)进行分割。\n",
        "\n",
        "### 变量\n",
        "\n",
        "* weight_ih_l[k] :第$k$层 (W_ir|W_iz|W_in)的可学习的输入层到隐藏层的权重，形如(3*hidden_size x input_size)\n",
        "\n",
        "* weight_hh_l[k] – 第$k$层 (W_hr|W_hz|W_hn)的可学习的隐藏层到隐藏层的权重, 形如(3*hidden_size x hidden_size)\n",
        "* bias_ih_l[k] – 第$k$层 (b_ir|b_iz|b_in)的可学习的输入层到隐藏层的偏置, 形如 (3*hidden_size)\n",
        "* bias_hh_l[k] – 第$k$层 (b_ir|b_iz|b_in)的可学习的隐藏层到隐藏层的偏置, 形如 (3*hidden_size)\n",
        "\n",
        "\n",
        "### 注意\n",
        "&emsp;&emsp;所有的权重和偏置均被初始化为$U(-\\sqrt k,\\sqrt k)$，其中$k=\\frac{1}{hidden_size}$。\n",
        "\n",
        "&emsp;&emsp;如果下列条件被给定：\n",
        "* cudnn可使用\n",
        "* 输入数据在GPU上\n",
        "* 输入数据类型为torch.float16\n",
        "* GPU型号为V100 GPU\n",
        "* 输入数据不是 PackedSequence格式\n",
        "\n",
        "可以选择持久性算法来提高性能。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhyVR_f2g8pO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn=torch.nn.GRU(10,20,2)\n",
        "input=torch.randn(5,3,10)\n",
        "h0=torch.randn(2,3,20)\n",
        "output,hn=rnn(input,h0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4NGVAbshgCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72a73301-b642-4831-c02c-f922b122d8b5"
      },
      "source": [
        "output.shape,hn.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 3, 20]), torch.Size([2, 3, 20]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4uvAPPfhjad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}